
<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
		<title>htlcswitch: Go Coverage Report</title>
		<style>
			body {
				background: black;
				color: rgb(80, 80, 80);
			}
			body, pre, #legend span {
				font-family: Menlo, monospace;
				font-weight: bold;
			}
			#topbar {
				background: black;
				position: fixed;
				top: 0; left: 0; right: 0;
				height: 42px;
				border-bottom: 1px solid rgb(80, 80, 80);
			}
			#content {
				margin-top: 50px;
			}
			#nav, #legend {
				float: left;
				margin-left: 10px;
			}
			#legend {
				margin-top: 12px;
			}
			#nav {
				margin-top: 10px;
			}
			#legend span {
				margin: 0 5px;
			}
			.cov0 { color: rgb(192, 0, 0) }
.cov1 { color: rgb(128, 128, 128) }
.cov2 { color: rgb(116, 140, 131) }
.cov3 { color: rgb(104, 152, 134) }
.cov4 { color: rgb(92, 164, 137) }
.cov5 { color: rgb(80, 176, 140) }
.cov6 { color: rgb(68, 188, 143) }
.cov7 { color: rgb(56, 200, 146) }
.cov8 { color: rgb(44, 212, 149) }
.cov9 { color: rgb(32, 224, 152) }
.cov10 { color: rgb(20, 236, 155) }

		</style>
	</head>
	<body>
		<div id="topbar">
			<div id="nav">
				<select id="files">
				
				<option value="file0">github.com/lightningnetwork/lnd/htlcswitch/circuit.go (73.3%)</option>
				
				<option value="file1">github.com/lightningnetwork/lnd/htlcswitch/circuit_map.go (79.5%)</option>
				
				<option value="file2">github.com/lightningnetwork/lnd/htlcswitch/decayedlog.go (62.4%)</option>
				
				<option value="file3">github.com/lightningnetwork/lnd/htlcswitch/failure.go (89.5%)</option>
				
				<option value="file4">github.com/lightningnetwork/lnd/htlcswitch/failure_detail.go (58.3%)</option>
				
				<option value="file5">github.com/lightningnetwork/lnd/htlcswitch/held_htlc_set.go (100.0%)</option>
				
				<option value="file6">github.com/lightningnetwork/lnd/htlcswitch/htlcnotifier.go (61.7%)</option>
				
				<option value="file7">github.com/lightningnetwork/lnd/htlcswitch/interceptable_switch.go (70.4%)</option>
				
				<option value="file8">github.com/lightningnetwork/lnd/htlcswitch/link.go (69.6%)</option>
				
				<option value="file9">github.com/lightningnetwork/lnd/htlcswitch/linkfailure.go (0.0%)</option>
				
				<option value="file10">github.com/lightningnetwork/lnd/htlcswitch/log.go (80.0%)</option>
				
				<option value="file11">github.com/lightningnetwork/lnd/htlcswitch/mailbox.go (93.0%)</option>
				
				<option value="file12">github.com/lightningnetwork/lnd/htlcswitch/mock.go (77.0%)</option>
				
				<option value="file13">github.com/lightningnetwork/lnd/htlcswitch/packet.go (100.0%)</option>
				
				<option value="file14">github.com/lightningnetwork/lnd/htlcswitch/payment_result.go (89.5%)</option>
				
				<option value="file15">github.com/lightningnetwork/lnd/htlcswitch/quiescer.go (80.0%)</option>
				
				<option value="file16">github.com/lightningnetwork/lnd/htlcswitch/resolution_store.go (84.5%)</option>
				
				<option value="file17">github.com/lightningnetwork/lnd/htlcswitch/sequencer.go (0.0%)</option>
				
				<option value="file18">github.com/lightningnetwork/lnd/htlcswitch/switch.go (71.8%)</option>
				
				<option value="file19">github.com/lightningnetwork/lnd/htlcswitch/test_utils.go (81.5%)</option>
				
				</select>
			</div>
			<div id="legend">
				<span>not tracked</span>
			
				<span class="cov0">not covered</span>
				<span class="cov8">covered</span>
			
			</div>
		</div>
		<div id="content">
		
		<pre class="file" id="file0" style="display: none">package htlcswitch

import (
        "encoding/binary"
        "io"

        "github.com/lightningnetwork/lnd/channeldb"
        "github.com/lightningnetwork/lnd/graph/db/models"
        "github.com/lightningnetwork/lnd/htlcswitch/hop"
        "github.com/lightningnetwork/lnd/lnwire"
)

// EmptyCircuitKey is a default value for an outgoing circuit key returned when
// a circuit's keystone has not been set. Note that this value is invalid for
// use as a keystone, since the outgoing channel id can never be equal to
// sourceHop.
var EmptyCircuitKey CircuitKey

// CircuitKey is a tuple of channel ID and HTLC ID, used to uniquely identify
// HTLCs in a circuit. Circuits are identified primarily by the circuit key of
// the incoming HTLC. However, a circuit may also be referenced by its outgoing
// circuit key after the HTLC has been forwarded via the outgoing link.
type CircuitKey = models.CircuitKey

// PaymentCircuit is used by the switch as placeholder between when the
// switch makes a forwarding decision and the outgoing link determines the
// proper HTLC ID for the local log. After the outgoing HTLC ID has been
// determined, the half circuit will be converted into a full PaymentCircuit.
type PaymentCircuit struct {
        // AddRef is the forward reference of the Add update in the incoming
        // link's forwarding package. This value is set on the htlcPacket of the
        // returned settle/fail so that it can be removed from disk.
        AddRef channeldb.AddRef

        // Incoming is the circuit key identifying the incoming channel and htlc
        // index from which this ADD originates.
        Incoming CircuitKey

        // Outgoing is the circuit key identifying the outgoing channel, and the
        // HTLC index that was used to forward the ADD. It will be nil if this
        // circuit's keystone has not been set.
        Outgoing *CircuitKey

        // PaymentHash used as unique identifier of payment.
        PaymentHash [32]byte

        // IncomingAmount is the value of the HTLC from the incoming link.
        IncomingAmount lnwire.MilliSatoshi

        // OutgoingAmount specifies the value of the HTLC leaving the switch,
        // either as a payment or forwarded amount.
        OutgoingAmount lnwire.MilliSatoshi

        // ErrorEncrypter is used to re-encrypt the onion failure before
        // sending it back to the originator of the payment.
        ErrorEncrypter hop.ErrorEncrypter

        // LoadedFromDisk is set true for any circuits loaded after the circuit
        // map is reloaded from disk.
        //
        // NOTE: This value is determined implicitly during a restart. It is not
        // persisted, and should never be set outside the circuit map.
        LoadedFromDisk bool
}

// HasKeystone returns true if an outgoing link has assigned this circuit's
// outgoing circuit key.
func (c *PaymentCircuit) HasKeystone() bool <span class="cov8" title="1">{
        return c.Outgoing != nil
}</span>

// newPaymentCircuit initializes a payment circuit on the heap using the payment
// hash and an in-memory htlc packet.
func newPaymentCircuit(hash *[32]byte, pkt *htlcPacket) *PaymentCircuit <span class="cov8" title="1">{
        var addRef channeldb.AddRef
        if pkt.sourceRef != nil </span><span class="cov8" title="1">{
                addRef = *pkt.sourceRef
        }</span>

        <span class="cov8" title="1">return &amp;PaymentCircuit{
                AddRef: addRef,
                Incoming: CircuitKey{
                        ChanID: pkt.incomingChanID,
                        HtlcID: pkt.incomingHTLCID,
                },
                PaymentHash:    *hash,
                IncomingAmount: pkt.incomingAmount,
                OutgoingAmount: pkt.amount,
                ErrorEncrypter: pkt.obfuscator,
        }</span>
}

// makePaymentCircuit initializes a payment circuit on the stack using the
// payment hash and an in-memory htlc packet.
func makePaymentCircuit(hash *[32]byte, pkt *htlcPacket) PaymentCircuit <span class="cov8" title="1">{
        var addRef channeldb.AddRef
        if pkt.sourceRef != nil </span><span class="cov0" title="0">{
                addRef = *pkt.sourceRef
        }</span>

        <span class="cov8" title="1">return PaymentCircuit{
                AddRef: addRef,
                Incoming: CircuitKey{
                        ChanID: pkt.incomingChanID,
                        HtlcID: pkt.incomingHTLCID,
                },
                PaymentHash:    *hash,
                IncomingAmount: pkt.incomingAmount,
                OutgoingAmount: pkt.amount,
                ErrorEncrypter: pkt.obfuscator,
        }</span>
}

// Encode writes a PaymentCircuit to the provided io.Writer.
func (c *PaymentCircuit) Encode(w io.Writer) error <span class="cov8" title="1">{
        if err := c.AddRef.Encode(w); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">if err := c.Incoming.Encode(w); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">if _, err := w.Write(c.PaymentHash[:]); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">var scratch [8]byte

        binary.BigEndian.PutUint64(scratch[:], uint64(c.IncomingAmount))
        if _, err := w.Write(scratch[:]); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">binary.BigEndian.PutUint64(scratch[:], uint64(c.OutgoingAmount))
        if _, err := w.Write(scratch[:]); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Defaults to EncrypterTypeNone.
        <span class="cov8" title="1">var encrypterType hop.EncrypterType
        if c.ErrorEncrypter != nil </span><span class="cov8" title="1">{
                encrypterType = c.ErrorEncrypter.Type()
        }</span>

        <span class="cov8" title="1">err := binary.Write(w, binary.BigEndian, encrypterType)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Skip encoding of error encrypter if this half add does not have one.
        <span class="cov8" title="1">if encrypterType == hop.EncrypterTypeNone </span><span class="cov8" title="1">{
                return nil
        }</span>

        <span class="cov8" title="1">return c.ErrorEncrypter.Encode(w)</span>
}

// Decode reads a PaymentCircuit from the provided io.Reader.
func (c *PaymentCircuit) Decode(r io.Reader) error <span class="cov8" title="1">{
        if err := c.AddRef.Decode(r); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">if err := c.Incoming.Decode(r); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">if _, err := io.ReadFull(r, c.PaymentHash[:]); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">var scratch [8]byte

        if _, err := io.ReadFull(r, scratch[:]); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">c.IncomingAmount = lnwire.MilliSatoshi(
                binary.BigEndian.Uint64(scratch[:]))

        if _, err := io.ReadFull(r, scratch[:]); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">c.OutgoingAmount = lnwire.MilliSatoshi(
                binary.BigEndian.Uint64(scratch[:]))

        // Read the encrypter type used for this circuit.
        var encrypterType hop.EncrypterType
        err := binary.Read(r, binary.BigEndian, &amp;encrypterType)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">switch encrypterType </span>{
        case hop.EncrypterTypeNone:<span class="cov8" title="1">
                // No encrypter was provided, such as when the payment is
                // locally initiated.
                return nil</span>

        case hop.EncrypterTypeSphinx:<span class="cov8" title="1">
                // Sphinx encrypter was used as this is a forwarded HTLC.
                c.ErrorEncrypter = hop.NewSphinxErrorEncrypter()</span>

        case hop.EncrypterTypeMock:<span class="cov8" title="1">
                // Test encrypter.
                c.ErrorEncrypter = NewMockObfuscator()</span>

        case hop.EncrypterTypeIntroduction:<span class="cov0" title="0">
                c.ErrorEncrypter = hop.NewIntroductionErrorEncrypter()</span>

        case hop.EncrypterTypeRelaying:<span class="cov0" title="0">
                c.ErrorEncrypter = hop.NewRelayingErrorEncrypter()</span>

        default:<span class="cov0" title="0">
                return UnknownEncrypterType(encrypterType)</span>
        }

        <span class="cov8" title="1">return c.ErrorEncrypter.Decode(r)</span>
}

// InKey returns the primary identifier for the circuit corresponding to the
// incoming HTLC.
func (c *PaymentCircuit) InKey() CircuitKey <span class="cov8" title="1">{
        return c.Incoming
}</span>

// OutKey returns the keystone identifying the outgoing link and HTLC ID. If the
// circuit hasn't been completed, this method returns an EmptyKeystone, which is
// an invalid outgoing circuit key. Only call this method if HasKeystone returns
// true.
func (c *PaymentCircuit) OutKey() CircuitKey <span class="cov8" title="1">{
        if c.Outgoing != nil </span><span class="cov8" title="1">{
                return *c.Outgoing
        }</span>

        <span class="cov8" title="1">return EmptyCircuitKey</span>
}
</pre>
		
		<pre class="file" id="file1" style="display: none">package htlcswitch

import (
        "bytes"
        "fmt"
        "sync"

        "github.com/go-errors/errors"
        "github.com/lightningnetwork/lnd/channeldb"
        "github.com/lightningnetwork/lnd/htlcswitch/hop"
        "github.com/lightningnetwork/lnd/kvdb"
        "github.com/lightningnetwork/lnd/lnutils"
        "github.com/lightningnetwork/lnd/lnwire"
)

var (
        // ErrCorruptedCircuitMap indicates that the on-disk bucketing structure
        // has altered since the circuit map instance was initialized.
        ErrCorruptedCircuitMap = errors.New("circuit map has been corrupted")

        // ErrCircuitNotInHashIndex indicates that a particular circuit did not
        // appear in the in-memory hash index.
        ErrCircuitNotInHashIndex = errors.New("payment circuit not found in " +
                "hash index")

        // ErrUnknownCircuit signals that circuit could not be removed from the
        // map because it was not found.
        ErrUnknownCircuit = errors.New("unknown payment circuit")

        // ErrCircuitClosing signals that an htlc has already closed this
        // circuit in-memory.
        ErrCircuitClosing = errors.New("circuit has already been closed")

        // ErrDuplicateCircuit signals that this circuit was previously
        // added.
        ErrDuplicateCircuit = errors.New("duplicate circuit add")

        // ErrUnknownKeystone signals that no circuit was found using the
        // outgoing circuit key.
        ErrUnknownKeystone = errors.New("unknown circuit keystone")

        // ErrDuplicateKeystone signals that this circuit was previously
        // assigned a keystone.
        ErrDuplicateKeystone = errors.New("cannot add duplicate keystone")
)

// CircuitModifier is a common interface used by channel links to modify the
// contents of the circuit map maintained by the switch.
type CircuitModifier interface {
        // OpenCircuits preemptively records a batch keystones that will mark
        // currently pending circuits as open. These changes can be rolled back
        // on restart if the outgoing Adds do not make it into a commitment
        // txn.
        OpenCircuits(...Keystone) error

        // TrimOpenCircuits removes a channel's open channels with htlc indexes
        // above `start`.
        TrimOpenCircuits(chanID lnwire.ShortChannelID, start uint64) error

        // DeleteCircuits removes the incoming circuit key to remove all
        // persistent references to a circuit. Returns a ErrUnknownCircuit if
        // any of the incoming keys are not known.
        DeleteCircuits(inKeys ...CircuitKey) error
}

// CircuitLookup is a common interface used to lookup information that is stored
// in the circuit map.
type CircuitLookup interface {
        // LookupCircuit queries the circuit map for the circuit identified by
        // inKey.
        LookupCircuit(inKey CircuitKey) *PaymentCircuit

        // LookupOpenCircuit queries the circuit map for a circuit identified
        // by its outgoing circuit key.
        LookupOpenCircuit(outKey CircuitKey) *PaymentCircuit
}

// CircuitFwdActions represents the forwarding decision made by the circuit
// map, and is returned from CommitCircuits. The sequence of circuits provided
// to CommitCircuits is split into three sub-sequences, allowing the caller to
// do an in-order scan, comparing the head of each subsequence, to determine
// the decision made by the circuit map.
type CircuitFwdActions struct {
        // Adds is the subsequence of circuits that were successfully committed
        // in the circuit map.
        Adds []*PaymentCircuit

        // Drops is the subsequence of circuits for which no action should be
        // done.
        Drops []*PaymentCircuit

        // Fails is the subsequence of circuits that should be failed back by
        // the calling link.
        Fails []*PaymentCircuit
}

// CircuitMap is an interface for managing the construction and teardown of
// payment circuits used by the switch.
type CircuitMap interface {
        CircuitModifier

        CircuitLookup

        // CommitCircuits attempts to add the given circuits to the circuit
        // map. The list of circuits is split into three distinct
        // sub-sequences, corresponding to adds, drops, and fails. Adds should
        // be forwarded to the switch, while fails should be failed back
        // locally within the calling link.
        CommitCircuits(circuit ...*PaymentCircuit) (*CircuitFwdActions, error)

        // CloseCircuit marks the circuit identified by `outKey` as closing
        // in-memory, which prevents duplicate settles/fails from completing an
        // open circuit twice.
        CloseCircuit(outKey CircuitKey) (*PaymentCircuit, error)

        // FailCircuit is used by locally failed HTLCs to mark the circuit
        // identified by `inKey` as closing in-memory, which prevents duplicate
        // settles/fails from being accepted for the same circuit.
        FailCircuit(inKey CircuitKey) (*PaymentCircuit, error)

        // LookupByPaymentHash queries the circuit map and returns all open
        // circuits that use the given payment hash.
        LookupByPaymentHash(hash [32]byte) []*PaymentCircuit

        // NumPending returns the total number of active circuits added by
        // CommitCircuits.
        NumPending() int

        // NumOpen returns the number of circuits with HTLCs that have been
        // forwarded via an outgoing link.
        NumOpen() int
}

var (
        // circuitAddKey is the key used to retrieve the bucket containing
        // payment circuits. A circuit records information about how to return
        // a packet to the source link, potentially including an error
        // encrypter for applying this hop's encryption to the payload in the
        // reverse direction.
        //
        // Bucket hierarchy:
        //
        // circuitAddKey(root-bucket)
        //             |
        //             |-- &lt;incoming-circuit-key&gt;: &lt;encoded bytes of PaymentCircuit&gt;
        //             |-- &lt;incoming-circuit-key&gt;: &lt;encoded bytes of PaymentCircuit&gt;
        //             |
        //             ...
        //
        circuitAddKey = []byte("circuit-adds")

        // circuitKeystoneKey is used to retrieve the bucket containing circuit
        // keystones, which are set in place once a forwarded packet is
        // assigned an index on an outgoing commitment txn.
        //
        // Bucket hierarchy:
        //
        // circuitKeystoneKey(root-bucket)
        //             |
        //             |-- &lt;outgoing-circuit-key&gt;: &lt;incoming-circuit-key&gt;
        //             |-- &lt;outgoing-circuit-key&gt;: &lt;incoming-circuit-key&gt;
        //             |
        //             ...
        //
        circuitKeystoneKey = []byte("circuit-keystones")
)

// circuitMap is a data structure that implements thread safe, persistent
// storage of circuit routing information. The switch consults a circuit map to
// determine where to forward returning HTLC update messages. Circuits are
// always identifiable by their incoming CircuitKey, in addition to their
// outgoing CircuitKey if the circuit is fully-opened.
type circuitMap struct {
        cfg *CircuitMapConfig

        mtx sync.RWMutex

        // pending is an in-memory mapping of all half payment circuits, and is
        // kept in sync with the on-disk contents of the circuit map.
        pending map[CircuitKey]*PaymentCircuit

        // opened is an in-memory mapping of all full payment circuits, which
        // is also synchronized with the persistent state of the circuit map.
        opened map[CircuitKey]*PaymentCircuit

        // closed is an in-memory set of circuits for which the switch has
        // received a settle or fail. This precedes the actual deletion of a
        // circuit from disk.
        closed map[CircuitKey]struct{}

        // hashIndex is a volatile index that facilitates fast queries by
        // payment hash against the contents of circuits. This index can be
        // reconstructed entirely from the set of persisted full circuits on
        // startup.
        hashIndex map[[32]byte]map[CircuitKey]struct{}
}

// CircuitMapConfig houses the critical interfaces and references necessary to
// parameterize an instance of circuitMap.
type CircuitMapConfig struct {
        // DB provides the persistent storage engine for the circuit map.
        DB kvdb.Backend

        // FetchAllOpenChannels is a function that fetches all currently open
        // channels from the channel database.
        FetchAllOpenChannels func() ([]*channeldb.OpenChannel, error)

        // FetchClosedChannels is a function that fetches all closed channels
        // from the channel database.
        FetchClosedChannels func(
                pendingOnly bool) ([]*channeldb.ChannelCloseSummary, error)

        // ExtractErrorEncrypter derives the shared secret used to encrypt
        // errors from the obfuscator's ephemeral public key.
        ExtractErrorEncrypter hop.ErrorEncrypterExtracter

        // CheckResolutionMsg checks whether a given resolution message exists
        // for the passed CircuitKey.
        CheckResolutionMsg func(outKey *CircuitKey) error
}

// NewCircuitMap creates a new instance of the circuitMap.
func NewCircuitMap(cfg *CircuitMapConfig) (CircuitMap, error) <span class="cov8" title="1">{
        cm := &amp;circuitMap{
                cfg: cfg,
        }

        // Initialize the on-disk buckets used by the circuit map.
        if err := cm.initBuckets(); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // Delete old circuits and keystones of closed channels.
        <span class="cov8" title="1">if err := cm.cleanClosedChannels(); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // Load any previously persisted circuit into back into memory.
        <span class="cov8" title="1">if err := cm.restoreMemState(); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // Trim any keystones that were not committed in an outgoing commit txn.
        //
        // NOTE: This operation will be applied to the persistent state of all
        // active channels. Therefore, it must be called before any links are
        // created to avoid interfering with normal operation.
        <span class="cov8" title="1">if err := cm.trimAllOpenCircuits(); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return cm, nil</span>
}

// initBuckets ensures that the primary buckets used by the circuit are
// initialized so that we can assume their existence after startup.
func (cm *circuitMap) initBuckets() error <span class="cov8" title="1">{
        return kvdb.Update(cm.cfg.DB, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                _, err := tx.CreateTopLevelBucket(circuitKeystoneKey)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov8" title="1">_, err = tx.CreateTopLevelBucket(circuitAddKey)
                return err</span>
        }, func() {<span class="cov8" title="1">}</span>)
}

// cleanClosedChannels deletes all circuits and keystones related to closed
// channels. It first reads all the closed channels and caches the ShortChanIDs
// into a map for fast lookup. Then it iterates the circuit bucket and keystone
// bucket and deletes items whose ChanID matches the ShortChanID.
//
// NOTE: this operation can also be built into restoreMemState since the latter
// already opens and iterates the two root buckets, circuitAddKey and
// circuitKeystoneKey. Depending on the size of the buckets, this marginal gain
// may be worth investigating. Atm, for clarity, this operation is wrapped into
// its own function.
func (cm *circuitMap) cleanClosedChannels() error <span class="cov8" title="1">{
        log.Infof("Cleaning circuits from disk for closed channels")

        // closedChanIDSet stores the short channel IDs for closed channels.
        closedChanIDSet := make(map[lnwire.ShortChannelID]struct{})

        // circuitKeySet stores the incoming circuit keys of the payment
        // circuits that need to be deleted.
        circuitKeySet := make(map[CircuitKey]struct{})

        // keystoneKeySet stores the outgoing keys of the keystones that need
        // to be deleted.
        keystoneKeySet := make(map[CircuitKey]struct{})

        // isClosedChannel is a helper closure that returns a bool indicating
        // the chanID belongs to a closed channel.
        isClosedChannel := func(chanID lnwire.ShortChannelID) bool </span><span class="cov8" title="1">{
                // Skip if the channel ID is zero value. This has the effect
                // that a zero value incoming or outgoing key will never be
                // matched and its corresponding circuits or keystones are not
                // deleted.
                if chanID.ToUint64() == 0 </span><span class="cov8" title="1">{
                        return false
                }</span>

                <span class="cov8" title="1">_, ok := closedChanIDSet[chanID]
                return ok</span>
        }

        // Find closed channels and cache their ShortChannelIDs into a map.
        // This map will be used for looking up relative circuits and keystones.
        <span class="cov8" title="1">closedChannels, err := cm.cfg.FetchClosedChannels(false)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">for _, closedChannel := range closedChannels </span><span class="cov8" title="1">{
                // Skip if the channel close is pending.
                if closedChannel.IsPending </span><span class="cov8" title="1">{
                        continue</span>
                }

                <span class="cov8" title="1">closedChanIDSet[closedChannel.ShortChanID] = struct{}{}</span>
        }

        <span class="cov8" title="1">log.Debugf("Found %v closed channels", len(closedChanIDSet))

        // Exit early if there are no closed channels.
        if len(closedChanIDSet) == 0 </span><span class="cov8" title="1">{
                log.Infof("Finished cleaning: no closed channels found, " +
                        "no actions taken.",
                )
                return nil
        }</span>

        // Find the payment circuits and keystones that need to be deleted.
        <span class="cov8" title="1">if err := kvdb.View(cm.cfg.DB, func(tx kvdb.RTx) error </span><span class="cov8" title="1">{
                circuitBkt := tx.ReadBucket(circuitAddKey)
                if circuitBkt == nil </span><span class="cov0" title="0">{
                        return ErrCorruptedCircuitMap
                }</span>
                <span class="cov8" title="1">keystoneBkt := tx.ReadBucket(circuitKeystoneKey)
                if keystoneBkt == nil </span><span class="cov0" title="0">{
                        return ErrCorruptedCircuitMap
                }</span>

                // If a circuit's incoming/outgoing key prefix matches the
                // ShortChanID, it will be deleted. However, if the ShortChanID
                // of the incoming key is zero, the circuit will be kept as it
                // indicates a locally initiated payment.
                <span class="cov8" title="1">if err := circuitBkt.ForEach(func(_, v []byte) error </span><span class="cov8" title="1">{
                        circuit, err := cm.decodeCircuit(v)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>

                        // Check if the incoming channel ID can be found in the
                        // closed channel ID map.
                        <span class="cov8" title="1">if !isClosedChannel(circuit.Incoming.ChanID) </span><span class="cov8" title="1">{
                                return nil
                        }</span>

                        <span class="cov8" title="1">circuitKeySet[circuit.Incoming] = struct{}{}

                        return nil</span>
                }); err != nil <span class="cov0" title="0">{
                        return err
                }</span>

                // If a keystone's InKey or OutKey matches the short channel id
                // in the closed channel ID map, it will be deleted.
                <span class="cov8" title="1">err := keystoneBkt.ForEach(func(k, v []byte) error </span><span class="cov8" title="1">{
                        var (
                                inKey  CircuitKey
                                outKey CircuitKey
                        )

                        // Decode the incoming and outgoing circuit keys.
                        if err := inKey.SetBytes(v); err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                        <span class="cov8" title="1">if err := outKey.SetBytes(k); err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>

                        // Check if the incoming channel ID can be found in the
                        // closed channel ID map.
                        <span class="cov8" title="1">if isClosedChannel(inKey.ChanID) </span><span class="cov8" title="1">{
                                // If the incoming channel is closed, we can
                                // skip checking on outgoing channel ID because
                                // this keystone will be deleted.
                                keystoneKeySet[outKey] = struct{}{}

                                // Technically the incoming keys found in
                                // keystone bucket should be a subset of
                                // circuit bucket. So a previous loop should
                                // have this inKey put inside circuitAddKey map
                                // already. We do this again to be sure the
                                // circuits are properly cleaned. Even this
                                // inKey doesn't exist in circuit bucket, we
                                // are fine as db deletion is a noop.
                                circuitKeySet[inKey] = struct{}{}
                                return nil
                        }</span>

                        // Check if the outgoing channel ID can be found in the
                        // closed channel ID map. Notice that we need to store
                        // the outgoing key because it's used for db query.
                        //
                        // NOTE: We skip this if a resolution message can be
                        // found under the outKey. This means that there is an
                        // existing resolution message(s) that need to get to
                        // the incoming links.
                        <span class="cov8" title="1">if isClosedChannel(outKey.ChanID) </span><span class="cov8" title="1">{
                                // Check the resolution message store. A return
                                // value of nil means we need to skip deleting
                                // these circuits.
                                if cm.cfg.CheckResolutionMsg(&amp;outKey) == nil </span><span class="cov8" title="1">{
                                        return nil
                                }</span>

                                <span class="cov8" title="1">keystoneKeySet[outKey] = struct{}{}

                                // Also update circuitKeySet to mark the
                                // payment circuit needs to be deleted.
                                circuitKeySet[inKey] = struct{}{}</span>
                        }

                        <span class="cov8" title="1">return nil</span>
                })
                <span class="cov8" title="1">return err</span>
        }, func() <span class="cov8" title="1">{
                // Reset the sets.
                circuitKeySet = make(map[CircuitKey]struct{})
                keystoneKeySet = make(map[CircuitKey]struct{})
        }</span>); err != nil <span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">log.Debugf("To be deleted: num_circuits=%v, num_keystones=%v",
                len(circuitKeySet), len(keystoneKeySet),
        )

        numCircuitsDeleted := 0
        numKeystonesDeleted := 0

        // Delete all the circuits and keystones for closed channels.
        if err := kvdb.Update(cm.cfg.DB, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                circuitBkt := tx.ReadWriteBucket(circuitAddKey)
                if circuitBkt == nil </span><span class="cov0" title="0">{
                        return ErrCorruptedCircuitMap
                }</span>
                <span class="cov8" title="1">keystoneBkt := tx.ReadWriteBucket(circuitKeystoneKey)
                if keystoneBkt == nil </span><span class="cov0" title="0">{
                        return ErrCorruptedCircuitMap
                }</span>

                // Delete the circuit.
                <span class="cov8" title="1">for inKey := range circuitKeySet </span><span class="cov8" title="1">{
                        if err := circuitBkt.Delete(inKey.Bytes()); err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>

                        <span class="cov8" title="1">numCircuitsDeleted++</span>
                }

                // Delete the keystone using the outgoing key.
                <span class="cov8" title="1">for outKey := range keystoneKeySet </span><span class="cov8" title="1">{
                        err := keystoneBkt.Delete(outKey.Bytes())
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>

                        <span class="cov8" title="1">numKeystonesDeleted++</span>
                }

                <span class="cov8" title="1">return nil</span>
        }, func() {<span class="cov8" title="1">}</span>); err != nil <span class="cov0" title="0">{
                numCircuitsDeleted = 0
                numKeystonesDeleted = 0
                return err
        }</span>

        <span class="cov8" title="1">log.Infof("Finished cleaning: num_closed_channel=%v, "+
                "num_circuits=%v, num_keystone=%v",
                len(closedChannels), numCircuitsDeleted, numKeystonesDeleted,
        )

        return nil</span>
}

// restoreMemState loads the contents of the half circuit and full circuit
// buckets from disk and reconstructs the in-memory representation of the
// circuit map. Afterwards, the state of the hash index is reconstructed using
// the recovered set of full circuits. This method will also remove any stray
// keystones, which are those that appear fully-opened, but have no pending
// circuit related to the intended incoming link.
func (cm *circuitMap) restoreMemState() error <span class="cov8" title="1">{
        log.Infof("Restoring in-memory circuit state from disk")

        var (
                opened  map[CircuitKey]*PaymentCircuit
                pending map[CircuitKey]*PaymentCircuit
        )

        if err := kvdb.Update(cm.cfg.DB, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                // Restore any of the circuits persisted in the circuit bucket
                // back into memory.
                circuitBkt := tx.ReadWriteBucket(circuitAddKey)
                if circuitBkt == nil </span><span class="cov0" title="0">{
                        return ErrCorruptedCircuitMap
                }</span>

                <span class="cov8" title="1">if err := circuitBkt.ForEach(func(_, v []byte) error </span><span class="cov8" title="1">{
                        circuit, err := cm.decodeCircuit(v)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>

                        <span class="cov8" title="1">circuit.LoadedFromDisk = true
                        pending[circuit.Incoming] = circuit

                        return nil</span>
                }); err != nil <span class="cov0" title="0">{
                        return err
                }</span>

                // Furthermore, load the keystone bucket and resurrect the
                // keystones used in any open circuits.
                <span class="cov8" title="1">keystoneBkt := tx.ReadWriteBucket(circuitKeystoneKey)
                if keystoneBkt == nil </span><span class="cov0" title="0">{
                        return ErrCorruptedCircuitMap
                }</span>

                <span class="cov8" title="1">var strayKeystones []Keystone
                if err := keystoneBkt.ForEach(func(k, v []byte) error </span><span class="cov8" title="1">{
                        var (
                                inKey  CircuitKey
                                outKey = &amp;CircuitKey{}
                        )

                        // Decode the incoming and outgoing circuit keys.
                        if err := inKey.SetBytes(v); err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                        <span class="cov8" title="1">if err := outKey.SetBytes(k); err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>

                        // Retrieve the pending circuit, set its keystone, then
                        // add it to the opened map.
                        <span class="cov8" title="1">circuit, ok := pending[inKey]
                        if ok </span><span class="cov8" title="1">{
                                circuit.Outgoing = outKey
                                opened[*outKey] = circuit
                        }</span> else<span class="cov0" title="0"> {
                                strayKeystones = append(strayKeystones, Keystone{
                                        InKey:  inKey,
                                        OutKey: *outKey,
                                })
                        }</span>

                        <span class="cov8" title="1">return nil</span>
                }); err != nil <span class="cov0" title="0">{
                        return err
                }</span>

                // If any stray keystones were found, we'll proceed to prune
                // them from the circuit map's persistent storage. This may
                // manifest on older nodes that had updated channels before
                // their short channel id was set properly. We believe this
                // issue has been fixed, though this will allow older nodes to
                // recover without additional intervention.
                <span class="cov8" title="1">for _, strayKeystone := range strayKeystones </span><span class="cov0" title="0">{
                        // As a precaution, we will only cleanup keystones
                        // related to locally-initiated payments. If a
                        // documented case of stray keystones emerges for
                        // forwarded payments, this check should be removed, but
                        // with extreme caution.
                        if strayKeystone.OutKey.ChanID != hop.Source </span><span class="cov0" title="0">{
                                continue</span>
                        }

                        <span class="cov0" title="0">log.Infof("Removing stray keystone: %v", strayKeystone)
                        err := keystoneBkt.Delete(strayKeystone.OutKey.Bytes())
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }

                <span class="cov8" title="1">return nil</span>

        }, func() <span class="cov8" title="1">{
                opened = make(map[CircuitKey]*PaymentCircuit)
                pending = make(map[CircuitKey]*PaymentCircuit)
        }</span>); err != nil <span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">cm.pending = pending
        cm.opened = opened
        cm.closed = make(map[CircuitKey]struct{})

        log.Infof("Payment circuits loaded: num_pending=%v, num_open=%v",
                len(pending), len(opened))

        // Finally, reconstruct the hash index by running through our set of
        // open circuits.
        cm.hashIndex = make(map[[32]byte]map[CircuitKey]struct{})
        for _, circuit := range opened </span><span class="cov8" title="1">{
                cm.addCircuitToHashIndex(circuit)
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// decodeCircuit reconstructs an in-memory payment circuit from a byte slice.
// The byte slice is assumed to have been generated by the circuit's Encode
// method. If the decoding is successful, the onion obfuscator will be
// reextracted, since it is not stored in plaintext on disk.
func (cm *circuitMap) decodeCircuit(v []byte) (*PaymentCircuit, error) <span class="cov8" title="1">{
        var circuit = &amp;PaymentCircuit{}

        circuitReader := bytes.NewReader(v)
        if err := circuit.Decode(circuitReader); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // If the error encrypter is nil, this is locally-source payment so
        // there is no encrypter.
        <span class="cov8" title="1">if circuit.ErrorEncrypter == nil </span><span class="cov8" title="1">{
                return circuit, nil
        }</span>

        // Otherwise, we need to reextract the encrypter, so that the shared
        // secret is rederived from what was decoded.
        <span class="cov8" title="1">err := circuit.ErrorEncrypter.Reextract(
                cm.cfg.ExtractErrorEncrypter,
        )
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return circuit, nil</span>
}

// trimAllOpenCircuits reads the set of active channels from disk and trims
// keystones for any non-pending channels using the next unallocated htlc index.
// This method is intended to be called on startup. Each link will also trim
// it's own circuits upon startup.
//
// NOTE: This operation will be applied to the persistent state of all active
// channels. Therefore, it must be called before any links are created to avoid
// interfering with normal operation.
func (cm *circuitMap) trimAllOpenCircuits() error <span class="cov8" title="1">{
        activeChannels, err := cm.cfg.FetchAllOpenChannels()
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">for _, activeChannel := range activeChannels </span><span class="cov8" title="1">{
                if activeChannel.IsPending </span><span class="cov0" title="0">{
                        continue</span>
                }

                // First, skip any channels that have not been assigned their
                // final channel identifier, otherwise we would try to trim
                // htlcs belonging to the all-zero, hop.Source ID.
                <span class="cov8" title="1">chanID := activeChannel.ShortChanID()
                if chanID == hop.Source </span><span class="cov0" title="0">{
                        continue</span>
                }

                // Next, retrieve the next unallocated htlc index, which bounds
                // the cutoff of confirmed htlc indexes.
                <span class="cov8" title="1">start, err := activeChannel.NextLocalHtlcIndex()
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                // Finally, remove all pending circuits above at or above the
                // next unallocated local htlc indexes. This has the effect of
                // reverting any circuits that have either not been locked in,
                // or had not been included in a pending commitment.
                <span class="cov8" title="1">err = cm.TrimOpenCircuits(chanID, start)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// TrimOpenCircuits removes a channel's keystones above the short chan id's
// highest committed htlc index. This has the effect of returning those
// circuits to a half-open state. Since opening of circuits is done in advance
// of actually committing the Add htlcs into a commitment txn, this allows
// circuits to be opened preemptively, since we can roll them back after any
// failures.
func (cm *circuitMap) TrimOpenCircuits(chanID lnwire.ShortChannelID,
        start uint64) error <span class="cov8" title="1">{

        log.Infof("Trimming open circuits for chan_id=%v, start_htlc_id=%v",
                chanID, start)

        var trimmedOutKeys []CircuitKey

        // Scan forward from the last unacked htlc id, stopping as soon as we
        // don't find any more. Outgoing htlc id's must be assigned in order,
        // so there should never be disjoint segments of keystones to trim.
        cm.mtx.Lock()
        for i := start; ; i++ </span><span class="cov8" title="1">{
                outKey := CircuitKey{
                        ChanID: chanID,
                        HtlcID: i,
                }

                circuit, ok := cm.opened[outKey]
                if !ok </span><span class="cov8" title="1">{
                        break</span>
                }

                <span class="cov8" title="1">circuit.Outgoing = nil
                delete(cm.opened, outKey)
                trimmedOutKeys = append(trimmedOutKeys, outKey)
                cm.removeCircuitFromHashIndex(circuit)</span>
        }
        <span class="cov8" title="1">cm.mtx.Unlock()

        if len(trimmedOutKeys) == 0 </span><span class="cov8" title="1">{
                return nil
        }</span>

        <span class="cov8" title="1">return kvdb.Update(cm.cfg.DB, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                keystoneBkt := tx.ReadWriteBucket(circuitKeystoneKey)
                if keystoneBkt == nil </span><span class="cov0" title="0">{
                        return ErrCorruptedCircuitMap
                }</span>

                <span class="cov8" title="1">for _, outKey := range trimmedOutKeys </span><span class="cov8" title="1">{
                        err := keystoneBkt.Delete(outKey.Bytes())
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }

                <span class="cov8" title="1">return nil</span>
        }, func() {<span class="cov8" title="1">}</span>)
}

// LookupCircuit queries the circuit map for the circuit identified by its
// incoming circuit key. Returns nil if there is no such circuit.
func (cm *circuitMap) LookupCircuit(inKey CircuitKey) *PaymentCircuit <span class="cov8" title="1">{
        cm.mtx.RLock()
        defer cm.mtx.RUnlock()

        return cm.pending[inKey]
}</span>

// LookupOpenCircuit searches for the circuit identified by its outgoing circuit
// key.
func (cm *circuitMap) LookupOpenCircuit(outKey CircuitKey) *PaymentCircuit <span class="cov8" title="1">{
        cm.mtx.RLock()
        defer cm.mtx.RUnlock()

        return cm.opened[outKey]
}</span>

// LookupByPaymentHash looks up and returns any payment circuits with a given
// payment hash.
func (cm *circuitMap) LookupByPaymentHash(hash [32]byte) []*PaymentCircuit <span class="cov8" title="1">{
        cm.mtx.RLock()
        defer cm.mtx.RUnlock()

        var circuits []*PaymentCircuit
        if circuitSet, ok := cm.hashIndex[hash]; ok </span><span class="cov8" title="1">{
                // Iterate over the outgoing circuit keys found with this hash,
                // and retrieve the circuit from the opened map.
                circuits = make([]*PaymentCircuit, 0, len(circuitSet))
                for key := range circuitSet </span><span class="cov8" title="1">{
                        if circuit, ok := cm.opened[key]; ok </span><span class="cov8" title="1">{
                                circuits = append(circuits, circuit)
                        }</span>
                }
        }

        <span class="cov8" title="1">return circuits</span>
}

// CommitCircuits accepts any number of circuits and persistently adds them to
// the switch's circuit map. The method returns a list of circuits that had not
// been seen prior by the switch. A link should only forward HTLCs corresponding
// to the returned circuits to the switch.
//
// NOTE: This method uses batched writes to improve performance, gains will only
// be realized if it is called concurrently from separate goroutines.
func (cm *circuitMap) CommitCircuits(circuits ...*PaymentCircuit) (
        *CircuitFwdActions, error) <span class="cov8" title="1">{

        inKeys := make([]CircuitKey, 0, len(circuits))
        for _, circuit := range circuits </span><span class="cov8" title="1">{
                inKeys = append(inKeys, circuit.Incoming)
        }</span>

        <span class="cov8" title="1">log.Tracef("Committing fresh circuits: %v", lnutils.SpewLogClosure(
                inKeys))

        actions := &amp;CircuitFwdActions{}

        // If an empty list was passed, return early to avoid grabbing the lock.
        if len(circuits) == 0 </span><span class="cov0" title="0">{
                return actions, nil
        }</span>

        // First, we reconcile the provided circuits with our set of pending
        // circuits to construct a set of new circuits that need to be written
        // to disk. The circuit's pointer is stored so that we only permit this
        // exact circuit to be forwarded through the switch. If a circuit is
        // already pending, the htlc will be reforwarded by the switch.
        //
        // NOTE: We track an additional addFails subsequence, which permits us
        // to fail back all packets that weren't dropped if we encounter an
        // error when committing the circuits.
        <span class="cov8" title="1">cm.mtx.Lock()
        var adds, drops, fails, addFails []*PaymentCircuit
        for _, circuit := range circuits </span><span class="cov8" title="1">{
                inKey := circuit.InKey()
                if foundCircuit, ok := cm.pending[inKey]; ok </span><span class="cov8" title="1">{
                        switch </span>{

                        // This circuit has a keystone, it's waiting for a
                        // response from the remote peer on the outgoing link.
                        // Drop it like it's hot, ensure duplicates get caught.
                        case foundCircuit.HasKeystone():<span class="cov8" title="1">
                                drops = append(drops, circuit)</span>

                        // If no keystone is set and the switch has not been
                        // restarted, the corresponding packet should still be
                        // in the outgoing link's mailbox. It will be delivered
                        // if it comes online before the switch goes down.
                        //
                        // NOTE: Dropping here prevents a flapping, incoming
                        // link from failing a duplicate add while it is still
                        // in the server's memory mailboxes.
                        case !foundCircuit.LoadedFromDisk:<span class="cov8" title="1">
                                drops = append(drops, circuit)</span>

                        // Otherwise, the in-mem packet has been lost due to a
                        // restart. It is now safe to send back a failure along
                        // the incoming link. The incoming link should be able
                        // detect and ignore duplicate packets of this type.
                        default:<span class="cov8" title="1">
                                fails = append(fails, circuit)
                                addFails = append(addFails, circuit)</span>
                        }

                        <span class="cov8" title="1">continue</span>
                }

                <span class="cov8" title="1">cm.pending[inKey] = circuit
                adds = append(adds, circuit)
                addFails = append(addFails, circuit)</span>
        }
        <span class="cov8" title="1">cm.mtx.Unlock()

        // If all circuits are dropped or failed, we are done.
        if len(adds) == 0 </span><span class="cov8" title="1">{
                actions.Drops = drops
                actions.Fails = fails
                return actions, nil
        }</span>

        // Now, optimistically serialize the circuits to add.
        <span class="cov8" title="1">var bs = make([]bytes.Buffer, len(adds))
        for i, circuit := range adds </span><span class="cov8" title="1">{
                if err := circuit.Encode(&amp;bs[i]); err != nil </span><span class="cov0" title="0">{
                        actions.Drops = drops
                        actions.Fails = addFails
                        return actions, err
                }</span>
        }

        // Write the entire batch of circuits to the persistent circuit bucket
        // using bolt's Batch write. This method must be called from multiple,
        // distinct goroutines to have any impact on performance.
        <span class="cov8" title="1">err := kvdb.Batch(cm.cfg.DB, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                circuitBkt := tx.ReadWriteBucket(circuitAddKey)
                if circuitBkt == nil </span><span class="cov0" title="0">{
                        return ErrCorruptedCircuitMap
                }</span>

                <span class="cov8" title="1">for i, circuit := range adds </span><span class="cov8" title="1">{
                        inKeyBytes := circuit.InKey().Bytes()
                        circuitBytes := bs[i].Bytes()

                        err := circuitBkt.Put(inKeyBytes, circuitBytes)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }

                <span class="cov8" title="1">return nil</span>
        })

        // Return if the write succeeded.
        <span class="cov8" title="1">if err == nil </span><span class="cov8" title="1">{
                actions.Adds = adds
                actions.Drops = drops
                actions.Fails = fails
                return actions, nil
        }</span>

        // Otherwise, rollback the circuits added to the pending set if the
        // write failed.
        <span class="cov0" title="0">cm.mtx.Lock()
        for _, circuit := range adds </span><span class="cov0" title="0">{
                delete(cm.pending, circuit.InKey())
        }</span>
        <span class="cov0" title="0">cm.mtx.Unlock()

        // Since our write failed, we will return the dropped packets and mark
        // all other circuits as failed.
        actions.Drops = drops
        actions.Fails = addFails

        return actions, err</span>
}

// Keystone is a tuple binding an incoming and outgoing CircuitKey. Keystones
// are preemptively written by an outgoing link before signing a new commitment
// state, and cements which HTLCs we are awaiting a response from a remote
// peer.
type Keystone struct {
        InKey  CircuitKey
        OutKey CircuitKey
}

// String returns a human readable description of the Keystone.
func (k Keystone) String() string <span class="cov8" title="1">{
        return fmt.Sprintf("%s --&gt; %s", k.InKey, k.OutKey)
}</span>

// OpenCircuits sets the outgoing circuit key for the circuit identified by
// inKey, persistently marking the circuit as opened. After the changes have
// been persisted, the circuit map's in-memory indexes are updated so that this
// circuit can be queried using LookupByKeystone or LookupByPaymentHash.
func (cm *circuitMap) OpenCircuits(keystones ...Keystone) error <span class="cov8" title="1">{
        if len(keystones) == 0 </span><span class="cov8" title="1">{
                return nil
        }</span>

        <span class="cov8" title="1">log.Tracef("Opening finalized circuits: %v", lnutils.SpewLogClosure(
                keystones))

        // Check that all keystones correspond to committed-but-unopened
        // circuits.
        cm.mtx.RLock()
        openedCircuits := make([]*PaymentCircuit, 0, len(keystones))
        for _, ks := range keystones </span><span class="cov8" title="1">{
                if _, ok := cm.opened[ks.OutKey]; ok </span><span class="cov8" title="1">{
                        cm.mtx.RUnlock()
                        return ErrDuplicateKeystone
                }</span>

                <span class="cov8" title="1">circuit, ok := cm.pending[ks.InKey]
                if !ok </span><span class="cov0" title="0">{
                        cm.mtx.RUnlock()
                        return ErrUnknownCircuit
                }</span>

                <span class="cov8" title="1">openedCircuits = append(openedCircuits, circuit)</span>
        }
        <span class="cov8" title="1">cm.mtx.RUnlock()

        err := kvdb.Update(cm.cfg.DB, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                // Now, load the circuit bucket to which we will write the
                // already serialized circuit.
                keystoneBkt := tx.ReadWriteBucket(circuitKeystoneKey)
                if keystoneBkt == nil </span><span class="cov0" title="0">{
                        return ErrCorruptedCircuitMap
                }</span>

                <span class="cov8" title="1">for _, ks := range keystones </span><span class="cov8" title="1">{
                        outBytes := ks.OutKey.Bytes()
                        inBytes := ks.InKey.Bytes()
                        err := keystoneBkt.Put(outBytes, inBytes)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }

                <span class="cov8" title="1">return nil</span>
        }, func() {<span class="cov8" title="1">}</span>)

        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">cm.mtx.Lock()
        for i, circuit := range openedCircuits </span><span class="cov8" title="1">{
                ks := keystones[i]

                // Since our persistent operation was successful, we can now
                // modify the in memory representations. Set the outgoing
                // circuit key on our pending circuit, add the same circuit to
                // set of opened circuits, and add this circuit to the hash
                // index.
                circuit.Outgoing = &amp;CircuitKey{}
                *circuit.Outgoing = ks.OutKey

                cm.opened[ks.OutKey] = circuit
                cm.addCircuitToHashIndex(circuit)
        }</span>
        <span class="cov8" title="1">cm.mtx.Unlock()

        return nil</span>
}

// addCirciutToHashIndex inserts a circuit into the circuit map's hash index, so
// that it can be queried using LookupByPaymentHash.
func (cm *circuitMap) addCircuitToHashIndex(c *PaymentCircuit) <span class="cov8" title="1">{
        if _, ok := cm.hashIndex[c.PaymentHash]; !ok </span><span class="cov8" title="1">{
                cm.hashIndex[c.PaymentHash] = make(map[CircuitKey]struct{})
        }</span>
        <span class="cov8" title="1">cm.hashIndex[c.PaymentHash][c.OutKey()] = struct{}{}</span>
}

// FailCircuit marks the circuit identified by `inKey` as closing in-memory,
// which prevents duplicate settles/fails from completing an open circuit twice.
func (cm *circuitMap) FailCircuit(inKey CircuitKey) (*PaymentCircuit, error) <span class="cov8" title="1">{

        cm.mtx.Lock()
        defer cm.mtx.Unlock()

        circuit, ok := cm.pending[inKey]
        if !ok </span><span class="cov0" title="0">{
                return nil, ErrUnknownCircuit
        }</span>

        <span class="cov8" title="1">_, ok = cm.closed[inKey]
        if ok </span><span class="cov8" title="1">{
                return nil, ErrCircuitClosing
        }</span>

        <span class="cov8" title="1">cm.closed[inKey] = struct{}{}

        return circuit, nil</span>
}

// CloseCircuit marks the circuit identified by `outKey` as closing in-memory,
// which prevents duplicate settles/fails from completing an open
// circuit twice.
func (cm *circuitMap) CloseCircuit(outKey CircuitKey) (*PaymentCircuit, error) <span class="cov8" title="1">{

        cm.mtx.Lock()
        defer cm.mtx.Unlock()

        circuit, ok := cm.opened[outKey]
        if !ok </span><span class="cov8" title="1">{
                return nil, ErrUnknownCircuit
        }</span>

        <span class="cov8" title="1">_, ok = cm.closed[circuit.Incoming]
        if ok </span><span class="cov0" title="0">{
                return nil, ErrCircuitClosing
        }</span>

        <span class="cov8" title="1">cm.closed[circuit.Incoming] = struct{}{}

        return circuit, nil</span>
}

// DeleteCircuits destroys the target circuits by removing them from the circuit
// map, additionally removing the circuits' keystones if any HTLCs were
// forwarded through an outgoing link. The circuits should be identified by its
// incoming circuit key. If a given circuit is not found in the circuit map, it
// will be ignored from the query. This would typically indicate that the
// circuit was already cleaned up at a different point in time.
func (cm *circuitMap) DeleteCircuits(inKeys ...CircuitKey) error <span class="cov8" title="1">{

        log.Tracef("Deleting resolved circuits: %v", lnutils.SpewLogClosure(
                inKeys))

        var (
                closingCircuits = make(map[CircuitKey]struct{})
                removedCircuits = make(map[CircuitKey]*PaymentCircuit)
        )

        cm.mtx.Lock()
        // Remove any references to the circuits from memory, keeping track of
        // which circuits were removed, and which ones had been marked closed.
        // This can be used to restore these entries later if the persistent
        // removal fails.
        for _, inKey := range inKeys </span><span class="cov8" title="1">{
                circuit, ok := cm.pending[inKey]
                if !ok </span><span class="cov8" title="1">{
                        continue</span>
                }
                <span class="cov8" title="1">delete(cm.pending, inKey)

                if _, ok := cm.closed[inKey]; ok </span><span class="cov8" title="1">{
                        closingCircuits[inKey] = struct{}{}
                        delete(cm.closed, inKey)
                }</span>

                <span class="cov8" title="1">if circuit.HasKeystone() </span><span class="cov8" title="1">{
                        delete(cm.opened, circuit.OutKey())
                        cm.removeCircuitFromHashIndex(circuit)
                }</span>

                <span class="cov8" title="1">removedCircuits[inKey] = circuit</span>
        }
        <span class="cov8" title="1">cm.mtx.Unlock()

        err := kvdb.Batch(cm.cfg.DB, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                for _, circuit := range removedCircuits </span><span class="cov8" title="1">{
                        // If this htlc made it to an outgoing link, load the
                        // keystone bucket from which we will remove the
                        // outgoing circuit key.
                        if circuit.HasKeystone() </span><span class="cov8" title="1">{
                                keystoneBkt := tx.ReadWriteBucket(circuitKeystoneKey)
                                if keystoneBkt == nil </span><span class="cov0" title="0">{
                                        return ErrCorruptedCircuitMap
                                }</span>

                                <span class="cov8" title="1">outKey := circuit.OutKey()

                                err := keystoneBkt.Delete(outKey.Bytes())
                                if err != nil </span><span class="cov0" title="0">{
                                        return err
                                }</span>
                        }

                        // Remove the circuit itself based on the incoming
                        // circuit key.
                        <span class="cov8" title="1">circuitBkt := tx.ReadWriteBucket(circuitAddKey)
                        if circuitBkt == nil </span><span class="cov0" title="0">{
                                return ErrCorruptedCircuitMap
                        }</span>

                        <span class="cov8" title="1">inKey := circuit.InKey()
                        if err := circuitBkt.Delete(inKey.Bytes()); err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }

                <span class="cov8" title="1">return nil</span>
        })

        // Return if the write succeeded.
        <span class="cov8" title="1">if err == nil </span><span class="cov8" title="1">{
                return nil
        }</span>

        // If the persistent changes failed, restore the circuit map to it's
        // previous state.
        <span class="cov0" title="0">cm.mtx.Lock()
        for inKey, circuit := range removedCircuits </span><span class="cov0" title="0">{
                cm.pending[inKey] = circuit

                if _, ok := closingCircuits[inKey]; ok </span><span class="cov0" title="0">{
                        cm.closed[inKey] = struct{}{}
                }</span>

                <span class="cov0" title="0">if circuit.HasKeystone() </span><span class="cov0" title="0">{
                        cm.opened[circuit.OutKey()] = circuit
                        cm.addCircuitToHashIndex(circuit)
                }</span>
        }
        <span class="cov0" title="0">cm.mtx.Unlock()

        return err</span>
}

// removeCircuitFromHashIndex removes the given circuit from the hash index,
// pruning any unnecessary memory optimistically.
func (cm *circuitMap) removeCircuitFromHashIndex(c *PaymentCircuit) <span class="cov8" title="1">{
        // Locate bucket containing this circuit's payment hashes.
        circuitsWithHash, ok := cm.hashIndex[c.PaymentHash]
        if !ok </span><span class="cov0" title="0">{
                return
        }</span>

        <span class="cov8" title="1">outKey := c.OutKey()

        // Remove this circuit from the set of circuitsWithHash.
        delete(circuitsWithHash, outKey)

        // Prune the payment hash bucket if no other entries remain.
        if len(circuitsWithHash) == 0 </span><span class="cov8" title="1">{
                delete(cm.hashIndex, c.PaymentHash)
        }</span>
}

// NumPending returns the number of active circuits added to the circuit map.
func (cm *circuitMap) NumPending() int <span class="cov8" title="1">{
        cm.mtx.RLock()
        defer cm.mtx.RUnlock()

        return len(cm.pending)
}</span>

// NumOpen returns the number of circuits that have been opened by way of
// setting their keystones. This is the number of HTLCs that are waiting for a
// settle/fail response from a remote peer.
func (cm *circuitMap) NumOpen() int <span class="cov8" title="1">{
        cm.mtx.RLock()
        defer cm.mtx.RUnlock()

        return len(cm.opened)
}</span>
</pre>
		
		<pre class="file" id="file2" style="display: none">package htlcswitch

import (
        "bytes"
        "encoding/binary"
        "errors"
        "fmt"
        "sync"
        "sync/atomic"

        sphinx "github.com/lightningnetwork/lightning-onion"
        "github.com/lightningnetwork/lnd/chainntnfs"
        "github.com/lightningnetwork/lnd/kvdb"
)

const (
        // defaultDbDirectory is the default directory where our decayed log
        // will store our (sharedHash, CLTV) key-value pairs.
        defaultDbDirectory = "sharedhashes"
)

var (
        // sharedHashBucket is a bucket which houses the first HashPrefixSize
        // bytes of a received HTLC's hashed shared secret as the key and the HTLC's
        // CLTV expiry as the value.
        sharedHashBucket = []byte("shared-hash")

        // batchReplayBucket is a bucket that maps batch identifiers to
        // serialized ReplaySets. This is used to give idempotency in the event
        // that a batch is processed more than once.
        batchReplayBucket = []byte("batch-replay")
)

var (
        // ErrDecayedLogInit is used to indicate a decayed log failed to create
        // the proper bucketing structure on startup.
        ErrDecayedLogInit = errors.New("unable to initialize decayed log")

        // ErrDecayedLogCorrupted signals that the anticipated bucketing
        // structure has diverged since initialization.
        ErrDecayedLogCorrupted = errors.New("decayed log structure corrupted")
)

// NewBoltBackendCreator returns a function that creates a new bbolt backend for
// the decayed logs database.
func NewBoltBackendCreator(dbPath,
        dbFileName string) func(boltCfg *kvdb.BoltConfig) (kvdb.Backend, error) <span class="cov8" title="1">{

        return func(boltCfg *kvdb.BoltConfig) (kvdb.Backend, error) </span><span class="cov8" title="1">{
                cfg := &amp;kvdb.BoltBackendConfig{
                        DBPath:            dbPath,
                        DBFileName:        dbFileName,
                        NoFreelistSync:    boltCfg.NoFreelistSync,
                        AutoCompact:       boltCfg.AutoCompact,
                        AutoCompactMinAge: boltCfg.AutoCompactMinAge,
                        DBTimeout:         boltCfg.DBTimeout,
                }

                // Use default path for log database.
                if dbPath == "" </span><span class="cov0" title="0">{
                        cfg.DBPath = defaultDbDirectory
                }</span>

                <span class="cov8" title="1">db, err := kvdb.GetBoltBackend(cfg)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, fmt.Errorf("could not open boltdb: %w", err)
                }</span>

                <span class="cov8" title="1">return db, nil</span>
        }
}

// DecayedLog implements the PersistLog interface. It stores the first
// HashPrefixSize bytes of a sha256-hashed shared secret along with a node's
// CLTV value. It is a decaying log meaning there will be a garbage collector
// to collect entries which are expired according to their stored CLTV value
// and the current block height. DecayedLog wraps boltdb for simplicity and
// batches writes to the database to decrease write contention.
type DecayedLog struct {
        started int32 // To be used atomically.
        stopped int32 // To be used atomically.

        db kvdb.Backend

        notifier chainntnfs.ChainNotifier

        wg   sync.WaitGroup
        quit chan struct{}
}

// NewDecayedLog creates a new DecayedLog, which caches recently seen hash
// shared secrets. Entries are evicted as their cltv expires using block epochs
// from the given notifier.
func NewDecayedLog(db kvdb.Backend,
        notifier chainntnfs.ChainNotifier) *DecayedLog <span class="cov8" title="1">{

        return &amp;DecayedLog{
                db:       db,
                notifier: notifier,
                quit:     make(chan struct{}),
        }
}</span>

// Start opens the database we will be using to store hashed shared secrets.
// It also starts the garbage collector in a goroutine to remove stale
// database entries.
func (d *DecayedLog) Start() error <span class="cov8" title="1">{
        if !atomic.CompareAndSwapInt32(&amp;d.started, 0, 1) </span><span class="cov0" title="0">{
                return nil
        }</span>

        // Initialize the primary buckets used by the decayed log.
        <span class="cov8" title="1">if err := d.initBuckets(); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Start garbage collector.
        <span class="cov8" title="1">if d.notifier != nil </span><span class="cov8" title="1">{
                epochClient, err := d.notifier.RegisterBlockEpochNtfn(nil)
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("unable to register for epoch "+
                                "notifications: %v", err)
                }</span>

                <span class="cov8" title="1">d.wg.Add(1)
                go d.garbageCollector(epochClient)</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// initBuckets initializes the primary buckets used by the decayed log, namely
// the shared hash bucket, and batch replay
func (d *DecayedLog) initBuckets() error <span class="cov8" title="1">{
        return kvdb.Update(d.db, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                _, err := tx.CreateTopLevelBucket(sharedHashBucket)
                if err != nil </span><span class="cov0" title="0">{
                        return ErrDecayedLogInit
                }</span>

                <span class="cov8" title="1">_, err = tx.CreateTopLevelBucket(batchReplayBucket)
                if err != nil </span><span class="cov0" title="0">{
                        return ErrDecayedLogInit
                }</span>

                <span class="cov8" title="1">return nil</span>
        }, func() {<span class="cov8" title="1">}</span>)
}

// Stop halts the garbage collector and closes boltdb.
func (d *DecayedLog) Stop() error <span class="cov8" title="1">{
        log.Debugf("DecayedLog shutting down...")
        defer log.Debugf("DecayedLog shutdown complete")

        if !atomic.CompareAndSwapInt32(&amp;d.stopped, 0, 1) </span><span class="cov8" title="1">{
                return nil
        }</span>

        // Stop garbage collector.
        <span class="cov8" title="1">close(d.quit)

        d.wg.Wait()

        return nil</span>
}

// garbageCollector deletes entries from sharedHashBucket whose expiry height
// has already past. This function MUST be run as a goroutine.
func (d *DecayedLog) garbageCollector(epochClient *chainntnfs.BlockEpochEvent) <span class="cov8" title="1">{
        defer d.wg.Done()
        defer epochClient.Cancel()

        for </span><span class="cov8" title="1">{
                select </span>{
                case epoch, ok := &lt;-epochClient.Epochs:<span class="cov8" title="1">
                        if !ok </span><span class="cov0" title="0">{
                                // Block epoch was canceled, shutting down.
                                log.Infof("Block epoch canceled, " +
                                        "decaying hash log shutting down")
                                return
                        }</span>

                        // Perform a bout of garbage collection using the
                        // epoch's block height.
                        <span class="cov8" title="1">height := uint32(epoch.Height)
                        numExpired, err := d.gcExpiredHashes(height)
                        if err != nil </span><span class="cov0" title="0">{
                                log.Errorf("unable to expire hashes at "+
                                        "height=%d", height)
                        }</span>

                        <span class="cov8" title="1">if numExpired &gt; 0 </span><span class="cov8" title="1">{
                                log.Infof("Garbage collected %v shared "+
                                        "secret hashes at height=%v",
                                        numExpired, height)
                        }</span>

                case &lt;-d.quit:<span class="cov8" title="1">
                        // Received shutdown request.
                        log.Infof("Decaying hash log received " +
                                "shutdown request")
                        return</span>
                }
        }
}

// gcExpiredHashes purges the decaying log of all entries whose CLTV expires
// below the provided height.
func (d *DecayedLog) gcExpiredHashes(height uint32) (uint32, error) <span class="cov8" title="1">{
        var numExpiredHashes uint32

        err := kvdb.Batch(d.db, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                numExpiredHashes = 0

                // Grab the shared hash bucket
                sharedHashes := tx.ReadWriteBucket(sharedHashBucket)
                if sharedHashes == nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("sharedHashBucket " +
                                "is nil")
                }</span>

                <span class="cov8" title="1">var expiredCltv [][]byte
                if err := sharedHashes.ForEach(func(k, v []byte) error </span><span class="cov8" title="1">{
                        // Deserialize the CLTV value for this entry.
                        cltv := uint32(binary.BigEndian.Uint32(v))

                        if cltv &lt; height </span><span class="cov8" title="1">{
                                // This CLTV is expired. We must add it to an
                                // array which we'll loop over and delete every
                                // hash contained from the db.
                                expiredCltv = append(expiredCltv, k)
                                numExpiredHashes++
                        }</span>

                        <span class="cov8" title="1">return nil</span>
                }); err != nil <span class="cov0" title="0">{
                        return err
                }</span>

                // Delete every item in the array. This must
                // be done explicitly outside of the ForEach
                // function for safety reasons.
                <span class="cov8" title="1">for _, hash := range expiredCltv </span><span class="cov8" title="1">{
                        err := sharedHashes.Delete(hash)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }

                <span class="cov8" title="1">return nil</span>
        })
        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                return 0, err
        }</span>

        <span class="cov8" title="1">return numExpiredHashes, nil</span>
}

// Delete removes a &lt;shared secret hash, CLTV&gt; key-pair from the
// sharedHashBucket.
func (d *DecayedLog) Delete(hash *sphinx.HashPrefix) error <span class="cov8" title="1">{
        return kvdb.Batch(d.db, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                sharedHashes := tx.ReadWriteBucket(sharedHashBucket)
                if sharedHashes == nil </span><span class="cov0" title="0">{
                        return ErrDecayedLogCorrupted
                }</span>

                <span class="cov8" title="1">return sharedHashes.Delete(hash[:])</span>
        })
}

// Get retrieves the CLTV of a processed HTLC given the first 20 bytes of the
// Sha-256 hash of the shared secret.
func (d *DecayedLog) Get(hash *sphinx.HashPrefix) (uint32, error) <span class="cov8" title="1">{
        var value uint32

        err := kvdb.View(d.db, func(tx kvdb.RTx) error </span><span class="cov8" title="1">{
                // Grab the shared hash bucket which stores the mapping from
                // truncated sha-256 hashes of shared secrets to CLTV's.
                sharedHashes := tx.ReadBucket(sharedHashBucket)
                if sharedHashes == nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("sharedHashes is nil, could " +
                                "not retrieve CLTV value")
                }</span>

                // Retrieve the bytes which represents the CLTV
                <span class="cov8" title="1">valueBytes := sharedHashes.Get(hash[:])
                if valueBytes == nil </span><span class="cov8" title="1">{
                        return sphinx.ErrLogEntryNotFound
                }</span>

                // The first 4 bytes represent the CLTV, store it in value.
                <span class="cov8" title="1">value = uint32(binary.BigEndian.Uint32(valueBytes))

                return nil</span>
        }, func() <span class="cov8" title="1">{
                value = 0
        }</span>)
        <span class="cov8" title="1">if err != nil </span><span class="cov8" title="1">{
                return value, err
        }</span>

        <span class="cov8" title="1">return value, nil</span>
}

// Put stores a shared secret hash as the key and the CLTV as the value.
func (d *DecayedLog) Put(hash *sphinx.HashPrefix, cltv uint32) error <span class="cov8" title="1">{
        // Optimisitically serialize the cltv value into the scratch buffer.
        var scratch [4]byte
        binary.BigEndian.PutUint32(scratch[:], cltv)

        return kvdb.Batch(d.db, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                sharedHashes := tx.ReadWriteBucket(sharedHashBucket)
                if sharedHashes == nil </span><span class="cov0" title="0">{
                        return ErrDecayedLogCorrupted
                }</span>

                // Check to see if this hash prefix has been recorded before. If
                // a value is found, this packet is being replayed.
                <span class="cov8" title="1">valueBytes := sharedHashes.Get(hash[:])
                if valueBytes != nil </span><span class="cov0" title="0">{
                        return sphinx.ErrReplayedPacket
                }</span>

                <span class="cov8" title="1">return sharedHashes.Put(hash[:], scratch[:])</span>
        })
}

// PutBatch accepts a pending batch of hashed secret entries to write to disk.
// Each hashed secret is inserted with a corresponding time value, dictating
// when the entry will be evicted from the log.
// NOTE: This method enforces idempotency by writing the replay set obtained
// from the first attempt for a particular batch ID, and decoding the return
// value to subsequent calls. For the indices of the replay set to be aligned
// properly, the batch MUST be constructed identically to the first attempt,
// pruning will cause the indices to become invalid.
func (d *DecayedLog) PutBatch(b *sphinx.Batch) (*sphinx.ReplaySet, error) <span class="cov0" title="0">{
        // Since batched boltdb txns may be executed multiple times before
        // succeeding, we will create a new replay set for each invocation to
        // avoid any side-effects. If the txn is successful, this replay set
        // will be merged with the replay set computed during batch construction
        // to generate the complete replay set. If this batch was previously
        // processed, the replay set will be deserialized from disk.
        var replays *sphinx.ReplaySet
        if err := kvdb.Batch(d.db, func(tx kvdb.RwTx) error </span><span class="cov0" title="0">{
                sharedHashes := tx.ReadWriteBucket(sharedHashBucket)
                if sharedHashes == nil </span><span class="cov0" title="0">{
                        return ErrDecayedLogCorrupted
                }</span>

                // Load the batch replay bucket, which will be used to either
                // retrieve the result of previously processing this batch, or
                // to write the result of this operation.
                <span class="cov0" title="0">batchReplayBkt := tx.ReadWriteBucket(batchReplayBucket)
                if batchReplayBkt == nil </span><span class="cov0" title="0">{
                        return ErrDecayedLogCorrupted
                }</span>

                // Check for the existence of this batch's id in the replay
                // bucket. If a non-nil value is found, this indicates that we
                // have already processed this batch before. We deserialize the
                // resulting and return it to ensure calls to put batch are
                // idempotent.
                <span class="cov0" title="0">replayBytes := batchReplayBkt.Get(b.ID)
                if replayBytes != nil </span><span class="cov0" title="0">{
                        replays = sphinx.NewReplaySet()
                        return replays.Decode(bytes.NewReader(replayBytes))
                }</span>

                // The CLTV will be stored into scratch and then stored into the
                // sharedHashBucket.
                <span class="cov0" title="0">var scratch [4]byte

                replays = sphinx.NewReplaySet()
                err := b.ForEach(func(seqNum uint16, hashPrefix *sphinx.HashPrefix, cltv uint32) error </span><span class="cov0" title="0">{
                        // Retrieve the bytes which represents the CLTV
                        valueBytes := sharedHashes.Get(hashPrefix[:])
                        if valueBytes != nil </span><span class="cov0" title="0">{
                                replays.Add(seqNum)
                                return nil
                        }</span>

                        // Serialize the cltv value and write an entry keyed by
                        // the hash prefix.
                        <span class="cov0" title="0">binary.BigEndian.PutUint32(scratch[:], cltv)
                        return sharedHashes.Put(hashPrefix[:], scratch[:])</span>
                })
                <span class="cov0" title="0">if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                // Merge the replay set computed from checking the on-disk
                // entries with the in-batch replays computed during this
                // batch's construction.
                <span class="cov0" title="0">replays.Merge(b.ReplaySet)

                // Write the replay set under the batch identifier to the batch
                // replays bucket. This can be used during recovery to test (1)
                // that a particular batch was successfully processed and (2)
                // recover the indexes of the adds that were rejected as
                // replays.
                var replayBuf bytes.Buffer
                if err := replays.Encode(&amp;replayBuf); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov0" title="0">return batchReplayBkt.Put(b.ID, replayBuf.Bytes())</span>
        }); err != nil <span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov0" title="0">b.ReplaySet = replays
        b.IsCommitted = true

        return replays, nil</span>
}

// A compile time check to see if DecayedLog adheres to the PersistLog
// interface.
var _ sphinx.ReplayLog = (*DecayedLog)(nil)
</pre>
		
		<pre class="file" id="file3" style="display: none">package htlcswitch

import (
        "bytes"
        "fmt"

        sphinx "github.com/lightningnetwork/lightning-onion"
        "github.com/lightningnetwork/lnd/htlcswitch/hop"
        "github.com/lightningnetwork/lnd/lnwire"
)

// ClearTextError is an interface which is implemented by errors that occur
// when we know the underlying wire failure message. These errors are the
// opposite to opaque errors which are onion-encrypted blobs only understandable
// to the initiating node. ClearTextErrors are used when we fail a htlc at our
// node, or one of our initiated payments failed and we can decrypt the onion
// encrypted error fully.
type ClearTextError interface {
        error

        // WireMessage extracts a valid wire failure message from an internal
        // error which may contain additional metadata (which should not be
        // exposed to the network). This value may be nil in the case where
        // an unknown wire error is returned by one of our peers.
        WireMessage() lnwire.FailureMessage
}

// LinkError is an implementation of the ClearTextError interface which
// represents failures that occur on our incoming or outgoing link.
type LinkError struct {
        // msg returns the wire failure associated with the error.
        // This value should *not* be nil, because we should always
        // know the failure type for failures which occur at our own
        // node.
        msg lnwire.FailureMessage

        // FailureDetail enriches the wire error with additional information.
        FailureDetail
}

// NewLinkError returns a LinkError with the failure message provided.
// The failure message provided should *not* be nil, because we should
// always know the failure type for failures which occur at our own node.
func NewLinkError(msg lnwire.FailureMessage) *LinkError <span class="cov8" title="1">{
        return &amp;LinkError{msg: msg}
}</span>

// NewDetailedLinkError returns a link error that enriches a wire message with
// a failure detail.
func NewDetailedLinkError(msg lnwire.FailureMessage,
        detail FailureDetail) *LinkError <span class="cov8" title="1">{

        return &amp;LinkError{
                msg:           msg,
                FailureDetail: detail,
        }
}</span>

// WireMessage extracts a valid wire failure message from an internal
// error which may contain additional metadata (which should not be
// exposed to the network). This value should never be nil for LinkErrors,
// because we are the ones failing the htlc.
//
// Note this is part of the ClearTextError interface.
func (l *LinkError) WireMessage() lnwire.FailureMessage <span class="cov8" title="1">{
        return l.msg
}</span>

// Error returns the string representation of a link error.
//
// Note this is part of the ClearTextError interface.
func (l *LinkError) Error() string <span class="cov8" title="1">{
        // If the link error has no failure detail, return the wire message's
        // error.
        if l.FailureDetail == nil </span><span class="cov8" title="1">{
                return l.msg.Error()
        }</span>

        <span class="cov8" title="1">return l.FailureDetail.FailureString()</span>
}

// ForwardingError wraps an lnwire.FailureMessage in a struct that also
// includes the source of the error.
type ForwardingError struct {
        // FailureSourceIdx is the index of the node that sent the failure. With
        // this information, the dispatcher of a payment can modify their set of
        // candidate routes in response to the type of failure extracted. Index
        // zero is the self node.
        FailureSourceIdx int

        // msg is the wire message associated with the error. This value may
        // be nil in the case where we fail to decode failure message sent by
        // a peer.
        msg lnwire.FailureMessage
}

// WireMessage extracts a valid wire failure message from an internal
// error which may contain additional metadata (which should not be
// exposed to the network). This value may be nil in the case where
// an unknown wire error is returned by one of our peers.
//
// Note this is part of the ClearTextError interface.
func (f *ForwardingError) WireMessage() lnwire.FailureMessage <span class="cov8" title="1">{
        return f.msg
}</span>

// Error implements the built-in error interface. We use this method to allow
// the switch or any callers to insert additional context to the error message
// returned.
func (f *ForwardingError) Error() string <span class="cov0" title="0">{
        return fmt.Sprintf(
                "%v@%v", f.msg, f.FailureSourceIdx,
        )
}</span>

// NewForwardingError creates a new payment error which wraps a wire error
// with additional metadata.
func NewForwardingError(failure lnwire.FailureMessage,
        index int) *ForwardingError <span class="cov8" title="1">{

        return &amp;ForwardingError{
                FailureSourceIdx: index,
                msg:              failure,
        }
}</span>

// NewUnknownForwardingError returns a forwarding error which has a nil failure
// message. This constructor should only be used in the case where we cannot
// decode the failure we have received from a peer.
func NewUnknownForwardingError(index int) *ForwardingError <span class="cov8" title="1">{
        return &amp;ForwardingError{
                FailureSourceIdx: index,
        }
}</span>

// ErrorDecrypter is an interface that is used to decrypt the onion encrypted
// failure reason an extra out a well formed error.
type ErrorDecrypter interface {
        // DecryptError peels off each layer of onion encryption from the first
        // hop, to the source of the error. A fully populated
        // lnwire.FailureMessage is returned along with the source of the
        // error.
        DecryptError(lnwire.OpaqueReason) (*ForwardingError, error)
}

// UnknownEncrypterType is an error message used to signal that an unexpected
// EncrypterType was encountered during decoding.
type UnknownEncrypterType hop.EncrypterType

// Error returns a formatted error indicating the invalid EncrypterType.
func (e UnknownEncrypterType) Error() string <span class="cov0" title="0">{
        return fmt.Sprintf("unknown error encrypter type: %d", e)
}</span>

// OnionErrorDecrypter is the interface that provides onion level error
// decryption.
type OnionErrorDecrypter interface {
        // DecryptError attempts to decrypt the passed encrypted error response.
        // The onion failure is encrypted in backward manner, starting from the
        // node where error have occurred. As a result, in order to decrypt the
        // error we need get all shared secret and apply decryption in the
        // reverse order.
        DecryptError(encryptedData []byte) (*sphinx.DecryptedError, error)
}

// SphinxErrorDecrypter wraps the sphinx data SphinxErrorDecrypter and maps the
// returned errors to concrete lnwire.FailureMessage instances.
type SphinxErrorDecrypter struct {
        OnionErrorDecrypter
}

// DecryptError peels off each layer of onion encryption from the first hop, to
// the source of the error. A fully populated lnwire.FailureMessage is returned
// along with the source of the error.
//
// NOTE: Part of the ErrorDecrypter interface.
func (s *SphinxErrorDecrypter) DecryptError(reason lnwire.OpaqueReason) (
        *ForwardingError, error) <span class="cov8" title="1">{

        failure, err := s.OnionErrorDecrypter.DecryptError(reason)
        if err != nil </span><span class="cov8" title="1">{
                return nil, err
        }</span>

        // Decode the failure. If an error occurs, we leave the failure message
        // field nil.
        <span class="cov8" title="1">r := bytes.NewReader(failure.Message)
        failureMsg, err := lnwire.DecodeFailure(r, 0)
        if err != nil </span><span class="cov8" title="1">{
                return NewUnknownForwardingError(failure.SenderIdx), nil
        }</span>

        <span class="cov8" title="1">return NewForwardingError(failureMsg, failure.SenderIdx), nil</span>
}

// A compile time check to ensure ErrorDecrypter implements the Deobfuscator
// interface.
var _ ErrorDecrypter = (*SphinxErrorDecrypter)(nil)
</pre>
		
		<pre class="file" id="file4" style="display: none">package htlcswitch

// FailureDetail is an interface implemented by failures that occur on
// our incoming or outgoing link, or within the switch itself.
type FailureDetail interface {
        // FailureString returns the string representation of a failure
        // detail.
        FailureString() string
}

// OutgoingFailure is an enum which is used to enrich failures which occur in
// the switch or on our outgoing link with additional metadata.
type OutgoingFailure int

const (
        // OutgoingFailureNone is returned when the wire message contains
        // sufficient information.
        OutgoingFailureNone OutgoingFailure = iota

        // OutgoingFailureDecodeError indicates that we could not decode the
        // failure reason provided for a failed payment.
        OutgoingFailureDecodeError

        // OutgoingFailureLinkNotEligible indicates that a routing attempt was
        // made over a link that is not eligible for routing.
        OutgoingFailureLinkNotEligible

        // OutgoingFailureOnChainTimeout indicates that a payment had to be
        // timed out on chain before it got past the first hop by us or the
        // remote party.
        OutgoingFailureOnChainTimeout

        // OutgoingFailureHTLCExceedsMax is returned when a htlc exceeds our
        // policy's maximum htlc amount.
        OutgoingFailureHTLCExceedsMax

        // OutgoingFailureInsufficientBalance is returned when we cannot route a
        // htlc due to insufficient outgoing capacity.
        OutgoingFailureInsufficientBalance

        // OutgoingFailureCircularRoute is returned when an attempt is made
        // to forward a htlc through our node which arrives and leaves on the
        // same channel.
        OutgoingFailureCircularRoute

        // OutgoingFailureIncompleteForward is returned when we cancel an incomplete
        // forward.
        OutgoingFailureIncompleteForward

        // OutgoingFailureDownstreamHtlcAdd is returned when we fail to add a
        // downstream htlc to our outgoing link.
        OutgoingFailureDownstreamHtlcAdd

        // OutgoingFailureForwardsDisabled is returned when the switch is
        // configured to disallow forwards.
        OutgoingFailureForwardsDisabled
)

// FailureString returns the string representation of a failure detail.
//
// Note: it is part of the FailureDetail interface.
func (fd OutgoingFailure) FailureString() string <span class="cov8" title="1">{
        switch fd </span>{
        case OutgoingFailureNone:<span class="cov0" title="0">
                return "no failure detail"</span>

        case OutgoingFailureDecodeError:<span class="cov0" title="0">
                return "could not decode wire failure"</span>

        case OutgoingFailureLinkNotEligible:<span class="cov8" title="1">
                return "link not eligible"</span>

        case OutgoingFailureOnChainTimeout:<span class="cov0" title="0">
                return "payment was resolved on-chain, then canceled back"</span>

        case OutgoingFailureHTLCExceedsMax:<span class="cov8" title="1">
                return "htlc exceeds maximum policy amount"</span>

        case OutgoingFailureInsufficientBalance:<span class="cov8" title="1">
                return "insufficient bandwidth to route htlc"</span>

        case OutgoingFailureCircularRoute:<span class="cov8" title="1">
                return "same incoming and outgoing channel"</span>

        case OutgoingFailureIncompleteForward:<span class="cov8" title="1">
                return "failed after detecting incomplete forward"</span>

        case OutgoingFailureDownstreamHtlcAdd:<span class="cov0" title="0">
                return "could not add downstream htlc"</span>

        case OutgoingFailureForwardsDisabled:<span class="cov8" title="1">
                return "node configured to disallow forwards"</span>

        default:<span class="cov0" title="0">
                return "unknown failure detail"</span>
        }
}
</pre>
		
		<pre class="file" id="file5" style="display: none">package htlcswitch

import (
        "errors"
        "fmt"

        "github.com/lightningnetwork/lnd/graph/db/models"
)

// heldHtlcSet keeps track of outstanding intercepted forwards. It exposes
// several methods to manipulate the underlying map structure in a consistent
// way.
type heldHtlcSet struct {
        set map[models.CircuitKey]InterceptedForward
}

func newHeldHtlcSet() *heldHtlcSet <span class="cov8" title="1">{
        return &amp;heldHtlcSet{
                set: make(map[models.CircuitKey]InterceptedForward),
        }
}</span>

// forEach iterates over all held forwards and calls the given callback for each
// of them.
func (h *heldHtlcSet) forEach(cb func(InterceptedForward)) <span class="cov8" title="1">{
        for _, fwd := range h.set </span><span class="cov8" title="1">{
                cb(fwd)
        }</span>
}

// popAll calls the callback for each forward and removes them from the set.
func (h *heldHtlcSet) popAll(cb func(InterceptedForward)) <span class="cov8" title="1">{
        for _, fwd := range h.set </span><span class="cov8" title="1">{
                cb(fwd)
        }</span>

        <span class="cov8" title="1">h.set = make(map[models.CircuitKey]InterceptedForward)</span>
}

// popAutoFails calls the callback for each forward that has an auto-fail height
// equal or less then the specified pop height and removes them from the set.
func (h *heldHtlcSet) popAutoFails(height uint32, cb func(InterceptedForward)) <span class="cov8" title="1">{
        for key, fwd := range h.set </span><span class="cov8" title="1">{
                if uint32(fwd.Packet().AutoFailHeight) &gt; height </span><span class="cov8" title="1">{
                        continue</span>
                }

                <span class="cov8" title="1">cb(fwd)

                delete(h.set, key)</span>
        }
}

// pop returns the specified forward and removes it from the set.
func (h *heldHtlcSet) pop(key models.CircuitKey) (InterceptedForward, error) <span class="cov8" title="1">{
        intercepted, ok := h.set[key]
        if !ok </span><span class="cov8" title="1">{
                return nil, fmt.Errorf("fwd %v not found", key)
        }</span>

        <span class="cov8" title="1">delete(h.set, key)

        return intercepted, nil</span>
}

// exists tests whether the specified forward is part of the set.
func (h *heldHtlcSet) exists(key models.CircuitKey) bool <span class="cov8" title="1">{
        _, ok := h.set[key]

        return ok
}</span>

// push adds the specified forward to the set. An error is returned if the
// forward exists already.
func (h *heldHtlcSet) push(key models.CircuitKey,
        fwd InterceptedForward) error <span class="cov8" title="1">{

        if fwd == nil </span><span class="cov8" title="1">{
                return errors.New("nil fwd pushed")
        }</span>

        <span class="cov8" title="1">if h.exists(key) </span><span class="cov8" title="1">{
                return errors.New("htlc already exists in set")
        }</span>

        <span class="cov8" title="1">h.set[key] = fwd

        return nil</span>
}
</pre>
		
		<pre class="file" id="file6" style="display: none">package htlcswitch

import (
        "fmt"
        "strings"
        "sync"
        "time"

        "github.com/lightningnetwork/lnd/channeldb"
        "github.com/lightningnetwork/lnd/graph/db/models"
        "github.com/lightningnetwork/lnd/htlcswitch/hop"
        "github.com/lightningnetwork/lnd/lntypes"
        "github.com/lightningnetwork/lnd/lnwire"
        "github.com/lightningnetwork/lnd/subscribe"
)

// HtlcNotifier notifies clients of htlc forwards, failures and settles for
// htlcs that the switch handles. It takes subscriptions for its events and
// notifies them when htlc events occur. These are served on a best-effort
// basis; events are not persisted, delivery is not guaranteed (in the event
// of a crash in the switch, forward events may be lost) and some events may
// be replayed upon restart. Events consumed from this package should be
// de-duplicated by the htlc's unique combination of incoming+outgoing circuit
// and not relied upon for critical operations.
//
// The htlc notifier sends the following kinds of events:
// Forwarding Event:
// - Represents a htlc which is forwarded onward from our node.
// - Present for htlc forwards through our node and local sends.
//
// Link Failure Event:
//   - Indicates that a htlc has failed on our incoming or outgoing link,
//     with an incoming boolean which indicates where the failure occurred.
//   - Incoming link failures are present for failed attempts to pay one of
//     our invoices (insufficient amount or mpp timeout, for example) and for
//     forwards that we cannot decode to forward onwards.
//   - Outgoing link failures are present for forwards or local payments that
//     do not meet our outgoing link's policy (insufficient fees, for example)
//     and when we fail to forward the payment on (insufficient outgoing
//     capacity, or an unknown outgoing link).
//
// Forwarding Failure Event:
//   - Forwarding failures indicate that a htlc we forwarded has failed at
//     another node down the route.
//   - Present for local sends and htlc forwards which fail after they left
//     our node.
//
// Settle event:
//   - Settle events are present when a htlc which we added is settled through
//     the release of a preimage.
//   - Present for local receives, and successful local sends or forwards.
//
// Each htlc is identified by its incoming and outgoing circuit key. Htlcs,
// and their subsequent settles or fails, can be identified by the combination
// of incoming and outgoing circuits. Note that receives to our node will
// have a zero outgoing circuit key because the htlc terminates at our
// node, and sends from our node will have a zero incoming circuit key because
// the send originates at our node.
type HtlcNotifier struct {
        started sync.Once
        stopped sync.Once

        // now returns the current time, it is set in the htlcnotifier to allow
        // for timestamp mocking in tests.
        now func() time.Time

        ntfnServer *subscribe.Server
}

// NewHtlcNotifier creates a new HtlcNotifier which gets htlc forwarded,
// failed and settled events from links our node has established with peers
// and sends notifications to subscribing clients.
func NewHtlcNotifier(now func() time.Time) *HtlcNotifier <span class="cov8" title="1">{
        return &amp;HtlcNotifier{
                now:        now,
                ntfnServer: subscribe.NewServer(),
        }
}</span>

// Start starts the HtlcNotifier and all goroutines it needs to consume
// events and provide subscriptions to clients.
func (h *HtlcNotifier) Start() error <span class="cov8" title="1">{
        var err error
        h.started.Do(func() </span><span class="cov8" title="1">{
                log.Info("HtlcNotifier starting")
                err = h.ntfnServer.Start()
        }</span>)
        <span class="cov8" title="1">return err</span>
}

// Stop signals the notifier for a graceful shutdown.
func (h *HtlcNotifier) Stop() error <span class="cov8" title="1">{
        var err error
        h.stopped.Do(func() </span><span class="cov8" title="1">{
                log.Info("HtlcNotifier shutting down...")
                defer log.Debug("HtlcNotifier shutdown complete")

                if err = h.ntfnServer.Stop(); err != nil </span><span class="cov0" title="0">{
                        log.Warnf("error stopping htlc notifier: %v", err)
                }</span>
        })
        <span class="cov8" title="1">return err</span>
}

// SubscribeHtlcEvents returns a subscribe.Client that will receive updates
// any time the server is made aware of a new event.
func (h *HtlcNotifier) SubscribeHtlcEvents() (*subscribe.Client, error) <span class="cov8" title="1">{
        return h.ntfnServer.Subscribe()
}</span>

// HtlcKey uniquely identifies the htlc.
type HtlcKey struct {
        // IncomingCircuit is the channel an htlc id of the incoming htlc.
        IncomingCircuit models.CircuitKey

        // OutgoingCircuit is the channel and htlc id of the outgoing htlc.
        OutgoingCircuit models.CircuitKey
}

// String returns a string representation of a htlc key.
func (k HtlcKey) String() string <span class="cov0" title="0">{
        switch </span>{
        case k.IncomingCircuit.ChanID == hop.Source:<span class="cov0" title="0">
                return k.OutgoingCircuit.String()</span>

        case k.OutgoingCircuit.ChanID == hop.Exit:<span class="cov0" title="0">
                return k.IncomingCircuit.String()</span>

        default:<span class="cov0" title="0">
                return fmt.Sprintf("%v -&gt; %v", k.IncomingCircuit,
                        k.OutgoingCircuit)</span>
        }
}

// HtlcInfo provides the details of a htlc that our node has processed. For
// forwards, incoming and outgoing values are set, whereas sends and receives
// will only have outgoing or incoming details set.
type HtlcInfo struct {
        // IncomingTimelock is the time lock of the htlc on our incoming
        // channel.
        IncomingTimeLock uint32

        // OutgoingTimelock is the time lock the htlc on our outgoing channel.
        OutgoingTimeLock uint32

        // IncomingAmt is the amount of the htlc on our incoming channel.
        IncomingAmt lnwire.MilliSatoshi

        // OutgoingAmt is the amount of the htlc on our outgoing channel.
        OutgoingAmt lnwire.MilliSatoshi
}

// String returns a string representation of a htlc.
func (h HtlcInfo) String() string <span class="cov0" title="0">{
        var details []string

        // If the incoming information is not zero, as is the case for a send,
        // we include the incoming amount and timelock.
        if h.IncomingAmt != 0 || h.IncomingTimeLock != 0 </span><span class="cov0" title="0">{
                str := fmt.Sprintf("incoming amount: %v, "+
                        "incoming timelock: %v", h.IncomingAmt,
                        h.IncomingTimeLock)

                details = append(details, str)
        }</span>

        // If the outgoing information is not zero, as is the case for a
        // receive, we include the outgoing amount and timelock.
        <span class="cov0" title="0">if h.OutgoingAmt != 0 || h.OutgoingTimeLock != 0 </span><span class="cov0" title="0">{
                str := fmt.Sprintf("outgoing amount: %v, "+
                        "outgoing timelock: %v", h.OutgoingAmt,
                        h.OutgoingTimeLock)

                details = append(details, str)
        }</span>

        <span class="cov0" title="0">return strings.Join(details, ", ")</span>
}

// HtlcEventType represents the type of event that a htlc was part of.
type HtlcEventType int

const (
        // HtlcEventTypeSend represents a htlc that was part of a send from
        // our node.
        HtlcEventTypeSend HtlcEventType = iota

        // HtlcEventTypeReceive represents a htlc that was part of a receive
        // to our node.
        HtlcEventTypeReceive

        // HtlcEventTypeForward represents a htlc that was forwarded through
        // our node.
        HtlcEventTypeForward
)

// String returns a string representation of a htlc event type.
func (h HtlcEventType) String() string <span class="cov0" title="0">{
        switch h </span>{
        case HtlcEventTypeSend:<span class="cov0" title="0">
                return "send"</span>

        case HtlcEventTypeReceive:<span class="cov0" title="0">
                return "receive"</span>

        case HtlcEventTypeForward:<span class="cov0" title="0">
                return "forward"</span>

        default:<span class="cov0" title="0">
                return "unknown"</span>
        }
}

// ForwardingEvent represents a htlc that was forwarded onwards from our node.
// Sends which originate from our node will report forward events with zero
// incoming circuits in their htlc key.
type ForwardingEvent struct {
        // HtlcKey uniquely identifies the htlc, and can be used to match the
        // forwarding event with subsequent settle/fail events.
        HtlcKey

        // HtlcInfo contains details about the htlc.
        HtlcInfo

        // HtlcEventType classifies the event as part of a local send or
        // receive, or as part of a forward.
        HtlcEventType

        // Timestamp is the time when this htlc was forwarded.
        Timestamp time.Time
}

// LinkFailEvent describes a htlc that failed on our incoming or outgoing
// link. The incoming bool is true for failures on incoming links, and false
// for failures on outgoing links. The failure reason is provided by a lnwire
// failure message which is enriched with a failure detail in the cases where
// the wire failure message does not contain full information about the
// failure.
type LinkFailEvent struct {
        // HtlcKey uniquely identifies the htlc.
        HtlcKey

        // HtlcInfo contains details about the htlc.
        HtlcInfo

        // HtlcEventType classifies the event as part of a local send or
        // receive, or as part of a forward.
        HtlcEventType

        // LinkError is the reason that we failed the htlc.
        LinkError *LinkError

        // Incoming is true if the htlc was failed on an incoming link.
        // If it failed on the outgoing link, it is false.
        Incoming bool

        // Timestamp is the time when the link failure occurred.
        Timestamp time.Time
}

// ForwardingFailEvent represents a htlc failure which occurred down the line
// after we forwarded a htlc onwards. An error is not included in this event
// because errors returned down the route are encrypted. HtlcInfo is not
// reliably available for forwarding failures, so it is omitted. These events
// should be matched with their corresponding forward event to obtain this
// information.
type ForwardingFailEvent struct {
        // HtlcKey uniquely identifies the htlc, and can be used to match the
        // htlc with its corresponding forwarding event.
        HtlcKey

        // HtlcEventType classifies the event as part of a local send or
        // receive, or as part of a forward.
        HtlcEventType

        // Timestamp is the time when the forwarding failure was received.
        Timestamp time.Time
}

// SettleEvent represents a htlc that was settled. HtlcInfo is not reliably
// available for forwarding failures, so it is omitted. These events should
// be matched with corresponding forward events or invoices (for receives)
// to obtain additional information about the htlc.
type SettleEvent struct {
        // HtlcKey uniquely identifies the htlc, and can be used to match
        // forwards with their corresponding forwarding event.
        HtlcKey

        // Preimage that was released for settling the htlc.
        Preimage lntypes.Preimage

        // HtlcEventType classifies the event as part of a local send or
        // receive, or as part of a forward.
        HtlcEventType

        // Timestamp is the time when this htlc was settled.
        Timestamp time.Time
}

type FinalHtlcEvent struct {
        CircuitKey

        Settled bool

        // Offchain is indicating whether the htlc was resolved off-chain.
        Offchain bool

        // Timestamp is the time when this htlc was settled.
        Timestamp time.Time
}

// NotifyForwardingEvent notifies the HtlcNotifier than a htlc has been
// forwarded.
//
// Note this is part of the htlcNotifier interface.
func (h *HtlcNotifier) NotifyForwardingEvent(key HtlcKey, info HtlcInfo,
        eventType HtlcEventType) <span class="cov8" title="1">{

        event := &amp;ForwardingEvent{
                HtlcKey:       key,
                HtlcInfo:      info,
                HtlcEventType: eventType,
                Timestamp:     h.now(),
        }

        log.Tracef("Notifying forward event: %v over %v, %v", eventType, key,
                info)

        if err := h.ntfnServer.SendUpdate(event); err != nil </span><span class="cov0" title="0">{
                log.Warnf("Unable to send forwarding event: %v", err)
        }</span>
}

// NotifyLinkFailEvent notifies that a htlc has failed on our incoming
// or outgoing link.
//
// Note this is part of the htlcNotifier interface.
func (h *HtlcNotifier) NotifyLinkFailEvent(key HtlcKey, info HtlcInfo,
        eventType HtlcEventType, linkErr *LinkError, incoming bool) <span class="cov8" title="1">{

        event := &amp;LinkFailEvent{
                HtlcKey:       key,
                HtlcInfo:      info,
                HtlcEventType: eventType,
                LinkError:     linkErr,
                Incoming:      incoming,
                Timestamp:     h.now(),
        }

        log.Tracef("Notifying link failure event: %v over %v, %v", eventType,
                key, info)

        if err := h.ntfnServer.SendUpdate(event); err != nil </span><span class="cov0" title="0">{
                log.Warnf("Unable to send link fail event: %v", err)
        }</span>
}

// NotifyForwardingFailEvent notifies the HtlcNotifier that a htlc we
// forwarded has failed down the line.
//
// Note this is part of the htlcNotifier interface.
func (h *HtlcNotifier) NotifyForwardingFailEvent(key HtlcKey,
        eventType HtlcEventType) <span class="cov8" title="1">{

        event := &amp;ForwardingFailEvent{
                HtlcKey:       key,
                HtlcEventType: eventType,
                Timestamp:     h.now(),
        }

        log.Tracef("Notifying forwarding failure event: %v over %v", eventType,
                key)

        if err := h.ntfnServer.SendUpdate(event); err != nil </span><span class="cov0" title="0">{
                log.Warnf("Unable to send forwarding fail event: %v", err)
        }</span>
}

// NotifySettleEvent notifies the HtlcNotifier that a htlc that we committed
// to as part of a forward or a receive to our node has been settled.
//
// Note this is part of the htlcNotifier interface.
func (h *HtlcNotifier) NotifySettleEvent(key HtlcKey,
        preimage lntypes.Preimage, eventType HtlcEventType) <span class="cov8" title="1">{

        event := &amp;SettleEvent{
                HtlcKey:       key,
                Preimage:      preimage,
                HtlcEventType: eventType,
                Timestamp:     h.now(),
        }

        log.Tracef("Notifying settle event: %v over %v", eventType, key)

        if err := h.ntfnServer.SendUpdate(event); err != nil </span><span class="cov0" title="0">{
                log.Warnf("Unable to send settle event: %v", err)
        }</span>
}

// NotifyFinalHtlcEvent notifies the HtlcNotifier that the final outcome for an
// htlc has been determined.
//
// Note this is part of the htlcNotifier interface.
func (h *HtlcNotifier) NotifyFinalHtlcEvent(key models.CircuitKey,
        info channeldb.FinalHtlcInfo) <span class="cov8" title="1">{

        event := &amp;FinalHtlcEvent{
                CircuitKey: key,
                Settled:    info.Settled,
                Offchain:   info.Offchain,
                Timestamp:  h.now(),
        }

        log.Tracef("Notifying final settle event: %v", key)

        if err := h.ntfnServer.SendUpdate(event); err != nil </span><span class="cov0" title="0">{
                log.Warnf("Unable to send settle event: %v", err)
        }</span>
}

// newHtlc key returns a htlc key for the packet provided. If the packet
// has a zero incoming channel ID, the packet is for one of our own sends,
// which has the payment id stashed in the incoming htlc id. If this is the
// case, we replace the incoming htlc id with zero so that the notifier
// consistently reports zero circuit keys for events that terminate or
// originate at our node.
func newHtlcKey(pkt *htlcPacket) HtlcKey <span class="cov8" title="1">{
        htlcKey := HtlcKey{
                IncomingCircuit: models.CircuitKey{
                        ChanID: pkt.incomingChanID,
                        HtlcID: pkt.incomingHTLCID,
                },
                OutgoingCircuit: CircuitKey{
                        ChanID: pkt.outgoingChanID,
                        HtlcID: pkt.outgoingHTLCID,
                },
        }

        // If the packet has a zero incoming channel ID, it is a send that was
        // initiated at our node. If this is the case, our internal pid is in
        // the incoming htlc ID, so we overwrite it with 0 for notification
        // purposes.
        if pkt.incomingChanID == hop.Source </span><span class="cov8" title="1">{
                htlcKey.IncomingCircuit.HtlcID = 0
        }</span>

        <span class="cov8" title="1">return htlcKey</span>
}

// newHtlcInfo returns HtlcInfo for the packet provided.
func newHtlcInfo(pkt *htlcPacket) HtlcInfo <span class="cov8" title="1">{
        return HtlcInfo{
                IncomingTimeLock: pkt.incomingTimeout,
                OutgoingTimeLock: pkt.outgoingTimeout,
                IncomingAmt:      pkt.incomingAmount,
                OutgoingAmt:      pkt.amount,
        }
}</span>

// getEventType returns the htlc type based on the fields set in the htlc
// packet. Sends that originate at our node have the source (zero) incoming
// channel ID. Receives to our node have the exit (zero) outgoing channel ID
// and forwards have both fields set.
func getEventType(pkt *htlcPacket) HtlcEventType <span class="cov8" title="1">{
        switch </span>{
        case pkt.incomingChanID == hop.Source:<span class="cov8" title="1">
                return HtlcEventTypeSend</span>

        case pkt.outgoingChanID == hop.Exit:<span class="cov8" title="1">
                return HtlcEventTypeReceive</span>

        default:<span class="cov8" title="1">
                return HtlcEventTypeForward</span>
        }
}
</pre>
		
		<pre class="file" id="file7" style="display: none">package htlcswitch

import (
        "crypto/sha256"
        "fmt"
        "sync"
        "sync/atomic"

        "github.com/go-errors/errors"
        "github.com/lightningnetwork/lnd/chainntnfs"
        "github.com/lightningnetwork/lnd/fn/v2"
        "github.com/lightningnetwork/lnd/graph/db/models"
        "github.com/lightningnetwork/lnd/htlcswitch/hop"
        "github.com/lightningnetwork/lnd/lntypes"
        "github.com/lightningnetwork/lnd/lnutils"
        "github.com/lightningnetwork/lnd/lnwire"
)

var (
        // ErrFwdNotExists is an error returned when the caller tries to resolve
        // a forward that doesn't exist anymore.
        ErrFwdNotExists = errors.New("forward does not exist")

        // ErrUnsupportedFailureCode when processing of an unsupported failure
        // code is attempted.
        ErrUnsupportedFailureCode = errors.New("unsupported failure code")

        errBlockStreamStopped = errors.New("block epoch stream stopped")
)

// InterceptableSwitch is an implementation of ForwardingSwitch interface.
// This implementation is used like a proxy that wraps the switch and
// intercepts forward requests. A reference to the Switch is held in order
// to communicate back the interception result where the options are:
// Resume - forwards the original request to the switch as is.
// Settle - routes UpdateFulfillHTLC to the originating link.
// Fail - routes UpdateFailHTLC to the originating link.
type InterceptableSwitch struct {
        started atomic.Bool
        stopped atomic.Bool

        // htlcSwitch is the underline switch
        htlcSwitch *Switch

        // intercepted is where we stream all intercepted packets coming from
        // the switch.
        intercepted chan *interceptedPackets

        // resolutionChan is where we stream all responses coming from the
        // interceptor client.
        resolutionChan chan *fwdResolution

        onchainIntercepted chan InterceptedForward

        // interceptorRegistration is a channel that we use to synchronize
        // client connect and disconnect.
        interceptorRegistration chan ForwardInterceptor

        // requireInterceptor indicates whether processing should block if no
        // interceptor is connected.
        requireInterceptor bool

        // interceptor is the handler for intercepted packets.
        interceptor ForwardInterceptor

        // heldHtlcSet keeps track of outstanding intercepted forwards.
        heldHtlcSet *heldHtlcSet

        // cltvRejectDelta defines the number of blocks before the expiry of the
        // htlc where we no longer intercept it and instead cancel it back.
        cltvRejectDelta uint32

        // cltvInterceptDelta defines the number of blocks before the expiry of
        // the htlc where we don't intercept anymore. This value must be greater
        // than CltvRejectDelta, because we don't want to offer htlcs to the
        // interceptor client for which there is no time left to resolve them
        // anymore.
        cltvInterceptDelta uint32

        // notifier is an instance of a chain notifier that we'll use to signal
        // the switch when a new block has arrived.
        notifier chainntnfs.ChainNotifier

        // blockEpochStream is an active block epoch event stream backed by an
        // active ChainNotifier instance. This will be used to retrieve the
        // latest height of the chain.
        blockEpochStream *chainntnfs.BlockEpochEvent

        // currentHeight is the currently best known height.
        currentHeight int32

        wg   sync.WaitGroup
        quit chan struct{}
}

type interceptedPackets struct {
        packets  []*htlcPacket
        linkQuit &lt;-chan struct{}
        isReplay bool
}

// FwdAction defines the various resolution types.
type FwdAction int

const (
        // FwdActionResume forwards the intercepted packet to the switch.
        FwdActionResume FwdAction = iota

        // FwdActionSettle settles the intercepted packet with a preimage.
        FwdActionSettle

        // FwdActionFail fails the intercepted packet back to the sender.
        FwdActionFail

        // FwdActionResumeModified forwards the intercepted packet to the switch
        // with modifications.
        FwdActionResumeModified
)

// FwdResolution defines the action to be taken on an intercepted packet.
type FwdResolution struct {
        // Key is the incoming circuit key of the htlc.
        Key models.CircuitKey

        // Action is the action to take on the intercepted htlc.
        Action FwdAction

        // Preimage is the preimage that is to be used for settling if Action is
        // FwdActionSettle.
        Preimage lntypes.Preimage

        // InAmountMsat is the amount that is to be used for validating if
        // Action is FwdActionResumeModified.
        InAmountMsat fn.Option[lnwire.MilliSatoshi]

        // OutAmountMsat is the amount that is to be used for forwarding if
        // Action is FwdActionResumeModified.
        OutAmountMsat fn.Option[lnwire.MilliSatoshi]

        // OutWireCustomRecords is the custom records that are to be used for
        // forwarding if Action is FwdActionResumeModified.
        OutWireCustomRecords fn.Option[lnwire.CustomRecords]

        // FailureMessage is the encrypted failure message that is to be passed
        // back to the sender if action is FwdActionFail.
        FailureMessage []byte

        // FailureCode is the failure code that is to be passed back to the
        // sender if action is FwdActionFail.
        FailureCode lnwire.FailCode
}

type fwdResolution struct {
        resolution *FwdResolution
        errChan    chan error
}

// InterceptableSwitchConfig contains the configuration of InterceptableSwitch.
type InterceptableSwitchConfig struct {
        // Switch is a reference to the actual switch implementation that
        // packets get sent to on resume.
        Switch *Switch

        // Notifier is an instance of a chain notifier that we'll use to signal
        // the switch when a new block has arrived.
        Notifier chainntnfs.ChainNotifier

        // CltvRejectDelta defines the number of blocks before the expiry of the
        // htlc where we auto-fail an intercepted htlc to prevent channel
        // force-closure.
        CltvRejectDelta uint32

        // CltvInterceptDelta defines the number of blocks before the expiry of
        // the htlc where we don't intercept anymore. This value must be greater
        // than CltvRejectDelta, because we don't want to offer htlcs to the
        // interceptor client for which there is no time left to resolve them
        // anymore.
        CltvInterceptDelta uint32

        // RequireInterceptor indicates whether processing should block if no
        // interceptor is connected.
        RequireInterceptor bool
}

// NewInterceptableSwitch returns an instance of InterceptableSwitch.
func NewInterceptableSwitch(cfg *InterceptableSwitchConfig) (
        *InterceptableSwitch, error) <span class="cov8" title="1">{

        if cfg.CltvInterceptDelta &lt;= cfg.CltvRejectDelta </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("cltv intercept delta %v not greater "+
                        "than cltv reject delta %v",
                        cfg.CltvInterceptDelta, cfg.CltvRejectDelta)
        }</span>

        <span class="cov8" title="1">return &amp;InterceptableSwitch{
                htlcSwitch:              cfg.Switch,
                intercepted:             make(chan *interceptedPackets),
                onchainIntercepted:      make(chan InterceptedForward),
                interceptorRegistration: make(chan ForwardInterceptor),
                heldHtlcSet:             newHeldHtlcSet(),
                resolutionChan:          make(chan *fwdResolution),
                requireInterceptor:      cfg.RequireInterceptor,
                cltvRejectDelta:         cfg.CltvRejectDelta,
                cltvInterceptDelta:      cfg.CltvInterceptDelta,
                notifier:                cfg.Notifier,

                quit: make(chan struct{}),
        }, nil</span>
}

// SetInterceptor sets the ForwardInterceptor to be used. A nil argument
// unregisters the current interceptor.
func (s *InterceptableSwitch) SetInterceptor(
        interceptor ForwardInterceptor) <span class="cov8" title="1">{

        // Synchronize setting the handler with the main loop to prevent race
        // conditions.
        select </span>{
        case s.interceptorRegistration &lt;- interceptor:<span class="cov8" title="1"></span>

        case &lt;-s.quit:<span class="cov0" title="0"></span>
        }
}

func (s *InterceptableSwitch) Start() error <span class="cov8" title="1">{
        log.Info("InterceptableSwitch starting...")

        if s.started.Swap(true) </span><span class="cov0" title="0">{
                return fmt.Errorf("InterceptableSwitch started more than once")
        }</span>

        <span class="cov8" title="1">blockEpochStream, err := s.notifier.RegisterBlockEpochNtfn(nil)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">s.blockEpochStream = blockEpochStream

        s.wg.Add(1)
        go func() </span><span class="cov8" title="1">{
                defer s.wg.Done()

                err := s.run()
                if err != nil </span><span class="cov0" title="0">{
                        log.Errorf("InterceptableSwitch stopped: %v", err)
                }</span>
        }()

        <span class="cov8" title="1">log.Debug("InterceptableSwitch started")

        return nil</span>
}

func (s *InterceptableSwitch) Stop() error <span class="cov8" title="1">{
        log.Info("InterceptableSwitch shutting down...")

        if s.stopped.Swap(true) </span><span class="cov0" title="0">{
                return fmt.Errorf("InterceptableSwitch stopped more than once")
        }</span>

        <span class="cov8" title="1">close(s.quit)
        s.wg.Wait()

        // We need to check whether the start routine run and initialized the
        // `blockEpochStream`.
        if s.blockEpochStream != nil </span><span class="cov8" title="1">{
                s.blockEpochStream.Cancel()
        }</span>

        <span class="cov8" title="1">log.Debug("InterceptableSwitch shutdown complete")

        return nil</span>
}

func (s *InterceptableSwitch) run() error <span class="cov8" title="1">{
        // The block epoch stream will immediately stream the current height.
        // Read it out here.
        select </span>{
        case currentBlock, ok := &lt;-s.blockEpochStream.Epochs:<span class="cov8" title="1">
                if !ok </span><span class="cov0" title="0">{
                        return errBlockStreamStopped
                }</span>
                <span class="cov8" title="1">s.currentHeight = currentBlock.Height</span>

        case &lt;-s.quit:<span class="cov0" title="0">
                return nil</span>
        }

        <span class="cov8" title="1">log.Debugf("InterceptableSwitch running: height=%v, "+
                "requireInterceptor=%v", s.currentHeight, s.requireInterceptor)

        for </span><span class="cov8" title="1">{
                select </span>{
                // An interceptor registration or de-registration came in.
                case interceptor := &lt;-s.interceptorRegistration:<span class="cov8" title="1">
                        s.setInterceptor(interceptor)</span>

                case packets := &lt;-s.intercepted:<span class="cov8" title="1">
                        var notIntercepted []*htlcPacket
                        for _, p := range packets.packets </span><span class="cov8" title="1">{
                                intercepted, err := s.interceptForward(
                                        p, packets.isReplay,
                                )
                                if err != nil </span><span class="cov0" title="0">{
                                        return err
                                }</span>

                                <span class="cov8" title="1">if !intercepted </span><span class="cov8" title="1">{
                                        notIntercepted = append(
                                                notIntercepted, p,
                                        )
                                }</span>
                        }
                        <span class="cov8" title="1">err := s.htlcSwitch.ForwardPackets(
                                packets.linkQuit, notIntercepted...,
                        )
                        if err != nil </span><span class="cov0" title="0">{
                                log.Errorf("Cannot forward packets: %v", err)
                        }</span>

                case fwd := &lt;-s.onchainIntercepted:<span class="cov0" title="0">
                        // For on-chain interceptions, we don't know if it has
                        // already been offered before. This information is in
                        // the forwarding package which isn't easily accessible
                        // from contractcourt. It is likely though that it was
                        // already intercepted in the off-chain flow. And even
                        // if not, it is safe to signal replay so that we won't
                        // unexpectedly skip over this htlc.
                        if _, err := s.forward(fwd, true); err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>

                case res := &lt;-s.resolutionChan:<span class="cov8" title="1">
                        res.errChan &lt;- s.resolve(res.resolution)</span>

                case currentBlock, ok := &lt;-s.blockEpochStream.Epochs:<span class="cov8" title="1">
                        if !ok </span><span class="cov0" title="0">{
                                return errBlockStreamStopped
                        }</span>

                        <span class="cov8" title="1">s.currentHeight = currentBlock.Height

                        // A new block is appended. Fail any held htlcs that
                        // expire at this height to prevent channel force-close.
                        s.failExpiredHtlcs()</span>

                case &lt;-s.quit:<span class="cov8" title="1">
                        return nil</span>
                }
        }
}

func (s *InterceptableSwitch) failExpiredHtlcs() <span class="cov8" title="1">{
        s.heldHtlcSet.popAutoFails(
                uint32(s.currentHeight),
                func(fwd InterceptedForward) </span><span class="cov8" title="1">{
                        err := fwd.FailWithCode(
                                lnwire.CodeTemporaryChannelFailure,
                        )
                        if err != nil </span><span class="cov0" title="0">{
                                log.Errorf("Cannot fail packet: %v", err)
                        }</span>
                },
        )
}

func (s *InterceptableSwitch) sendForward(fwd InterceptedForward) <span class="cov8" title="1">{
        err := s.interceptor(fwd.Packet())
        if err != nil </span><span class="cov0" title="0">{
                // Only log the error. If we couldn't send the packet, we assume
                // that the interceptor will reconnect so that we can retry.
                log.Debugf("Interceptor cannot handle forward: %v", err)
        }</span>
}

func (s *InterceptableSwitch) setInterceptor(interceptor ForwardInterceptor) <span class="cov8" title="1">{
        s.interceptor = interceptor

        // Replay all currently held htlcs. When an interceptor is not required,
        // there may be none because they've been cleared after the previous
        // disconnect.
        if interceptor != nil </span><span class="cov8" title="1">{
                log.Debugf("Interceptor connected")

                s.heldHtlcSet.forEach(s.sendForward)

                return
        }</span>

        // The interceptor disconnects. If an interceptor is required, keep the
        // held htlcs.
        <span class="cov8" title="1">if s.requireInterceptor </span><span class="cov8" title="1">{
                log.Infof("Interceptor disconnected, retaining held packets")

                return
        }</span>

        // Interceptor is not required. Release held forwards.
        <span class="cov8" title="1">log.Infof("Interceptor disconnected, resolving held packets")

        s.heldHtlcSet.popAll(func(fwd InterceptedForward) </span><span class="cov8" title="1">{
                err := fwd.Resume()
                if err != nil </span><span class="cov0" title="0">{
                        log.Errorf("Failed to resume hold forward %v", err)
                }</span>
        })
}

// resolve processes a HTLC given the resolution type specified by the
// intercepting client.
func (s *InterceptableSwitch) resolve(res *FwdResolution) error <span class="cov8" title="1">{
        intercepted, err := s.heldHtlcSet.pop(res.Key)
        if err != nil </span><span class="cov8" title="1">{
                return err
        }</span>

        <span class="cov8" title="1">switch res.Action </span>{
        case FwdActionResume:<span class="cov8" title="1">
                return intercepted.Resume()</span>

        case FwdActionResumeModified:<span class="cov0" title="0">
                return intercepted.ResumeModified(
                        res.InAmountMsat, res.OutAmountMsat,
                        res.OutWireCustomRecords,
                )</span>

        case FwdActionSettle:<span class="cov8" title="1">
                return intercepted.Settle(res.Preimage)</span>

        case FwdActionFail:<span class="cov8" title="1">
                if len(res.FailureMessage) &gt; 0 </span><span class="cov8" title="1">{
                        return intercepted.Fail(res.FailureMessage)
                }</span>

                <span class="cov8" title="1">return intercepted.FailWithCode(res.FailureCode)</span>

        default:<span class="cov0" title="0">
                return fmt.Errorf("unrecognized action %v", res.Action)</span>
        }
}

// Resolve resolves an intercepted packet.
func (s *InterceptableSwitch) Resolve(res *FwdResolution) error <span class="cov8" title="1">{
        internalRes := &amp;fwdResolution{
                resolution: res,
                errChan:    make(chan error, 1),
        }

        select </span>{
        case s.resolutionChan &lt;- internalRes:<span class="cov8" title="1"></span>

        case &lt;-s.quit:<span class="cov0" title="0">
                return errors.New("switch shutting down")</span>
        }

        <span class="cov8" title="1">select </span>{
        case err := &lt;-internalRes.errChan:<span class="cov8" title="1">
                return err</span>

        case &lt;-s.quit:<span class="cov0" title="0">
                return errors.New("switch shutting down")</span>
        }
}

// ForwardPackets attempts to forward the batch of htlcs to a connected
// interceptor. If the interceptor signals the resume action, the htlcs are
// forwarded to the switch. The link's quit signal should be provided to allow
// cancellation of forwarding during link shutdown.
func (s *InterceptableSwitch) ForwardPackets(linkQuit &lt;-chan struct{},
        isReplay bool, packets ...*htlcPacket) error <span class="cov8" title="1">{

        // Synchronize with the main event loop. This should be light in the
        // case where there is no interceptor.
        select </span>{
        case s.intercepted &lt;- &amp;interceptedPackets{
                packets:  packets,
                linkQuit: linkQuit,
                isReplay: isReplay,
        }:<span class="cov8" title="1"></span>

        case &lt;-linkQuit:<span class="cov0" title="0">
                log.Debugf("Forward cancelled because link quit")</span>

        case &lt;-s.quit:<span class="cov0" title="0">
                return errors.New("interceptable switch quit")</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// ForwardPacket forwards a single htlc to the external interceptor.
func (s *InterceptableSwitch) ForwardPacket(
        fwd InterceptedForward) error <span class="cov0" title="0">{

        select </span>{
        case s.onchainIntercepted &lt;- fwd:<span class="cov0" title="0"></span>

        case &lt;-s.quit:<span class="cov0" title="0">
                return errors.New("interceptable switch quit")</span>
        }

        <span class="cov0" title="0">return nil</span>
}

// interceptForward forwards the packet to the external interceptor after
// checking the interception criteria.
func (s *InterceptableSwitch) interceptForward(packet *htlcPacket,
        isReplay bool) (bool, error) <span class="cov8" title="1">{

        switch htlc := packet.htlc.(type) </span>{
        case *lnwire.UpdateAddHTLC:<span class="cov8" title="1">
                // We are not interested in intercepting initiated payments.
                if packet.incomingChanID == hop.Source </span><span class="cov0" title="0">{
                        return false, nil
                }</span>

                <span class="cov8" title="1">intercepted := &amp;interceptedForward{
                        htlc:       htlc,
                        packet:     packet,
                        htlcSwitch: s.htlcSwitch,
                        autoFailHeight: int32(packet.incomingTimeout -
                                s.cltvRejectDelta),
                }

                // Handle forwards that are too close to expiry.
                handled, err := s.handleExpired(intercepted)
                if err != nil </span><span class="cov8" title="1">{
                        log.Errorf("Error handling intercepted htlc "+
                                "that expires too soon: circuit=%v, "+
                                "incoming_timeout=%v, err=%v",
                                packet.inKey(), packet.incomingTimeout, err)

                        // Return false so that the packet is offered as normal
                        // to the switch. This isn't ideal because interception
                        // may be configured as always-on and is skipped now.
                        // Returning true isn't great either, because the htlc
                        // will remain stuck and potentially force-close the
                        // channel. But in the end, we should never get here, so
                        // the actual return value doesn't matter that much.
                        return false, nil
                }</span>
                <span class="cov8" title="1">if handled </span><span class="cov8" title="1">{
                        return true, nil
                }</span>

                <span class="cov8" title="1">return s.forward(intercepted, isReplay)</span>

        default:<span class="cov8" title="1">
                return false, nil</span>
        }
}

// forward records the intercepted htlc and forwards it to the interceptor.
func (s *InterceptableSwitch) forward(
        fwd InterceptedForward, isReplay bool) (bool, error) <span class="cov8" title="1">{

        inKey := fwd.Packet().IncomingCircuit

        // Ignore already held htlcs.
        if s.heldHtlcSet.exists(inKey) </span><span class="cov0" title="0">{
                return true, nil
        }</span>

        // If there is no interceptor currently registered, configuration and packet
        // replay status determine how the packet is handled.
        <span class="cov8" title="1">if s.interceptor == nil </span><span class="cov8" title="1">{
                // Process normally if an interceptor is not required.
                if !s.requireInterceptor </span><span class="cov0" title="0">{
                        return false, nil
                }</span>

                // We are in interceptor-required mode. If this is a new packet, it is
                // still safe to fail back. The interceptor has never seen this packet
                // yet. This limits the backlog of htlcs when the interceptor is down.
                <span class="cov8" title="1">if !isReplay </span><span class="cov8" title="1">{
                        err := fwd.FailWithCode(
                                lnwire.CodeTemporaryChannelFailure,
                        )
                        if err != nil </span><span class="cov0" title="0">{
                                log.Errorf("Cannot fail packet: %v", err)
                        }</span>

                        <span class="cov8" title="1">return true, nil</span>
                }

                // This packet is a replay. It is not safe to fail back, because the
                // interceptor may still signal otherwise upon reconnect. Keep the
                // packet in the queue until then.
                <span class="cov8" title="1">if err := s.heldHtlcSet.push(inKey, fwd); err != nil </span><span class="cov0" title="0">{
                        return false, err
                }</span>

                <span class="cov8" title="1">return true, nil</span>
        }

        // There is an interceptor registered. We can forward the packet right now.
        // Hold it in the queue too to track what is outstanding.
        <span class="cov8" title="1">if err := s.heldHtlcSet.push(inKey, fwd); err != nil </span><span class="cov0" title="0">{
                return false, err
        }</span>

        <span class="cov8" title="1">s.sendForward(fwd)

        return true, nil</span>
}

// handleExpired checks that the htlc isn't too close to the channel
// force-close broadcast height. If it is, it is cancelled back.
func (s *InterceptableSwitch) handleExpired(fwd *interceptedForward) (
        bool, error) <span class="cov8" title="1">{

        height := uint32(s.currentHeight)
        if fwd.packet.incomingTimeout &gt;= height+s.cltvInterceptDelta </span><span class="cov8" title="1">{
                return false, nil
        }</span>

        <span class="cov8" title="1">log.Debugf("Interception rejected because htlc "+
                "expires too soon: circuit=%v, "+
                "height=%v, incoming_timeout=%v",
                fwd.packet.inKey(), height,
                fwd.packet.incomingTimeout)

        err := fwd.FailWithCode(
                lnwire.CodeExpiryTooSoon,
        )
        if err != nil </span><span class="cov8" title="1">{
                return false, err
        }</span>

        <span class="cov8" title="1">return true, nil</span>
}

// interceptedForward implements the InterceptedForward interface.
// It is passed from the switch to external interceptors that are interested
// in holding forwards and resolve them manually.
type interceptedForward struct {
        htlc           *lnwire.UpdateAddHTLC
        packet         *htlcPacket
        htlcSwitch     *Switch
        autoFailHeight int32
}

// Packet returns the intercepted htlc packet.
func (f *interceptedForward) Packet() InterceptedPacket <span class="cov8" title="1">{
        return InterceptedPacket{
                IncomingCircuit: models.CircuitKey{
                        ChanID: f.packet.incomingChanID,
                        HtlcID: f.packet.incomingHTLCID,
                },
                OutgoingChanID:       f.packet.outgoingChanID,
                Hash:                 f.htlc.PaymentHash,
                OutgoingExpiry:       f.htlc.Expiry,
                OutgoingAmount:       f.htlc.Amount,
                IncomingAmount:       f.packet.incomingAmount,
                IncomingExpiry:       f.packet.incomingTimeout,
                InOnionCustomRecords: f.packet.inOnionCustomRecords,
                OnionBlob:            f.htlc.OnionBlob,
                AutoFailHeight:       f.autoFailHeight,
                InWireCustomRecords:  f.packet.inWireCustomRecords,
        }
}</span>

// Resume resumes the default behavior as if the packet was not intercepted.
func (f *interceptedForward) Resume() error <span class="cov8" title="1">{
        // Forward to the switch. A link quit channel isn't needed, because we
        // are on a different thread now.
        return f.htlcSwitch.ForwardPackets(nil, f.packet)
}</span>

// ResumeModified resumes the default behavior with field modifications. The
// input amount (if provided) specifies that the value of the inbound HTLC
// should be interpreted differently from the on-chain amount during further
// validation. The presence of an output amount and/or custom records indicates
// that those values should be modified on the outgoing HTLC.
func (f *interceptedForward) ResumeModified(
        inAmountMsat fn.Option[lnwire.MilliSatoshi],
        outAmountMsat fn.Option[lnwire.MilliSatoshi],
        outWireCustomRecords fn.Option[lnwire.CustomRecords]) error <span class="cov0" title="0">{

        // Convert the optional custom records to the correct type and validate
        // them.
        validatedRecords, err := fn.MapOptionZ(
                outWireCustomRecords,
                func(cr lnwire.CustomRecords) fn.Result[lnwire.CustomRecords] </span><span class="cov0" title="0">{
                        if len(cr) == 0 </span><span class="cov0" title="0">{
                                return fn.Ok[lnwire.CustomRecords](nil)
                        }</span>

                        // Type cast and validate custom records.
                        <span class="cov0" title="0">err := cr.Validate()
                        if err != nil </span><span class="cov0" title="0">{
                                return fn.Err[lnwire.CustomRecords](
                                        fmt.Errorf("failed to validate "+
                                                "custom records: %w", err),
                                )
                        }</span>

                        <span class="cov0" title="0">return fn.Ok(cr)</span>
                },
        ).Unpack()
        <span class="cov0" title="0">if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to encode custom records: %w",
                        err)
        }</span>

        // Set the incoming amount, if it is provided, on the packet.
        <span class="cov0" title="0">inAmountMsat.WhenSome(func(amount lnwire.MilliSatoshi) </span><span class="cov0" title="0">{
                f.packet.incomingAmount = amount
        }</span>)

        // Modify the wire message contained in the packet.
        <span class="cov0" title="0">switch htlc := f.packet.htlc.(type) </span>{
        case *lnwire.UpdateAddHTLC:<span class="cov0" title="0">
                outAmountMsat.WhenSome(func(amount lnwire.MilliSatoshi) </span><span class="cov0" title="0">{
                        f.packet.amount = amount
                        htlc.Amount = amount
                }</span>)

                // Merge custom records with any validated records that were
                // added in the modify request, overwriting any existing values
                // with those supplied in the modifier API.
                <span class="cov0" title="0">htlc.CustomRecords = htlc.CustomRecords.MergedCopy(
                        validatedRecords,
                )</span>

        case *lnwire.UpdateFulfillHTLC:<span class="cov0" title="0">
                if len(validatedRecords) &gt; 0 </span><span class="cov0" title="0">{
                        htlc.CustomRecords = validatedRecords
                }</span>
        }

        <span class="cov0" title="0">log.Tracef("Forwarding packet %v", lnutils.SpewLogClosure(f.packet))

        // Forward to the switch. A link quit channel isn't needed, because we
        // are on a different thread now.
        return f.htlcSwitch.ForwardPackets(nil, f.packet)</span>
}

// Fail notifies the intention to Fail an existing hold forward with an
// encrypted failure reason.
func (f *interceptedForward) Fail(reason []byte) error <span class="cov8" title="1">{
        obfuscatedReason := f.packet.obfuscator.IntermediateEncrypt(reason)

        return f.resolve(&amp;lnwire.UpdateFailHTLC{
                Reason: obfuscatedReason,
        })
}</span>

// FailWithCode notifies the intention to fail an existing hold forward with the
// specified failure code.
func (f *interceptedForward) FailWithCode(code lnwire.FailCode) error <span class="cov8" title="1">{
        shaOnionBlob := func() [32]byte </span><span class="cov8" title="1">{
                return sha256.Sum256(f.htlc.OnionBlob[:])
        }</span>

        // Create a local failure.
        <span class="cov8" title="1">var failureMsg lnwire.FailureMessage

        switch code </span>{
        case lnwire.CodeInvalidOnionVersion:<span class="cov0" title="0">
                failureMsg = &amp;lnwire.FailInvalidOnionVersion{
                        OnionSHA256: shaOnionBlob(),
                }</span>

        case lnwire.CodeInvalidOnionHmac:<span class="cov0" title="0">
                failureMsg = &amp;lnwire.FailInvalidOnionHmac{
                        OnionSHA256: shaOnionBlob(),
                }</span>

        case lnwire.CodeInvalidOnionKey:<span class="cov8" title="1">
                failureMsg = &amp;lnwire.FailInvalidOnionKey{
                        OnionSHA256: shaOnionBlob(),
                }</span>

        case lnwire.CodeTemporaryChannelFailure:<span class="cov8" title="1">
                update := f.htlcSwitch.failAliasUpdate(
                        f.packet.incomingChanID, true,
                )
                if update == nil </span><span class="cov8" title="1">{
                        // Fallback to the original, non-alias behavior.
                        var err error
                        update, err = f.htlcSwitch.cfg.FetchLastChannelUpdate(
                                f.packet.incomingChanID,
                        )
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }

                <span class="cov8" title="1">failureMsg = lnwire.NewTemporaryChannelFailure(update)</span>

        case lnwire.CodeExpiryTooSoon:<span class="cov8" title="1">
                update, err := f.htlcSwitch.cfg.FetchLastChannelUpdate(
                        f.packet.incomingChanID,
                )
                if err != nil </span><span class="cov8" title="1">{
                        return err
                }</span>

                <span class="cov8" title="1">failureMsg = lnwire.NewExpiryTooSoon(*update)</span>

        default:<span class="cov0" title="0">
                return ErrUnsupportedFailureCode</span>
        }

        // Encrypt the failure for the first hop. This node will be the origin
        // of the failure.
        <span class="cov8" title="1">reason, err := f.packet.obfuscator.EncryptFirstHop(failureMsg)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("failed to encrypt failure reason %w", err)
        }</span>

        <span class="cov8" title="1">return f.resolve(&amp;lnwire.UpdateFailHTLC{
                Reason: reason,
        })</span>
}

// Settle forwards a settled packet to the switch.
func (f *interceptedForward) Settle(preimage lntypes.Preimage) error <span class="cov8" title="1">{
        if !preimage.Matches(f.htlc.PaymentHash) </span><span class="cov0" title="0">{
                return errors.New("preimage does not match hash")
        }</span>
        <span class="cov8" title="1">return f.resolve(&amp;lnwire.UpdateFulfillHTLC{
                PaymentPreimage: preimage,
        })</span>
}

// resolve is used for both Settle and Fail and forwards the message to the
// switch.
func (f *interceptedForward) resolve(message lnwire.Message) error <span class="cov8" title="1">{
        pkt := &amp;htlcPacket{
                incomingChanID: f.packet.incomingChanID,
                incomingHTLCID: f.packet.incomingHTLCID,
                outgoingChanID: f.packet.outgoingChanID,
                outgoingHTLCID: f.packet.outgoingHTLCID,
                isResolution:   true,
                circuit:        f.packet.circuit,
                htlc:           message,
                obfuscator:     f.packet.obfuscator,
                sourceRef:      f.packet.sourceRef,
        }
        return f.htlcSwitch.mailOrchestrator.Deliver(pkt.incomingChanID, pkt)
}</span>
</pre>
		
		<pre class="file" id="file8" style="display: none">package htlcswitch

import (
        "bytes"
        "context"
        crand "crypto/rand"
        "crypto/sha256"
        "errors"
        "fmt"
        prand "math/rand"
        "sync"
        "sync/atomic"
        "time"

        "github.com/btcsuite/btcd/btcutil"
        "github.com/btcsuite/btcd/wire"
        "github.com/btcsuite/btclog/v2"
        "github.com/lightningnetwork/lnd/channeldb"
        "github.com/lightningnetwork/lnd/contractcourt"
        "github.com/lightningnetwork/lnd/fn/v2"
        "github.com/lightningnetwork/lnd/graph/db/models"
        "github.com/lightningnetwork/lnd/htlcswitch/hodl"
        "github.com/lightningnetwork/lnd/htlcswitch/hop"
        "github.com/lightningnetwork/lnd/input"
        "github.com/lightningnetwork/lnd/invoices"
        "github.com/lightningnetwork/lnd/lnpeer"
        "github.com/lightningnetwork/lnd/lntypes"
        "github.com/lightningnetwork/lnd/lnutils"
        "github.com/lightningnetwork/lnd/lnwallet"
        "github.com/lightningnetwork/lnd/lnwallet/chainfee"
        "github.com/lightningnetwork/lnd/lnwire"
        "github.com/lightningnetwork/lnd/queue"
        "github.com/lightningnetwork/lnd/record"
        "github.com/lightningnetwork/lnd/ticker"
        "github.com/lightningnetwork/lnd/tlv"
)

func init() <span class="cov8" title="1">{
        prand.Seed(time.Now().UnixNano())
}</span>

const (
        // DefaultMaxOutgoingCltvExpiry is the maximum outgoing time lock that
        // the node accepts for forwarded payments. The value is relative to the
        // current block height. The reason to have a maximum is to prevent
        // funds getting locked up unreasonably long. Otherwise, an attacker
        // willing to lock its own funds too, could force the funds of this node
        // to be locked up for an indefinite (max int32) number of blocks.
        //
        // The value 2016 corresponds to on average two weeks worth of blocks
        // and is based on the maximum number of hops (20), the default CLTV
        // delta (40), and some extra margin to account for the other lightning
        // implementations and past lnd versions which used to have a default
        // CLTV delta of 144.
        DefaultMaxOutgoingCltvExpiry = 2016

        // DefaultMinLinkFeeUpdateTimeout represents the minimum interval in
        // which a link should propose to update its commitment fee rate.
        DefaultMinLinkFeeUpdateTimeout = 10 * time.Minute

        // DefaultMaxLinkFeeUpdateTimeout represents the maximum interval in
        // which a link should propose to update its commitment fee rate.
        DefaultMaxLinkFeeUpdateTimeout = 60 * time.Minute

        // DefaultMaxLinkFeeAllocation is the highest allocation we'll allow
        // a channel's commitment fee to be of its balance. This only applies to
        // the initiator of the channel.
        DefaultMaxLinkFeeAllocation float64 = 0.5
)

// ExpectedFee computes the expected fee for a given htlc amount. The value
// returned from this function is to be used as a sanity check when forwarding
// HTLC's to ensure that an incoming HTLC properly adheres to our propagated
// forwarding policy.
//
// TODO(roasbeef): also add in current available channel bandwidth, inverse
// func
func ExpectedFee(f models.ForwardingPolicy,
        htlcAmt lnwire.MilliSatoshi) lnwire.MilliSatoshi <span class="cov8" title="1">{

        return f.BaseFee + (htlcAmt*f.FeeRate)/1000000
}</span>

// ChannelLinkConfig defines the configuration for the channel link. ALL
// elements within the configuration MUST be non-nil for channel link to carry
// out its duties.
type ChannelLinkConfig struct {
        // FwrdingPolicy is the initial forwarding policy to be used when
        // deciding whether to forwarding incoming HTLC's or not. This value
        // can be updated with subsequent calls to UpdateForwardingPolicy
        // targeted at a given ChannelLink concrete interface implementation.
        FwrdingPolicy models.ForwardingPolicy

        // Circuits provides restricted access to the switch's circuit map,
        // allowing the link to open and close circuits.
        Circuits CircuitModifier

        // BestHeight returns the best known height.
        BestHeight func() uint32

        // ForwardPackets attempts to forward the batch of htlcs through the
        // switch. The function returns and error in case it fails to send one or
        // more packets. The link's quit signal should be provided to allow
        // cancellation of forwarding during link shutdown.
        ForwardPackets func(&lt;-chan struct{}, bool, ...*htlcPacket) error

        // DecodeHopIterators facilitates batched decoding of HTLC Sphinx onion
        // blobs, which are then used to inform how to forward an HTLC.
        //
        // NOTE: This function assumes the same set of readers and preimages
        // are always presented for the same identifier.
        DecodeHopIterators func([]byte, []hop.DecodeHopIteratorRequest) (
                []hop.DecodeHopIteratorResponse, error)

        // ExtractErrorEncrypter function is responsible for decoding HTLC
        // Sphinx onion blob, and creating onion failure obfuscator.
        ExtractErrorEncrypter hop.ErrorEncrypterExtracter

        // FetchLastChannelUpdate retrieves the latest routing policy for a
        // target channel. This channel will typically be the outgoing channel
        // specified when we receive an incoming HTLC.  This will be used to
        // provide payment senders our latest policy when sending encrypted
        // error messages.
        FetchLastChannelUpdate func(lnwire.ShortChannelID) (
                *lnwire.ChannelUpdate1, error)

        // Peer is a lightning network node with which we have the channel link
        // opened.
        Peer lnpeer.Peer

        // Registry is a sub-system which responsible for managing the invoices
        // in thread-safe manner.
        Registry InvoiceDatabase

        // PreimageCache is a global witness beacon that houses any new
        // preimages discovered by other links. We'll use this to add new
        // witnesses that we discover which will notify any sub-systems
        // subscribed to new events.
        PreimageCache contractcourt.WitnessBeacon

        // OnChannelFailure is a function closure that we'll call if the
        // channel failed for some reason. Depending on the severity of the
        // error, the closure potentially must force close this channel and
        // disconnect the peer.
        //
        // NOTE: The method must return in order for the ChannelLink to be able
        // to shut down properly.
        OnChannelFailure func(lnwire.ChannelID, lnwire.ShortChannelID,
                LinkFailureError)

        // UpdateContractSignals is a function closure that we'll use to update
        // outside sub-systems with this channel's latest ShortChannelID.
        UpdateContractSignals func(*contractcourt.ContractSignals) error

        // NotifyContractUpdate is a function closure that we'll use to update
        // the contractcourt and more specifically the ChannelArbitrator of the
        // latest channel state.
        NotifyContractUpdate func(*contractcourt.ContractUpdate) error

        // ChainEvents is an active subscription to the chain watcher for this
        // channel to be notified of any on-chain activity related to this
        // channel.
        ChainEvents *contractcourt.ChainEventSubscription

        // FeeEstimator is an instance of a live fee estimator which will be
        // used to dynamically regulate the current fee of the commitment
        // transaction to ensure timely confirmation.
        FeeEstimator chainfee.Estimator

        // hodl.Mask is a bitvector composed of hodl.Flags, specifying breakpoints
        // for HTLC forwarding internal to the switch.
        //
        // NOTE: This should only be used for testing.
        HodlMask hodl.Mask

        // SyncStates is used to indicate that we need send the channel
        // reestablishment message to the remote peer. It should be done if our
        // clients have been restarted, or remote peer have been reconnected.
        SyncStates bool

        // BatchTicker is the ticker that determines the interval that we'll
        // use to check the batch to see if there're any updates we should
        // flush out. By batching updates into a single commit, we attempt to
        // increase throughput by maximizing the number of updates coalesced
        // into a single commit.
        BatchTicker ticker.Ticker

        // FwdPkgGCTicker is the ticker determining the frequency at which
        // garbage collection of forwarding packages occurs. We use a
        // time-based approach, as opposed to block epochs, as to not hinder
        // syncing.
        FwdPkgGCTicker ticker.Ticker

        // PendingCommitTicker is a ticker that allows the link to determine if
        // a locally initiated commitment dance gets stuck waiting for the
        // remote party to revoke.
        PendingCommitTicker ticker.Ticker

        // BatchSize is the max size of a batch of updates done to the link
        // before we do a state update.
        BatchSize uint32

        // UnsafeReplay will cause a link to replay the adds in its latest
        // commitment txn after the link is restarted. This should only be used
        // in testing, it is here to ensure the sphinx replay detection on the
        // receiving node is persistent.
        UnsafeReplay bool

        // MinUpdateTimeout represents the minimum interval in which a link
        // will propose to update its commitment fee rate. A random timeout will
        // be selected between this and MaxUpdateTimeout.
        MinUpdateTimeout time.Duration

        // MaxUpdateTimeout represents the maximum interval in which a link
        // will propose to update its commitment fee rate. A random timeout will
        // be selected between this and MinUpdateTimeout.
        MaxUpdateTimeout time.Duration

        // OutgoingCltvRejectDelta defines the number of blocks before expiry of
        // an htlc where we don't offer an htlc anymore. This should be at least
        // the outgoing broadcast delta, because in any case we don't want to
        // risk offering an htlc that triggers channel closure.
        OutgoingCltvRejectDelta uint32

        // TowerClient is an optional engine that manages the signing,
        // encrypting, and uploading of justice transactions to the daemon's
        // configured set of watchtowers for legacy channels.
        TowerClient TowerClient

        // MaxOutgoingCltvExpiry is the maximum outgoing timelock that the link
        // should accept for a forwarded HTLC. The value is relative to the
        // current block height.
        MaxOutgoingCltvExpiry uint32

        // MaxFeeAllocation is the highest allocation we'll allow a channel's
        // commitment fee to be of its balance. This only applies to the
        // initiator of the channel.
        MaxFeeAllocation float64

        // MaxAnchorsCommitFeeRate is the max commitment fee rate we'll use as
        // the initiator for channels of the anchor type.
        MaxAnchorsCommitFeeRate chainfee.SatPerKWeight

        // NotifyActiveLink allows the link to tell the ChannelNotifier when a
        // link is first started.
        NotifyActiveLink func(wire.OutPoint)

        // NotifyActiveChannel allows the link to tell the ChannelNotifier when
        // channels becomes active.
        NotifyActiveChannel func(wire.OutPoint)

        // NotifyInactiveChannel allows the switch to tell the ChannelNotifier
        // when channels become inactive.
        NotifyInactiveChannel func(wire.OutPoint)

        // NotifyInactiveLinkEvent allows the switch to tell the
        // ChannelNotifier when a channel link become inactive.
        NotifyInactiveLinkEvent func(wire.OutPoint)

        // HtlcNotifier is an instance of a htlcNotifier which we will pipe htlc
        // events through.
        HtlcNotifier htlcNotifier

        // FailAliasUpdate is a function used to fail an HTLC for an
        // option_scid_alias channel.
        FailAliasUpdate func(sid lnwire.ShortChannelID,
                incoming bool) *lnwire.ChannelUpdate1

        // GetAliases is used by the link and switch to fetch the set of
        // aliases for a given link.
        GetAliases func(base lnwire.ShortChannelID) []lnwire.ShortChannelID

        // PreviouslySentShutdown is an optional value that is set if, at the
        // time of the link being started, persisted shutdown info was found for
        // the channel. This value being set means that we previously sent a
        // Shutdown message to our peer, and so we should do so again on
        // re-establish and should not allow anymore HTLC adds on the outgoing
        // direction of the link.
        PreviouslySentShutdown fn.Option[lnwire.Shutdown]

        // Adds the option to disable forwarding payments in blinded routes
        // by failing back any blinding-related payloads as if they were
        // invalid.
        DisallowRouteBlinding bool

        // DisallowQuiescence is a flag that can be used to disable the
        // quiescence protocol.
        DisallowQuiescence bool

        // MaxFeeExposure is the threshold in milli-satoshis after which we'll
        // restrict the flow of HTLCs and fee updates.
        MaxFeeExposure lnwire.MilliSatoshi

        // ShouldFwdExpEndorsement is a closure that indicates whether the link
        // should forward experimental endorsement signals.
        ShouldFwdExpEndorsement func() bool

        // AuxTrafficShaper is an optional auxiliary traffic shaper that can be
        // used to manage the bandwidth of the link.
        AuxTrafficShaper fn.Option[AuxTrafficShaper]
}

// channelLink is the service which drives a channel's commitment update
// state-machine. In the event that an HTLC needs to be propagated to another
// link, the forward handler from config is used which sends HTLC to the
// switch. Additionally, the link encapsulate logic of commitment protocol
// message ordering and updates.
type channelLink struct {
        // The following fields are only meant to be used *atomically*
        started       int32
        reestablished int32
        shutdown      int32

        // failed should be set to true in case a link error happens, making
        // sure we don't process any more updates.
        failed bool

        // keystoneBatch represents a volatile list of keystones that must be
        // written before attempting to sign the next commitment txn. These
        // represent all the HTLC's forwarded to the link from the switch. Once
        // we lock them into our outgoing commitment, then the circuit has a
        // keystone, and is fully opened.
        keystoneBatch []Keystone

        // openedCircuits is the set of all payment circuits that will be open
        // once we make our next commitment. After making the commitment we'll
        // ACK all these from our mailbox to ensure that they don't get
        // re-delivered if we reconnect.
        openedCircuits []CircuitKey

        // closedCircuits is the set of all payment circuits that will be
        // closed once we make our next commitment. After taking the commitment
        // we'll ACK all these to ensure that they don't get re-delivered if we
        // reconnect.
        closedCircuits []CircuitKey

        // channel is a lightning network channel to which we apply htlc
        // updates.
        channel *lnwallet.LightningChannel

        // cfg is a structure which carries all dependable fields/handlers
        // which may affect behaviour of the service.
        cfg ChannelLinkConfig

        // mailBox is the main interface between the outside world and the
        // link. All incoming messages will be sent over this mailBox. Messages
        // include new updates from our connected peer, and new packets to be
        // forwarded sent by the switch.
        mailBox MailBox

        // upstream is a channel that new messages sent from the remote peer to
        // the local peer will be sent across.
        upstream chan lnwire.Message

        // downstream is a channel in which new multi-hop HTLC's to be
        // forwarded will be sent across. Messages from this channel are sent
        // by the HTLC switch.
        downstream chan *htlcPacket

        // updateFeeTimer is the timer responsible for updating the link's
        // commitment fee every time it fires.
        updateFeeTimer *time.Timer

        // uncommittedPreimages stores a list of all preimages that have been
        // learned since receiving the last CommitSig from the remote peer. The
        // batch will be flushed just before accepting the subsequent CommitSig
        // or on shutdown to avoid doing a write for each preimage received.
        uncommittedPreimages []lntypes.Preimage

        sync.RWMutex

        // hodlQueue is used to receive exit hop htlc resolutions from invoice
        // registry.
        hodlQueue *queue.ConcurrentQueue

        // hodlMap stores related htlc data for a circuit key. It allows
        // resolving those htlcs when we receive a message on hodlQueue.
        hodlMap map[models.CircuitKey]hodlHtlc

        // log is a link-specific logging instance.
        log btclog.Logger

        // isOutgoingAddBlocked tracks whether the channelLink can send an
        // UpdateAddHTLC.
        isOutgoingAddBlocked atomic.Bool

        // isIncomingAddBlocked tracks whether the channelLink can receive an
        // UpdateAddHTLC.
        isIncomingAddBlocked atomic.Bool

        // flushHooks is a hookMap that is triggered when we reach a channel
        // state with no live HTLCs.
        flushHooks hookMap

        // outgoingCommitHooks is a hookMap that is triggered after we send our
        // next CommitSig.
        outgoingCommitHooks hookMap

        // incomingCommitHooks is a hookMap that is triggered after we receive
        // our next CommitSig.
        incomingCommitHooks hookMap

        // quiescer is the state machine that tracks where this channel is with
        // respect to the quiescence protocol.
        quiescer Quiescer

        // quiescenceReqs is a queue of requests to quiesce this link. The
        // members of the queue are send-only channels we should call back with
        // the result.
        quiescenceReqs chan StfuReq

        // cg is a helper that encapsulates a wait group and quit channel and
        // allows contexts that either block or cancel on those depending on
        // the use case.
        cg *fn.ContextGuard
}

// hookMap is a data structure that is used to track the hooks that need to be
// called in various parts of the channelLink's lifecycle.
//
// WARNING: NOT thread-safe.
type hookMap struct {
        // allocIdx keeps track of the next id we haven't yet allocated.
        allocIdx atomic.Uint64

        // transient is a map of hooks that are only called the next time invoke
        // is called. These hooks are deleted during invoke.
        transient map[uint64]func()

        // newTransients is a channel that we use to accept new hooks into the
        // hookMap.
        newTransients chan func()
}

// newHookMap initializes a new empty hookMap.
func newHookMap() hookMap <span class="cov8" title="1">{
        return hookMap{
                allocIdx:      atomic.Uint64{},
                transient:     make(map[uint64]func()),
                newTransients: make(chan func()),
        }
}</span>

// alloc allocates space in the hook map for the supplied hook, the second
// argument determines whether it goes into the transient or persistent part
// of the hookMap.
func (m *hookMap) alloc(hook func()) uint64 <span class="cov8" title="1">{
        // We assume we never overflow a uint64. Seems OK.
        hookID := m.allocIdx.Add(1)
        if hookID == 0 </span><span class="cov0" title="0">{
                panic("hookMap allocIdx overflow")</span>
        }
        <span class="cov8" title="1">m.transient[hookID] = hook

        return hookID</span>
}

// invoke is used on a hook map to call all the registered hooks and then clear
// out the transient hooks so they are not called again.
func (m *hookMap) invoke() <span class="cov8" title="1">{
        for _, hook := range m.transient </span><span class="cov8" title="1">{
                hook()
        }</span>

        <span class="cov8" title="1">m.transient = make(map[uint64]func())</span>
}

// hodlHtlc contains htlc data that is required for resolution.
type hodlHtlc struct {
        add        lnwire.UpdateAddHTLC
        sourceRef  channeldb.AddRef
        obfuscator hop.ErrorEncrypter
}

// NewChannelLink creates a new instance of a ChannelLink given a configuration
// and active channel that will be used to verify/apply updates to.
func NewChannelLink(cfg ChannelLinkConfig,
        channel *lnwallet.LightningChannel) ChannelLink <span class="cov8" title="1">{

        logPrefix := fmt.Sprintf("ChannelLink(%v):", channel.ChannelPoint())

        // If the max fee exposure isn't set, use the default.
        if cfg.MaxFeeExposure == 0 </span><span class="cov8" title="1">{
                cfg.MaxFeeExposure = DefaultMaxFeeExposure
        }</span>

        <span class="cov8" title="1">var qsm Quiescer
        if !cfg.DisallowQuiescence </span><span class="cov8" title="1">{
                qsm = NewQuiescer(QuiescerCfg{
                        chanID: lnwire.NewChanIDFromOutPoint(
                                channel.ChannelPoint(),
                        ),
                        channelInitiator: channel.Initiator(),
                        sendMsg: func(s lnwire.Stfu) error </span><span class="cov8" title="1">{
                                return cfg.Peer.SendMessage(false, &amp;s)
                        }</span>,
                        timeoutDuration: defaultQuiescenceTimeout,
                        onTimeout: func() <span class="cov0" title="0">{
                                cfg.Peer.Disconnect(ErrQuiescenceTimeout)
                        }</span>,
                })
        } else<span class="cov0" title="0"> {
                qsm = &amp;quiescerNoop{}
        }</span>

        <span class="cov8" title="1">quiescenceReqs := make(
                chan fn.Req[fn.Unit, fn.Result[lntypes.ChannelParty]], 1,
        )

        return &amp;channelLink{
                cfg:                 cfg,
                channel:             channel,
                hodlMap:             make(map[models.CircuitKey]hodlHtlc),
                hodlQueue:           queue.NewConcurrentQueue(10),
                log:                 log.WithPrefix(logPrefix),
                flushHooks:          newHookMap(),
                outgoingCommitHooks: newHookMap(),
                incomingCommitHooks: newHookMap(),
                quiescer:            qsm,
                quiescenceReqs:      quiescenceReqs,
                cg:                  fn.NewContextGuard(),
        }</span>
}

// A compile time check to ensure channelLink implements the ChannelLink
// interface.
var _ ChannelLink = (*channelLink)(nil)

// Start starts all helper goroutines required for the operation of the channel
// link.
//
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) Start() error <span class="cov8" title="1">{
        if !atomic.CompareAndSwapInt32(&amp;l.started, 0, 1) </span><span class="cov0" title="0">{
                err := fmt.Errorf("channel link(%v): already started", l)
                l.log.Warn("already started")
                return err
        }</span>

        <span class="cov8" title="1">l.log.Info("starting")

        // If the config supplied watchtower client, ensure the channel is
        // registered before trying to use it during operation.
        if l.cfg.TowerClient != nil </span><span class="cov0" title="0">{
                err := l.cfg.TowerClient.RegisterChannel(
                        l.ChanID(), l.channel.State().ChanType,
                )
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        <span class="cov8" title="1">l.mailBox.ResetMessages()
        l.hodlQueue.Start()

        // Before launching the htlcManager messages, revert any circuits that
        // were marked open in the switch's circuit map, but did not make it
        // into a commitment txn. We use the next local htlc index as the cut
        // off point, since all indexes below that are committed. This action
        // is only performed if the link's final short channel ID has been
        // assigned, otherwise we would try to trim the htlcs belonging to the
        // all-zero, hop.Source ID.
        if l.ShortChanID() != hop.Source </span><span class="cov8" title="1">{
                localHtlcIndex, err := l.channel.NextLocalHtlcIndex()
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("unable to retrieve next local "+
                                "htlc index: %v", err)
                }</span>

                // NOTE: This is automatically done by the switch when it
                // starts up, but is necessary to prevent inconsistencies in
                // the case that the link flaps. This is a result of a link's
                // life-cycle being shorter than that of the switch.
                <span class="cov8" title="1">chanID := l.ShortChanID()
                err = l.cfg.Circuits.TrimOpenCircuits(chanID, localHtlcIndex)
                if err != nil </span><span class="cov0" title="0">{
                        return fmt.Errorf("unable to trim circuits above "+
                                "local htlc index %d: %v", localHtlcIndex, err)
                }</span>

                // Since the link is live, before we start the link we'll update
                // the ChainArbitrator with the set of new channel signals for
                // this channel.
                //
                // TODO(roasbeef): split goroutines within channel arb to avoid
                <span class="cov8" title="1">go func() </span><span class="cov8" title="1">{
                        signals := &amp;contractcourt.ContractSignals{
                                ShortChanID: l.channel.ShortChanID(),
                        }

                        err := l.cfg.UpdateContractSignals(signals)
                        if err != nil </span><span class="cov0" title="0">{
                                l.log.Errorf("unable to update signals")
                        }</span>
                }()
        }

        <span class="cov8" title="1">l.updateFeeTimer = time.NewTimer(l.randomFeeUpdateTimeout())

        l.cg.WgAdd(1)
        go l.htlcManager(context.TODO())

        return nil</span>
}

// Stop gracefully stops all active helper goroutines, then waits until they've
// exited.
//
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) Stop() <span class="cov8" title="1">{
        if !atomic.CompareAndSwapInt32(&amp;l.shutdown, 0, 1) </span><span class="cov8" title="1">{
                l.log.Warn("already stopped")
                return
        }</span>

        <span class="cov8" title="1">l.log.Info("stopping")

        // As the link is stopping, we are no longer interested in htlc
        // resolutions coming from the invoice registry.
        l.cfg.Registry.HodlUnsubscribeAll(l.hodlQueue.ChanIn())

        if l.cfg.ChainEvents.Cancel != nil </span><span class="cov0" title="0">{
                l.cfg.ChainEvents.Cancel()
        }</span>

        // Ensure the channel for the timer is drained.
        <span class="cov8" title="1">if l.updateFeeTimer != nil </span><span class="cov8" title="1">{
                if !l.updateFeeTimer.Stop() </span><span class="cov0" title="0">{
                        select </span>{
                        case &lt;-l.updateFeeTimer.C:<span class="cov0" title="0"></span>
                        default:<span class="cov0" title="0"></span>
                        }
                }
        }

        <span class="cov8" title="1">if l.hodlQueue != nil </span><span class="cov8" title="1">{
                l.hodlQueue.Stop()
        }</span>

        <span class="cov8" title="1">l.cg.Quit()
        l.cg.WgWait()

        // Now that the htlcManager has completely exited, reset the packet
        // courier. This allows the mailbox to revaluate any lingering Adds that
        // were delivered but didn't make it on a commitment to be failed back
        // if the link is offline for an extended period of time. The error is
        // ignored since it can only fail when the daemon is exiting.
        _ = l.mailBox.ResetPackets()

        // As a final precaution, we will attempt to flush any uncommitted
        // preimages to the preimage cache. The preimages should be re-delivered
        // after channel reestablishment, however this adds an extra layer of
        // protection in case the peer never returns. Without this, we will be
        // unable to settle any contracts depending on the preimages even though
        // we had learned them at some point.
        err := l.cfg.PreimageCache.AddPreimages(l.uncommittedPreimages...)
        if err != nil </span><span class="cov0" title="0">{
                l.log.Errorf("unable to add preimages=%v to cache: %v",
                        l.uncommittedPreimages, err)
        }</span>
}

// WaitForShutdown blocks until the link finishes shutting down, which includes
// termination of all dependent goroutines.
func (l *channelLink) WaitForShutdown() <span class="cov0" title="0">{
        l.cg.WgWait()
}</span>

// EligibleToForward returns a bool indicating if the channel is able to
// actively accept requests to forward HTLC's. We're able to forward HTLC's if
// we are eligible to update AND the channel isn't currently flushing the
// outgoing half of the channel.
//
// NOTE: MUST NOT be called from the main event loop.
func (l *channelLink) EligibleToForward() bool <span class="cov8" title="1">{
        l.RLock()
        defer l.RUnlock()

        return l.eligibleToForward()
}</span>

// eligibleToForward returns a bool indicating if the channel is able to
// actively accept requests to forward HTLC's. We're able to forward HTLC's if
// we are eligible to update AND the channel isn't currently flushing the
// outgoing half of the channel.
//
// NOTE: MUST be called from the main event loop.
func (l *channelLink) eligibleToForward() bool <span class="cov8" title="1">{
        return l.eligibleToUpdate() &amp;&amp; !l.IsFlushing(Outgoing)
}</span>

// eligibleToUpdate returns a bool indicating if the channel is able to update
// channel state. We're able to update channel state if we know the remote
// party's next revocation point. Otherwise, we can't initiate new channel
// state. We also require that the short channel ID not be the all-zero source
// ID, meaning that the channel has had its ID finalized.
//
// NOTE: MUST be called from the main event loop.
func (l *channelLink) eligibleToUpdate() bool <span class="cov8" title="1">{
        return l.channel.RemoteNextRevocation() != nil &amp;&amp;
                l.channel.ShortChanID() != hop.Source &amp;&amp;
                l.isReestablished() &amp;&amp;
                l.quiescer.CanSendUpdates()
}</span>

// EnableAdds sets the ChannelUpdateHandler state to allow UpdateAddHtlc's in
// the specified direction. It returns true if the state was changed and false
// if the desired state was already set before the method was called.
func (l *channelLink) EnableAdds(linkDirection LinkDirection) bool <span class="cov8" title="1">{
        if linkDirection == Outgoing </span><span class="cov8" title="1">{
                return l.isOutgoingAddBlocked.Swap(false)
        }</span>

        <span class="cov8" title="1">return l.isIncomingAddBlocked.Swap(false)</span>
}

// DisableAdds sets the ChannelUpdateHandler state to allow UpdateAddHtlc's in
// the specified direction. It returns true if the state was changed and false
// if the desired state was already set before the method was called.
func (l *channelLink) DisableAdds(linkDirection LinkDirection) bool <span class="cov8" title="1">{
        if linkDirection == Outgoing </span><span class="cov8" title="1">{
                return !l.isOutgoingAddBlocked.Swap(true)
        }</span>

        <span class="cov8" title="1">return !l.isIncomingAddBlocked.Swap(true)</span>
}

// IsFlushing returns true when UpdateAddHtlc's are disabled in the direction of
// the argument.
func (l *channelLink) IsFlushing(linkDirection LinkDirection) bool <span class="cov8" title="1">{
        if linkDirection == Outgoing </span><span class="cov8" title="1">{
                return l.isOutgoingAddBlocked.Load()
        }</span>

        <span class="cov8" title="1">return l.isIncomingAddBlocked.Load()</span>
}

// OnFlushedOnce adds a hook that will be called the next time the channel
// state reaches zero htlcs. This hook will only ever be called once. If the
// channel state already has zero htlcs, then this will be called immediately.
func (l *channelLink) OnFlushedOnce(hook func()) <span class="cov8" title="1">{
        select </span>{
        case l.flushHooks.newTransients &lt;- hook:<span class="cov8" title="1"></span>
        case &lt;-l.cg.Done():<span class="cov0" title="0"></span>
        }
}

// OnCommitOnce adds a hook that will be called the next time a CommitSig
// message is sent in the argument's LinkDirection. This hook will only ever be
// called once. If no CommitSig is owed in the argument's LinkDirection, then
// we will call this hook be run immediately.
func (l *channelLink) OnCommitOnce(direction LinkDirection, hook func()) <span class="cov8" title="1">{
        var queue chan func()

        if direction == Outgoing </span><span class="cov8" title="1">{
                queue = l.outgoingCommitHooks.newTransients
        }</span> else<span class="cov0" title="0"> {
                queue = l.incomingCommitHooks.newTransients
        }</span>

        <span class="cov8" title="1">select </span>{
        case queue &lt;- hook:<span class="cov8" title="1"></span>
        case &lt;-l.cg.Done():<span class="cov0" title="0"></span>
        }
}

// InitStfu allows us to initiate quiescence on this link. It returns a receive
// only channel that will block until quiescence has been achieved, or
// definitively fails.
//
// This operation has been added to allow channels to be quiesced via RPC. It
// may be removed or reworked in the future as RPC initiated quiescence is a
// holdover until we have downstream protocols that use it.
func (l *channelLink) InitStfu() &lt;-chan fn.Result[lntypes.ChannelParty] <span class="cov8" title="1">{
        req, out := fn.NewReq[fn.Unit, fn.Result[lntypes.ChannelParty]](
                fn.Unit{},
        )

        select </span>{
        case l.quiescenceReqs &lt;- req:<span class="cov8" title="1"></span>
        case &lt;-l.cg.Done():<span class="cov0" title="0">
                req.Resolve(fn.Err[lntypes.ChannelParty](ErrLinkShuttingDown))</span>
        }

        <span class="cov8" title="1">return out</span>
}

// isReestablished returns true if the link has successfully completed the
// channel reestablishment dance.
func (l *channelLink) isReestablished() bool <span class="cov8" title="1">{
        return atomic.LoadInt32(&amp;l.reestablished) == 1
}</span>

// markReestablished signals that the remote peer has successfully exchanged
// channel reestablish messages and that the channel is ready to process
// subsequent messages.
func (l *channelLink) markReestablished() <span class="cov8" title="1">{
        atomic.StoreInt32(&amp;l.reestablished, 1)
}</span>

// IsUnadvertised returns true if the underlying channel is unadvertised.
func (l *channelLink) IsUnadvertised() bool <span class="cov8" title="1">{
        state := l.channel.State()
        return state.ChannelFlags&amp;lnwire.FFAnnounceChannel == 0
}</span>

// sampleNetworkFee samples the current fee rate on the network to get into the
// chain in a timely manner. The returned value is expressed in fee-per-kw, as
// this is the native rate used when computing the fee for commitment
// transactions, and the second-level HTLC transactions.
func (l *channelLink) sampleNetworkFee() (chainfee.SatPerKWeight, error) <span class="cov8" title="1">{
        // We'll first query for the sat/kw recommended to be confirmed within 3
        // blocks.
        feePerKw, err := l.cfg.FeeEstimator.EstimateFeePerKW(3)
        if err != nil </span><span class="cov0" title="0">{
                return 0, err
        }</span>

        <span class="cov8" title="1">l.log.Debugf("sampled fee rate for 3 block conf: %v sat/kw",
                int64(feePerKw))

        return feePerKw, nil</span>
}

// shouldAdjustCommitFee returns true if we should update our commitment fee to
// match that of the network fee. We'll only update our commitment fee if the
// network fee is +/- 10% to our commitment fee or if our current commitment
// fee is below the minimum relay fee.
func shouldAdjustCommitFee(netFee, chanFee,
        minRelayFee chainfee.SatPerKWeight) bool <span class="cov8" title="1">{

        switch </span>{
        // If the network fee is greater than our current commitment fee and
        // our current commitment fee is below the minimum relay fee then
        // we should switch to it no matter if it is less than a 10% increase.
        case netFee &gt; chanFee &amp;&amp; chanFee &lt; minRelayFee:<span class="cov8" title="1">
                return true</span>

        // If the network fee is greater than the commitment fee, then we'll
        // switch to it if it's at least 10% greater than the commit fee.
        case netFee &gt; chanFee &amp;&amp; netFee &gt;= (chanFee+(chanFee*10)/100):<span class="cov8" title="1">
                return true</span>

        // If the network fee is less than our commitment fee, then we'll
        // switch to it if it's at least 10% less than the commitment fee.
        case netFee &lt; chanFee &amp;&amp; netFee &lt;= (chanFee-(chanFee*10)/100):<span class="cov8" title="1">
                return true</span>

        // Otherwise, we won't modify our fee.
        default:<span class="cov8" title="1">
                return false</span>
        }
}

// failCb is used to cut down on the argument verbosity.
type failCb func(update *lnwire.ChannelUpdate1) lnwire.FailureMessage

// createFailureWithUpdate creates a ChannelUpdate when failing an incoming or
// outgoing HTLC. It may return a FailureMessage that references a channel's
// alias. If the channel does not have an alias, then the regular channel
// update from disk will be returned.
func (l *channelLink) createFailureWithUpdate(incoming bool,
        outgoingScid lnwire.ShortChannelID, cb failCb) lnwire.FailureMessage <span class="cov8" title="1">{

        // Determine which SCID to use in case we need to use aliases in the
        // ChannelUpdate.
        scid := outgoingScid
        if incoming </span><span class="cov0" title="0">{
                scid = l.ShortChanID()
        }</span>

        // Try using the FailAliasUpdate function. If it returns nil, fallback
        // to the non-alias behavior.
        <span class="cov8" title="1">update := l.cfg.FailAliasUpdate(scid, incoming)
        if update == nil </span><span class="cov8" title="1">{
                // Fallback to the non-alias behavior.
                var err error
                update, err = l.cfg.FetchLastChannelUpdate(l.ShortChanID())
                if err != nil </span><span class="cov0" title="0">{
                        return &amp;lnwire.FailTemporaryNodeFailure{}
                }</span>
        }

        <span class="cov8" title="1">return cb(update)</span>
}

// syncChanState attempts to synchronize channel states with the remote party.
// This method is to be called upon reconnection after the initial funding
// flow. We'll compare out commitment chains with the remote party, and re-send
// either a danging commit signature, a revocation, or both.
func (l *channelLink) syncChanStates(ctx context.Context) error <span class="cov8" title="1">{
        chanState := l.channel.State()

        l.log.Infof("Attempting to re-synchronize channel: %v", chanState)

        // First, we'll generate our ChanSync message to send to the other
        // side. Based on this message, the remote party will decide if they
        // need to retransmit any data or not.
        localChanSyncMsg, err := chanState.ChanSyncMsg()
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("unable to generate chan sync message for "+
                        "ChannelPoint(%v)", l.channel.ChannelPoint())
        }</span>
        <span class="cov8" title="1">if err := l.cfg.Peer.SendMessage(true, localChanSyncMsg); err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("unable to send chan sync message for "+
                        "ChannelPoint(%v): %v", l.channel.ChannelPoint(), err)
        }</span>

        <span class="cov8" title="1">var msgsToReSend []lnwire.Message

        // Next, we'll wait indefinitely to receive the ChanSync message. The
        // first message sent MUST be the ChanSync message.
        select </span>{
        case msg := &lt;-l.upstream:<span class="cov8" title="1">
                l.log.Tracef("Received msg=%v from peer(%x)", msg.MsgType(),
                        l.cfg.Peer.PubKey())

                remoteChanSyncMsg, ok := msg.(*lnwire.ChannelReestablish)
                if !ok </span><span class="cov0" title="0">{
                        return fmt.Errorf("first message sent to sync "+
                                "should be ChannelReestablish, instead "+
                                "received: %T", msg)
                }</span>

                // If the remote party indicates that they think we haven't
                // done any state updates yet, then we'll retransmit the
                // channel_ready message first. We do this, as at this point
                // we can't be sure if they've really received the
                // ChannelReady message.
                <span class="cov8" title="1">if remoteChanSyncMsg.NextLocalCommitHeight == 1 &amp;&amp;
                        localChanSyncMsg.NextLocalCommitHeight == 1 &amp;&amp;
                        !l.channel.IsPending() </span><span class="cov8" title="1">{

                        l.log.Infof("resending ChannelReady message to peer")

                        nextRevocation, err := l.channel.NextRevocationKey()
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("unable to create next "+
                                        "revocation: %v", err)
                        }</span>

                        <span class="cov8" title="1">channelReadyMsg := lnwire.NewChannelReady(
                                l.ChanID(), nextRevocation,
                        )

                        // If this is a taproot channel, then we'll send the
                        // very same nonce that we sent above, as they should
                        // take the latest verification nonce we send.
                        if chanState.ChanType.IsTaproot() </span><span class="cov0" title="0">{
                                //nolint:ll
                                channelReadyMsg.NextLocalNonce = localChanSyncMsg.LocalNonce
                        }</span>

                        // For channels that negotiated the option-scid-alias
                        // feature bit, ensure that we send over the alias in
                        // the channel_ready message. We'll send the first
                        // alias we find for the channel since it does not
                        // matter which alias we send. We'll error out if no
                        // aliases are found.
                        <span class="cov8" title="1">if l.negotiatedAliasFeature() </span><span class="cov0" title="0">{
                                aliases := l.getAliases()
                                if len(aliases) == 0 </span><span class="cov0" title="0">{
                                        // This shouldn't happen since we
                                        // always add at least one alias before
                                        // the channel reaches the link.
                                        return fmt.Errorf("no aliases found")
                                }</span>

                                // getAliases returns a copy of the alias slice
                                // so it is ok to use a pointer to the first
                                // entry.
                                <span class="cov0" title="0">channelReadyMsg.AliasScid = &amp;aliases[0]</span>
                        }

                        <span class="cov8" title="1">err = l.cfg.Peer.SendMessage(false, channelReadyMsg)
                        if err != nil </span><span class="cov0" title="0">{
                                return fmt.Errorf("unable to re-send "+
                                        "ChannelReady: %v", err)
                        }</span>
                }

                // In any case, we'll then process their ChanSync message.
                <span class="cov8" title="1">l.log.Info("received re-establishment message from remote side")

                var (
                        openedCircuits []CircuitKey
                        closedCircuits []CircuitKey
                )

                // We've just received a ChanSync message from the remote
                // party, so we'll process the message  in order to determine
                // if we need to re-transmit any messages to the remote party.
                ctx, cancel := l.cg.Create(ctx)
                defer cancel()
                msgsToReSend, openedCircuits, closedCircuits, err =
                        l.channel.ProcessChanSyncMsg(ctx, remoteChanSyncMsg)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                // Repopulate any identifiers for circuits that may have been
                // opened or unclosed. This may happen if we needed to
                // retransmit a commitment signature message.
                <span class="cov8" title="1">l.openedCircuits = openedCircuits
                l.closedCircuits = closedCircuits

                // Ensure that all packets have been have been removed from the
                // link's mailbox.
                if err := l.ackDownStreamPackets(); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov8" title="1">if len(msgsToReSend) &gt; 0 </span><span class="cov8" title="1">{
                        l.log.Infof("sending %v updates to synchronize the "+
                                "state", len(msgsToReSend))
                }</span>

                // If we have any messages to retransmit, we'll do so
                // immediately so we return to a synchronized state as soon as
                // possible.
                <span class="cov8" title="1">for _, msg := range msgsToReSend </span><span class="cov8" title="1">{
                        l.cfg.Peer.SendMessage(false, msg)
                }</span>

        case &lt;-l.cg.Done():<span class="cov0" title="0">
                return ErrLinkShuttingDown</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// resolveFwdPkgs loads any forwarding packages for this link from disk, and
// reprocesses them in order. The primary goal is to make sure that any HTLCs
// we previously received are reinstated in memory, and forwarded to the switch
// if necessary. After a restart, this will also delete any previously
// completed packages.
func (l *channelLink) resolveFwdPkgs(ctx context.Context) error <span class="cov8" title="1">{
        fwdPkgs, err := l.channel.LoadFwdPkgs()
        if err != nil </span><span class="cov8" title="1">{
                return err
        }</span>

        <span class="cov8" title="1">l.log.Debugf("loaded %d fwd pks", len(fwdPkgs))

        for _, fwdPkg := range fwdPkgs </span><span class="cov8" title="1">{
                if err := l.resolveFwdPkg(fwdPkg); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        // If any of our reprocessing steps require an update to the commitment
        // txn, we initiate a state transition to capture all relevant changes.
        <span class="cov8" title="1">if l.channel.NumPendingUpdates(lntypes.Local, lntypes.Remote) &gt; 0 </span><span class="cov0" title="0">{
                return l.updateCommitTx(ctx)
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// resolveFwdPkg interprets the FwdState of the provided package, either
// reprocesses any outstanding htlcs in the package, or performs garbage
// collection on the package.
func (l *channelLink) resolveFwdPkg(fwdPkg *channeldb.FwdPkg) error <span class="cov8" title="1">{
        // Remove any completed packages to clear up space.
        if fwdPkg.State == channeldb.FwdStateCompleted </span><span class="cov8" title="1">{
                l.log.Debugf("removing completed fwd pkg for height=%d",
                        fwdPkg.Height)

                err := l.channel.RemoveFwdPkgs(fwdPkg.Height)
                if err != nil </span><span class="cov0" title="0">{
                        l.log.Errorf("unable to remove fwd pkg for height=%d: "+
                                "%v", fwdPkg.Height, err)
                        return err
                }</span>
        }

        // Otherwise this is either a new package or one has gone through
        // processing, but contains htlcs that need to be restored in memory.
        // We replay this forwarding package to make sure our local mem state
        // is resurrected, we mimic any original responses back to the remote
        // party, and re-forward the relevant HTLCs to the switch.

        // If the package is fully acked but not completed, it must still have
        // settles and fails to propagate.
        <span class="cov8" title="1">if !fwdPkg.SettleFailFilter.IsFull() </span><span class="cov0" title="0">{
                l.processRemoteSettleFails(fwdPkg)
        }</span>

        // Finally, replay *ALL ADDS* in this forwarding package. The
        // downstream logic is able to filter out any duplicates, but we must
        // shove the entire, original set of adds down the pipeline so that the
        // batch of adds presented to the sphinx router does not ever change.
        <span class="cov8" title="1">if !fwdPkg.AckFilter.IsFull() </span><span class="cov8" title="1">{
                l.processRemoteAdds(fwdPkg)

                // If the link failed during processing the adds, we must
                // return to ensure we won't attempted to update the state
                // further.
                if l.failed </span><span class="cov0" title="0">{
                        return fmt.Errorf("link failed while " +
                                "processing remote adds")
                }</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// fwdPkgGarbager periodically reads all forwarding packages from disk and
// removes those that can be discarded. It is safe to do this entirely in the
// background, since all state is coordinated on disk. This also ensures the
// link can continue to process messages and interleave database accesses.
//
// NOTE: This MUST be run as a goroutine.
func (l *channelLink) fwdPkgGarbager() <span class="cov8" title="1">{
        defer l.cg.WgDone()

        l.cfg.FwdPkgGCTicker.Resume()
        defer l.cfg.FwdPkgGCTicker.Stop()

        if err := l.loadAndRemove(); err != nil </span><span class="cov0" title="0">{
                l.log.Warnf("unable to run initial fwd pkgs gc: %v", err)
        }</span>

        <span class="cov8" title="1">for </span><span class="cov8" title="1">{
                select </span>{
                case &lt;-l.cfg.FwdPkgGCTicker.Ticks():<span class="cov8" title="1">
                        if err := l.loadAndRemove(); err != nil </span><span class="cov8" title="1">{
                                l.log.Warnf("unable to remove fwd pkgs: %v",
                                        err)
                                continue</span>
                        }
                case &lt;-l.cg.Done():<span class="cov8" title="1">
                        return</span>
                }
        }
}

// loadAndRemove loads all the channels forwarding packages and determines if
// they can be removed. It is called once before the FwdPkgGCTicker ticks so that
// a longer tick interval can be used.
func (l *channelLink) loadAndRemove() error <span class="cov8" title="1">{
        fwdPkgs, err := l.channel.LoadFwdPkgs()
        if err != nil </span><span class="cov8" title="1">{
                return err
        }</span>

        <span class="cov8" title="1">var removeHeights []uint64
        for _, fwdPkg := range fwdPkgs </span><span class="cov8" title="1">{
                if fwdPkg.State != channeldb.FwdStateCompleted </span><span class="cov8" title="1">{
                        continue</span>
                }

                <span class="cov0" title="0">removeHeights = append(removeHeights, fwdPkg.Height)</span>
        }

        // If removeHeights is empty, return early so we don't use a db
        // transaction.
        <span class="cov8" title="1">if len(removeHeights) == 0 </span><span class="cov8" title="1">{
                return nil
        }</span>

        <span class="cov0" title="0">return l.channel.RemoveFwdPkgs(removeHeights...)</span>
}

// handleChanSyncErr performs the error handling logic in the case where we
// could not successfully syncChanStates with our channel peer.
func (l *channelLink) handleChanSyncErr(err error) <span class="cov0" title="0">{
        l.log.Warnf("error when syncing channel states: %v", err)

        var errDataLoss *lnwallet.ErrCommitSyncLocalDataLoss

        switch </span>{
        case errors.Is(err, ErrLinkShuttingDown):<span class="cov0" title="0">
                l.log.Debugf("unable to sync channel states, link is " +
                        "shutting down")
                return</span>

        // We failed syncing the commit chains, probably because the remote has
        // lost state. We should force close the channel.
        case errors.Is(err, lnwallet.ErrCommitSyncRemoteDataLoss):<span class="cov0" title="0">
                fallthrough</span>

        // The remote sent us an invalid last commit secret, we should force
        // close the channel.
        // TODO(halseth): and permanently ban the peer?
        case errors.Is(err, lnwallet.ErrInvalidLastCommitSecret):<span class="cov0" title="0">
                fallthrough</span>

        // The remote sent us a commit point different from what they sent us
        // before.
        // TODO(halseth): ban peer?
        case errors.Is(err, lnwallet.ErrInvalidLocalUnrevokedCommitPoint):<span class="cov0" title="0">
                // We'll fail the link and tell the peer to force close the
                // channel. Note that the database state is not updated here,
                // but will be updated when the close transaction is ready to
                // avoid that we go down before storing the transaction in the
                // db.
                l.failf(
                        LinkFailureError{
                                code:          ErrSyncError,
                                FailureAction: LinkFailureForceClose,
                        },
                        "unable to synchronize channel states: %v", err,
                )</span>

        // We have lost state and cannot safely force close the channel. Fail
        // the channel and wait for the remote to hopefully force close it. The
        // remote has sent us its latest unrevoked commitment point, and we'll
        // store it in the database, such that we can attempt to recover the
        // funds if the remote force closes the channel.
        case errors.As(err, &amp;errDataLoss):<span class="cov0" title="0">
                err := l.channel.MarkDataLoss(
                        errDataLoss.CommitPoint,
                )
                if err != nil </span><span class="cov0" title="0">{
                        l.log.Errorf("unable to mark channel data loss: %v",
                                err)
                }</span>

        // We determined the commit chains were not possible to sync. We
        // cautiously fail the channel, but don't force close.
        // TODO(halseth): can we safely force close in any cases where this
        // error is returned?
        case errors.Is(err, lnwallet.ErrCannotSyncCommitChains):<span class="cov0" title="0">
                if err := l.channel.MarkBorked(); err != nil </span><span class="cov0" title="0">{
                        l.log.Errorf("unable to mark channel borked: %v", err)
                }</span>

        // Other, unspecified error.
        default:<span class="cov0" title="0"></span>
        }

        <span class="cov0" title="0">l.failf(
                LinkFailureError{
                        code:          ErrRecoveryError,
                        FailureAction: LinkFailureForceNone,
                },
                "unable to synchronize channel states: %v", err,
        )</span>
}

// htlcManager is the primary goroutine which drives a channel's commitment
// update state-machine in response to messages received via several channels.
// This goroutine reads messages from the upstream (remote) peer, and also from
// downstream channel managed by the channel link. In the event that an htlc
// needs to be forwarded, then send-only forward handler is used which sends
// htlc packets to the switch. Additionally, this goroutine handles acting upon
// all timeouts for any active HTLCs, manages the channel's revocation window,
// and also the htlc trickle queue+timer for this active channels.
//
// NOTE: This MUST be run as a goroutine.
//
//nolint:funlen
func (l *channelLink) htlcManager(ctx context.Context) <span class="cov8" title="1">{
        defer func() </span><span class="cov8" title="1">{
                l.cfg.BatchTicker.Stop()
                l.cg.WgDone()
                l.log.Infof("exited")
        }</span>()

        <span class="cov8" title="1">l.log.Infof("HTLC manager started, bandwidth=%v", l.Bandwidth())

        // Notify any clients that the link is now in the switch via an
        // ActiveLinkEvent. We'll also defer an inactive link notification for
        // when the link exits to ensure that every active notification is
        // matched by an inactive one.
        l.cfg.NotifyActiveLink(l.ChannelPoint())
        defer l.cfg.NotifyInactiveLinkEvent(l.ChannelPoint())

        // TODO(roasbeef): need to call wipe chan whenever D/C?

        // If this isn't the first time that this channel link has been
        // created, then we'll need to check to see if we need to
        // re-synchronize state with the remote peer. settledHtlcs is a map of
        // HTLC's that we re-settled as part of the channel state sync.
        if l.cfg.SyncStates </span><span class="cov8" title="1">{
                err := l.syncChanStates(ctx)
                if err != nil </span><span class="cov0" title="0">{
                        l.handleChanSyncErr(err)
                        return
                }</span>
        }

        // If a shutdown message has previously been sent on this link, then we
        // need to make sure that we have disabled any HTLC adds on the outgoing
        // direction of the link and that we re-resend the same shutdown message
        // that we previously sent.
        <span class="cov8" title="1">l.cfg.PreviouslySentShutdown.WhenSome(func(shutdown lnwire.Shutdown) </span><span class="cov0" title="0">{
                // Immediately disallow any new outgoing HTLCs.
                if !l.DisableAdds(Outgoing) </span><span class="cov0" title="0">{
                        l.log.Warnf("Outgoing link adds already disabled")
                }</span>

                // Re-send the shutdown message the peer. Since syncChanStates
                // would have sent any outstanding CommitSig, it is fine for us
                // to immediately queue the shutdown message now.
                <span class="cov0" title="0">err := l.cfg.Peer.SendMessage(false, &amp;shutdown)
                if err != nil </span><span class="cov0" title="0">{
                        l.log.Warnf("Error sending shutdown message: %v", err)
                }</span>
        })

        // We've successfully reestablished the channel, mark it as such to
        // allow the switch to forward HTLCs in the outbound direction.
        <span class="cov8" title="1">l.markReestablished()

        // Now that we've received both channel_ready and channel reestablish,
        // we can go ahead and send the active channel notification. We'll also
        // defer the inactive notification for when the link exits to ensure
        // that every active notification is matched by an inactive one.
        l.cfg.NotifyActiveChannel(l.ChannelPoint())
        defer l.cfg.NotifyInactiveChannel(l.ChannelPoint())

        // With the channel states synced, we now reset the mailbox to ensure
        // we start processing all unacked packets in order. This is done here
        // to ensure that all acknowledgments that occur during channel
        // resynchronization have taken affect, causing us only to pull unacked
        // packets after starting to read from the downstream mailbox.
        l.mailBox.ResetPackets()

        // After cleaning up any memory pertaining to incoming packets, we now
        // replay our forwarding packages to handle any htlcs that can be
        // processed locally, or need to be forwarded out to the switch. We will
        // only attempt to resolve packages if our short chan id indicates that
        // the channel is not pending, otherwise we should have no htlcs to
        // reforward.
        if l.ShortChanID() != hop.Source </span><span class="cov8" title="1">{
                err := l.resolveFwdPkgs(ctx)
                switch err </span>{
                // No error was encountered, success.
                case nil:<span class="cov8" title="1"></span>

                // If the duplicate keystone error was encountered, we'll fail
                // without sending an Error message to the peer.
                case ErrDuplicateKeystone:<span class="cov0" title="0">
                        l.failf(LinkFailureError{code: ErrCircuitError},
                                "temporary circuit error: %v", err)
                        return</span>

                // A non-nil error was encountered, send an Error message to
                // the peer.
                default:<span class="cov8" title="1">
                        l.failf(LinkFailureError{code: ErrInternalError},
                                "unable to resolve fwd pkgs: %v", err)
                        return</span>
                }

                // With our link's in-memory state fully reconstructed, spawn a
                // goroutine to manage the reclamation of disk space occupied by
                // completed forwarding packages.
                <span class="cov8" title="1">l.cg.WgAdd(1)
                go l.fwdPkgGarbager()</span>
        }

        <span class="cov8" title="1">for </span><span class="cov8" title="1">{
                // We must always check if we failed at some point processing
                // the last update before processing the next.
                if l.failed </span><span class="cov8" title="1">{
                        l.log.Errorf("link failed, exiting htlcManager")
                        return
                }</span>

                // If the previous event resulted in a non-empty batch, resume
                // the batch ticker so that it can be cleared. Otherwise pause
                // the ticker to prevent waking up the htlcManager while the
                // batch is empty.
                <span class="cov8" title="1">numUpdates := l.channel.NumPendingUpdates(
                        lntypes.Local, lntypes.Remote,
                )
                if numUpdates &gt; 0 </span><span class="cov8" title="1">{
                        l.cfg.BatchTicker.Resume()
                        l.log.Tracef("BatchTicker resumed, "+
                                "NumPendingUpdates(Local, Remote)=%d",
                                numUpdates,
                        )
                }</span> else<span class="cov8" title="1"> {
                        l.cfg.BatchTicker.Pause()
                        l.log.Trace("BatchTicker paused due to zero " +
                                "NumPendingUpdates(Local, Remote)")
                }</span>

                <span class="cov8" title="1">select </span>{
                // We have a new hook that needs to be run when we reach a clean
                // channel state.
                case hook := &lt;-l.flushHooks.newTransients:<span class="cov8" title="1">
                        if l.channel.IsChannelClean() </span><span class="cov0" title="0">{
                                hook()
                        }</span> else<span class="cov8" title="1"> {
                                l.flushHooks.alloc(hook)
                        }</span>

                // We have a new hook that needs to be run when we have
                // committed all of our updates.
                case hook := &lt;-l.outgoingCommitHooks.newTransients:<span class="cov8" title="1">
                        if !l.channel.OweCommitment() </span><span class="cov0" title="0">{
                                hook()
                        }</span> else<span class="cov8" title="1"> {
                                l.outgoingCommitHooks.alloc(hook)
                        }</span>

                // We have a new hook that needs to be run when our peer has
                // committed all of their updates.
                case hook := &lt;-l.incomingCommitHooks.newTransients:<span class="cov0" title="0">
                        if !l.channel.NeedCommitment() </span><span class="cov0" title="0">{
                                hook()
                        }</span> else<span class="cov0" title="0"> {
                                l.incomingCommitHooks.alloc(hook)
                        }</span>

                // Our update fee timer has fired, so we'll check the network
                // fee to see if we should adjust our commitment fee.
                case &lt;-l.updateFeeTimer.C:<span class="cov8" title="1">
                        l.updateFeeTimer.Reset(l.randomFeeUpdateTimeout())

                        // If we're not the initiator of the channel, don't we
                        // don't control the fees, so we can ignore this.
                        if !l.channel.IsInitiator() </span><span class="cov0" title="0">{
                                continue</span>
                        }

                        // If we are the initiator, then we'll sample the
                        // current fee rate to get into the chain within 3
                        // blocks.
                        <span class="cov8" title="1">netFee, err := l.sampleNetworkFee()
                        if err != nil </span><span class="cov0" title="0">{
                                l.log.Errorf("unable to sample network fee: %v",
                                        err)
                                continue</span>
                        }

                        <span class="cov8" title="1">minRelayFee := l.cfg.FeeEstimator.RelayFeePerKW()

                        newCommitFee := l.channel.IdealCommitFeeRate(
                                netFee, minRelayFee,
                                l.cfg.MaxAnchorsCommitFeeRate,
                                l.cfg.MaxFeeAllocation,
                        )

                        // We determine if we should adjust the commitment fee
                        // based on the current commitment fee, the suggested
                        // new commitment fee and the current minimum relay fee
                        // rate.
                        commitFee := l.channel.CommitFeeRate()
                        if !shouldAdjustCommitFee(
                                newCommitFee, commitFee, minRelayFee,
                        ) </span><span class="cov8" title="1">{

                                continue</span>
                        }

                        // If we do, then we'll send a new UpdateFee message to
                        // the remote party, to be locked in with a new update.
                        <span class="cov8" title="1">err = l.updateChannelFee(ctx, newCommitFee)
                        if err != nil </span><span class="cov0" title="0">{
                                l.log.Errorf("unable to update fee rate: %v",
                                        err)
                                continue</span>
                        }

                // The underlying channel has notified us of a unilateral close
                // carried out by the remote peer. In the case of such an
                // event, we'll wipe the channel state from the peer, and mark
                // the contract as fully settled. Afterwards we can exit.
                //
                // TODO(roasbeef): add force closure? also breach?
                case &lt;-l.cfg.ChainEvents.RemoteUnilateralClosure:<span class="cov0" title="0">
                        l.log.Warnf("remote peer has closed on-chain")

                        // TODO(roasbeef): remove all together
                        go func() </span><span class="cov0" title="0">{
                                chanPoint := l.channel.ChannelPoint()
                                l.cfg.Peer.WipeChannel(&amp;chanPoint)
                        }</span>()

                        <span class="cov0" title="0">return</span>

                case &lt;-l.cfg.BatchTicker.Ticks():<span class="cov8" title="1">
                        // Attempt to extend the remote commitment chain
                        // including all the currently pending entries. If the
                        // send was unsuccessful, then abandon the update,
                        // waiting for the revocation window to open up.
                        if !l.updateCommitTxOrFail(ctx) </span><span class="cov0" title="0">{
                                return
                        }</span>

                case &lt;-l.cfg.PendingCommitTicker.Ticks():<span class="cov8" title="1">
                        l.failf(
                                LinkFailureError{
                                        code:          ErrRemoteUnresponsive,
                                        FailureAction: LinkFailureDisconnect,
                                },
                                "unable to complete dance",
                        )
                        return</span>

                // A message from the switch was just received. This indicates
                // that the link is an intermediate hop in a multi-hop HTLC
                // circuit.
                case pkt := &lt;-l.downstream:<span class="cov8" title="1">
                        l.handleDownstreamPkt(ctx, pkt)</span>

                // A message from the connected peer was just received. This
                // indicates that we have a new incoming HTLC, either directly
                // for us, or part of a multi-hop HTLC circuit.
                case msg := &lt;-l.upstream:<span class="cov8" title="1">
                        l.handleUpstreamMsg(ctx, msg)</span>

                // A htlc resolution is received. This means that we now have a
                // resolution for a previously accepted htlc.
                case hodlItem := &lt;-l.hodlQueue.ChanOut():<span class="cov8" title="1">
                        htlcResolution := hodlItem.(invoices.HtlcResolution)
                        err := l.processHodlQueue(ctx, htlcResolution)
                        switch err </span>{
                        // No error, success.
                        case nil:<span class="cov8" title="1"></span>

                        // If the duplicate keystone error was encountered,
                        // fail back gracefully.
                        case ErrDuplicateKeystone:<span class="cov0" title="0">
                                l.failf(LinkFailureError{
                                        code: ErrCircuitError,
                                }, "process hodl queue: "+
                                        "temporary circuit error: %v",
                                        err,
                                )</span>

                        // Send an Error message to the peer.
                        default:<span class="cov8" title="1">
                                l.failf(LinkFailureError{
                                        code: ErrInternalError,
                                }, "process hodl queue: unable to update "+
                                        "commitment: %v", err,
                                )</span>
                        }

                case qReq := &lt;-l.quiescenceReqs:<span class="cov8" title="1">
                        l.quiescer.InitStfu(qReq)

                        if l.noDanglingUpdates(lntypes.Local) </span><span class="cov8" title="1">{
                                err := l.quiescer.SendOwedStfu()
                                if err != nil </span><span class="cov0" title="0">{
                                        l.stfuFailf(
                                                "SendOwedStfu: %s", err.Error(),
                                        )
                                        res := fn.Err[lntypes.ChannelParty](err)
                                        qReq.Resolve(res)
                                }</span>
                        }

                case &lt;-l.cg.Done():<span class="cov8" title="1">
                        return</span>
                }
        }
}

// processHodlQueue processes a received htlc resolution and continues reading
// from the hodl queue until no more resolutions remain. When this function
// returns without an error, the commit tx should be updated.
func (l *channelLink) processHodlQueue(ctx context.Context,
        firstResolution invoices.HtlcResolution) error <span class="cov8" title="1">{

        // Try to read all waiting resolution messages, so that they can all be
        // processed in a single commitment tx update.
        htlcResolution := firstResolution
loop:
        for </span><span class="cov8" title="1">{
                // Lookup all hodl htlcs that can be failed or settled with this event.
                // The hodl htlc must be present in the map.
                circuitKey := htlcResolution.CircuitKey()
                hodlHtlc, ok := l.hodlMap[circuitKey]
                if !ok </span><span class="cov0" title="0">{
                        return fmt.Errorf("hodl htlc not found: %v", circuitKey)
                }</span>

                <span class="cov8" title="1">if err := l.processHtlcResolution(htlcResolution, hodlHtlc); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                // Clean up hodl map.
                <span class="cov8" title="1">delete(l.hodlMap, circuitKey)

                select </span>{
                case item := &lt;-l.hodlQueue.ChanOut():<span class="cov0" title="0">
                        htlcResolution = item.(invoices.HtlcResolution)</span>

                // No need to process it if the link is broken.
                case &lt;-l.cg.Done():<span class="cov0" title="0">
                        return ErrLinkShuttingDown</span>

                default:<span class="cov8" title="1">
                        break loop</span>
                }
        }

        // Update the commitment tx.
        <span class="cov8" title="1">if err := l.updateCommitTx(ctx); err != nil </span><span class="cov8" title="1">{
                return err
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// processHtlcResolution applies a received htlc resolution to the provided
// htlc. When this function returns without an error, the commit tx should be
// updated.
func (l *channelLink) processHtlcResolution(resolution invoices.HtlcResolution,
        htlc hodlHtlc) error <span class="cov8" title="1">{

        circuitKey := resolution.CircuitKey()

        // Determine required action for the resolution based on the type of
        // resolution we have received.
        switch res := resolution.(type) </span>{
        // Settle htlcs that returned a settle resolution using the preimage
        // in the resolution.
        case *invoices.HtlcSettleResolution:<span class="cov8" title="1">
                l.log.Debugf("received settle resolution for %v "+
                        "with outcome: %v", circuitKey, res.Outcome)

                return l.settleHTLC(
                        res.Preimage, htlc.add.ID, htlc.sourceRef,
                )</span>

        // For htlc failures, we get the relevant failure message based
        // on the failure resolution and then fail the htlc.
        case *invoices.HtlcFailResolution:<span class="cov8" title="1">
                l.log.Debugf("received cancel resolution for "+
                        "%v with outcome: %v", circuitKey, res.Outcome)

                // Get the lnwire failure message based on the resolution
                // result.
                failure := getResolutionFailure(res, htlc.add.Amount)

                l.sendHTLCError(
                        htlc.add, htlc.sourceRef, failure, htlc.obfuscator,
                        true,
                )
                return nil</span>

        // Fail if we do not get a settle of fail resolution, since we
        // are only expecting to handle settles and fails.
        default:<span class="cov0" title="0">
                return fmt.Errorf("unknown htlc resolution type: %T",
                        resolution)</span>
        }
}

// getResolutionFailure returns the wire message that a htlc resolution should
// be failed with.
func getResolutionFailure(resolution *invoices.HtlcFailResolution,
        amount lnwire.MilliSatoshi) *LinkError <span class="cov8" title="1">{

        // If the resolution has been resolved as part of a MPP timeout,
        // we need to fail the htlc with lnwire.FailMppTimeout.
        if resolution.Outcome == invoices.ResultMppTimeout </span><span class="cov0" title="0">{
                return NewDetailedLinkError(
                        &amp;lnwire.FailMPPTimeout{}, resolution.Outcome,
                )
        }</span>

        // If the htlc is not a MPP timeout, we fail it with
        // FailIncorrectDetails. This error is sent for invoice payment
        // failures such as underpayment/ expiry too soon and hodl invoices
        // (which return FailIncorrectDetails to avoid leaking information).
        <span class="cov8" title="1">incorrectDetails := lnwire.NewFailIncorrectDetails(
                amount, uint32(resolution.AcceptHeight),
        )

        return NewDetailedLinkError(incorrectDetails, resolution.Outcome)</span>
}

// randomFeeUpdateTimeout returns a random timeout between the bounds defined
// within the link's configuration that will be used to determine when the link
// should propose an update to its commitment fee rate.
func (l *channelLink) randomFeeUpdateTimeout() time.Duration <span class="cov8" title="1">{
        lower := int64(l.cfg.MinUpdateTimeout)
        upper := int64(l.cfg.MaxUpdateTimeout)
        return time.Duration(prand.Int63n(upper-lower) + lower)
}</span>

// handleDownstreamUpdateAdd processes an UpdateAddHTLC packet sent from the
// downstream HTLC Switch.
func (l *channelLink) handleDownstreamUpdateAdd(ctx context.Context,
        pkt *htlcPacket) error <span class="cov8" title="1">{

        htlc, ok := pkt.htlc.(*lnwire.UpdateAddHTLC)
        if !ok </span><span class="cov0" title="0">{
                return errors.New("not an UpdateAddHTLC packet")
        }</span>

        // If we are flushing the link in the outgoing direction or we have
        // already sent Stfu, then we can't add new htlcs to the link and we
        // need to bounce it.
        <span class="cov8" title="1">if l.IsFlushing(Outgoing) || !l.quiescer.CanSendUpdates() </span><span class="cov0" title="0">{
                l.mailBox.FailAdd(pkt)

                return NewDetailedLinkError(
                        &amp;lnwire.FailTemporaryChannelFailure{},
                        OutgoingFailureLinkNotEligible,
                )
        }</span>

        // If hodl.AddOutgoing mode is active, we exit early to simulate
        // arbitrary delays between the switch adding an ADD to the
        // mailbox, and the HTLC being added to the commitment state.
        <span class="cov8" title="1">if l.cfg.HodlMask.Active(hodl.AddOutgoing) </span><span class="cov0" title="0">{
                l.log.Warnf(hodl.AddOutgoing.Warning())
                l.mailBox.AckPacket(pkt.inKey())
                return nil
        }</span>

        // Check if we can add the HTLC here without exceededing the max fee
        // exposure threshold.
        <span class="cov8" title="1">if l.isOverexposedWithHtlc(htlc, false) </span><span class="cov8" title="1">{
                l.log.Debugf("Unable to handle downstream HTLC - max fee " +
                        "exposure exceeded")

                l.mailBox.FailAdd(pkt)

                return NewDetailedLinkError(
                        lnwire.NewTemporaryChannelFailure(nil),
                        OutgoingFailureDownstreamHtlcAdd,
                )
        }</span>

        // A new payment has been initiated via the downstream channel,
        // so we add the new HTLC to our local log, then update the
        // commitment chains.
        <span class="cov8" title="1">htlc.ChanID = l.ChanID()
        openCircuitRef := pkt.inKey()

        // We enforce the fee buffer for the commitment transaction because
        // we are in control of adding this htlc. Nothing has locked-in yet so
        // we can securely enforce the fee buffer which is only relevant if we
        // are the initiator of the channel.
        index, err := l.channel.AddHTLC(htlc, &amp;openCircuitRef)
        if err != nil </span><span class="cov8" title="1">{
                // The HTLC was unable to be added to the state machine,
                // as a result, we'll signal the switch to cancel the
                // pending payment.
                l.log.Warnf("Unable to handle downstream add HTLC: %v",
                        err)

                // Remove this packet from the link's mailbox, this
                // prevents it from being reprocessed if the link
                // restarts and resets it mailbox. If this response
                // doesn't make it back to the originating link, it will
                // be rejected upon attempting to reforward the Add to
                // the switch, since the circuit was never fully opened,
                // and the forwarding package shows it as
                // unacknowledged.
                l.mailBox.FailAdd(pkt)

                return NewDetailedLinkError(
                        lnwire.NewTemporaryChannelFailure(nil),
                        OutgoingFailureDownstreamHtlcAdd,
                )
        }</span>

        <span class="cov8" title="1">l.log.Tracef("received downstream htlc: payment_hash=%x, "+
                "local_log_index=%v, pend_updates=%v",
                htlc.PaymentHash[:], index,
                l.channel.NumPendingUpdates(lntypes.Local, lntypes.Remote))

        pkt.outgoingChanID = l.ShortChanID()
        pkt.outgoingHTLCID = index
        htlc.ID = index

        l.log.Debugf("queueing keystone of ADD open circuit: %s-&gt;%s",
                pkt.inKey(), pkt.outKey())

        l.openedCircuits = append(l.openedCircuits, pkt.inKey())
        l.keystoneBatch = append(l.keystoneBatch, pkt.keystone())

        _ = l.cfg.Peer.SendMessage(false, htlc)

        // Send a forward event notification to htlcNotifier.
        l.cfg.HtlcNotifier.NotifyForwardingEvent(
                newHtlcKey(pkt),
                HtlcInfo{
                        IncomingTimeLock: pkt.incomingTimeout,
                        IncomingAmt:      pkt.incomingAmount,
                        OutgoingTimeLock: htlc.Expiry,
                        OutgoingAmt:      htlc.Amount,
                },
                getEventType(pkt),
        )

        l.tryBatchUpdateCommitTx(ctx)

        return nil</span>
}

// handleDownstreamPkt processes an HTLC packet sent from the downstream HTLC
// Switch. Possible messages sent by the switch include requests to forward new
// HTLCs, timeout previously cleared HTLCs, and finally to settle currently
// cleared HTLCs with the upstream peer.
//
// TODO(roasbeef): add sync ntfn to ensure switch always has consistent view?
func (l *channelLink) handleDownstreamPkt(ctx context.Context,
        pkt *htlcPacket) <span class="cov8" title="1">{

        if pkt.htlc.MsgType().IsChannelUpdate() &amp;&amp;
                !l.quiescer.CanSendUpdates() </span><span class="cov0" title="0">{

                l.log.Warnf("unable to process channel update. "+
                        "ChannelID=%v is quiescent.", l.ChanID)

                return
        }</span>

        <span class="cov8" title="1">switch htlc := pkt.htlc.(type) </span>{
        case *lnwire.UpdateAddHTLC:<span class="cov8" title="1">
                // Handle add message. The returned error can be ignored,
                // because it is also sent through the mailbox.
                _ = l.handleDownstreamUpdateAdd(ctx, pkt)</span>

        case *lnwire.UpdateFulfillHTLC:<span class="cov8" title="1">
                // If hodl.SettleOutgoing mode is active, we exit early to
                // simulate arbitrary delays between the switch adding the
                // SETTLE to the mailbox, and the HTLC being added to the
                // commitment state.
                if l.cfg.HodlMask.Active(hodl.SettleOutgoing) </span><span class="cov0" title="0">{
                        l.log.Warnf(hodl.SettleOutgoing.Warning())
                        l.mailBox.AckPacket(pkt.inKey())
                        return
                }</span>

                // An HTLC we forward to the switch has just settled somewhere
                // upstream. Therefore we settle the HTLC within the our local
                // state machine.
                <span class="cov8" title="1">inKey := pkt.inKey()
                err := l.channel.SettleHTLC(
                        htlc.PaymentPreimage,
                        pkt.incomingHTLCID,
                        pkt.sourceRef,
                        pkt.destRef,
                        &amp;inKey,
                )
                if err != nil </span><span class="cov0" title="0">{
                        l.log.Errorf("unable to settle incoming HTLC for "+
                                "circuit-key=%v: %v", inKey, err)

                        // If the HTLC index for Settle response was not known
                        // to our commitment state, it has already been
                        // cleaned up by a prior response. We'll thus try to
                        // clean up any lingering state to ensure we don't
                        // continue reforwarding.
                        if _, ok := err.(lnwallet.ErrUnknownHtlcIndex); ok </span><span class="cov0" title="0">{
                                l.cleanupSpuriousResponse(pkt)
                        }</span>

                        // Remove the packet from the link's mailbox to ensure
                        // it doesn't get replayed after a reconnection.
                        <span class="cov0" title="0">l.mailBox.AckPacket(inKey)

                        return</span>
                }

                <span class="cov8" title="1">l.log.Debugf("queueing removal of SETTLE closed circuit: "+
                        "%s-&gt;%s", pkt.inKey(), pkt.outKey())

                l.closedCircuits = append(l.closedCircuits, pkt.inKey())

                // With the HTLC settled, we'll need to populate the wire
                // message to target the specific channel and HTLC to be
                // canceled.
                htlc.ChanID = l.ChanID()
                htlc.ID = pkt.incomingHTLCID

                // Then we send the HTLC settle message to the connected peer
                // so we can continue the propagation of the settle message.
                l.cfg.Peer.SendMessage(false, htlc)

                // Send a settle event notification to htlcNotifier.
                l.cfg.HtlcNotifier.NotifySettleEvent(
                        newHtlcKey(pkt),
                        htlc.PaymentPreimage,
                        getEventType(pkt),
                )

                // Immediately update the commitment tx to minimize latency.
                l.updateCommitTxOrFail(ctx)</span>

        case *lnwire.UpdateFailHTLC:<span class="cov8" title="1">
                // If hodl.FailOutgoing mode is active, we exit early to
                // simulate arbitrary delays between the switch adding a FAIL to
                // the mailbox, and the HTLC being added to the commitment
                // state.
                if l.cfg.HodlMask.Active(hodl.FailOutgoing) </span><span class="cov0" title="0">{
                        l.log.Warnf(hodl.FailOutgoing.Warning())
                        l.mailBox.AckPacket(pkt.inKey())
                        return
                }</span>

                // An HTLC cancellation has been triggered somewhere upstream,
                // we'll remove then HTLC from our local state machine.
                <span class="cov8" title="1">inKey := pkt.inKey()
                err := l.channel.FailHTLC(
                        pkt.incomingHTLCID,
                        htlc.Reason,
                        pkt.sourceRef,
                        pkt.destRef,
                        &amp;inKey,
                )
                if err != nil </span><span class="cov8" title="1">{
                        l.log.Errorf("unable to cancel incoming HTLC for "+
                                "circuit-key=%v: %v", inKey, err)

                        // If the HTLC index for Fail response was not known to
                        // our commitment state, it has already been cleaned up
                        // by a prior response. We'll thus try to clean up any
                        // lingering state to ensure we don't continue
                        // reforwarding.
                        if _, ok := err.(lnwallet.ErrUnknownHtlcIndex); ok </span><span class="cov8" title="1">{
                                l.cleanupSpuriousResponse(pkt)
                        }</span>

                        // Remove the packet from the link's mailbox to ensure
                        // it doesn't get replayed after a reconnection.
                        <span class="cov8" title="1">l.mailBox.AckPacket(inKey)

                        return</span>
                }

                <span class="cov8" title="1">l.log.Debugf("queueing removal of FAIL closed circuit: %s-&gt;%s",
                        pkt.inKey(), pkt.outKey())

                l.closedCircuits = append(l.closedCircuits, pkt.inKey())

                // With the HTLC removed, we'll need to populate the wire
                // message to target the specific channel and HTLC to be
                // canceled. The "Reason" field will have already been set
                // within the switch.
                htlc.ChanID = l.ChanID()
                htlc.ID = pkt.incomingHTLCID

                // We send the HTLC message to the peer which initially created
                // the HTLC. If the incoming blinding point is non-nil, we
                // know that we are a relaying node in a blinded path.
                // Otherwise, we're either an introduction node or not part of
                // a blinded path at all.
                if err := l.sendIncomingHTLCFailureMsg(
                        htlc.ID,
                        pkt.obfuscator,
                        htlc.Reason,
                ); err != nil </span><span class="cov0" title="0">{
                        l.log.Errorf("unable to send HTLC failure: %v",
                                err)

                        return
                }</span>

                // If the packet does not have a link failure set, it failed
                // further down the route so we notify a forwarding failure.
                // Otherwise, we notify a link failure because it failed at our
                // node.
                <span class="cov8" title="1">if pkt.linkFailure != nil </span><span class="cov8" title="1">{
                        l.cfg.HtlcNotifier.NotifyLinkFailEvent(
                                newHtlcKey(pkt),
                                newHtlcInfo(pkt),
                                getEventType(pkt),
                                pkt.linkFailure,
                                false,
                        )
                }</span> else<span class="cov8" title="1"> {
                        l.cfg.HtlcNotifier.NotifyForwardingFailEvent(
                                newHtlcKey(pkt), getEventType(pkt),
                        )
                }</span>

                // Immediately update the commitment tx to minimize latency.
                <span class="cov8" title="1">l.updateCommitTxOrFail(ctx)</span>
        }
}

// tryBatchUpdateCommitTx updates the commitment transaction if the batch is
// full.
func (l *channelLink) tryBatchUpdateCommitTx(ctx context.Context) <span class="cov8" title="1">{
        pending := l.channel.NumPendingUpdates(lntypes.Local, lntypes.Remote)
        if pending &lt; uint64(l.cfg.BatchSize) </span><span class="cov8" title="1">{
                return
        }</span>

        <span class="cov8" title="1">l.updateCommitTxOrFail(ctx)</span>
}

// cleanupSpuriousResponse attempts to ack any AddRef or SettleFailRef
// associated with this packet. If successful in doing so, it will also purge
// the open circuit from the circuit map and remove the packet from the link's
// mailbox.
func (l *channelLink) cleanupSpuriousResponse(pkt *htlcPacket) <span class="cov8" title="1">{
        inKey := pkt.inKey()

        l.log.Debugf("cleaning up spurious response for incoming "+
                "circuit-key=%v", inKey)

        // If the htlc packet doesn't have a source reference, it is unsafe to
        // proceed, as skipping this ack may cause the htlc to be reforwarded.
        if pkt.sourceRef == nil </span><span class="cov8" title="1">{
                l.log.Errorf("unable to cleanup response for incoming "+
                        "circuit-key=%v, does not contain source reference",
                        inKey)
                return
        }</span>

        // If the source reference is present,  we will try to prevent this link
        // from resending the packet to the switch. To do so, we ack the AddRef
        // of the incoming HTLC belonging to this link.
        <span class="cov8" title="1">err := l.channel.AckAddHtlcs(*pkt.sourceRef)
        if err != nil </span><span class="cov0" title="0">{
                l.log.Errorf("unable to ack AddRef for incoming "+
                        "circuit-key=%v: %v", inKey, err)

                // If this operation failed, it is unsafe to attempt removal of
                // the destination reference or circuit, so we exit early. The
                // cleanup may proceed with a different packet in the future
                // that succeeds on this step.
                return
        }</span>

        // Now that we know this link will stop retransmitting Adds to the
        // switch, we can begin to teardown the response reference and circuit
        // map.
        //
        // If the packet includes a destination reference, then a response for
        // this HTLC was locked into the outgoing channel. Attempt to remove
        // this reference, so we stop retransmitting the response internally.
        // Even if this fails, we will proceed in trying to delete the circuit.
        // When retransmitting responses, the destination references will be
        // cleaned up if an open circuit is not found in the circuit map.
        <span class="cov8" title="1">if pkt.destRef != nil </span><span class="cov0" title="0">{
                err := l.channel.AckSettleFails(*pkt.destRef)
                if err != nil </span><span class="cov0" title="0">{
                        l.log.Errorf("unable to ack SettleFailRef "+
                                "for incoming circuit-key=%v: %v",
                                inKey, err)
                }</span>
        }

        <span class="cov8" title="1">l.log.Debugf("deleting circuit for incoming circuit-key=%x", inKey)

        // With all known references acked, we can now safely delete the circuit
        // from the switch's circuit map, as the state is no longer needed.
        err = l.cfg.Circuits.DeleteCircuits(inKey)
        if err != nil </span><span class="cov0" title="0">{
                l.log.Errorf("unable to delete circuit for "+
                        "circuit-key=%v: %v", inKey, err)
        }</span>
}

// handleUpstreamMsg processes wire messages related to commitment state
// updates from the upstream peer. The upstream peer is the peer whom we have a
// direct channel with, updating our respective commitment chains.
//
//nolint:funlen
func (l *channelLink) handleUpstreamMsg(ctx context.Context,
        msg lnwire.Message) <span class="cov8" title="1">{

        l.log.Tracef("receive upstream msg %v, handling now... ", msg.MsgType())
        defer l.log.Tracef("handled upstream msg %v", msg.MsgType())

        // First check if the message is an update and we are capable of
        // receiving updates right now.
        if msg.MsgType().IsChannelUpdate() &amp;&amp; !l.quiescer.CanRecvUpdates() </span><span class="cov0" title="0">{
                l.stfuFailf("update received after stfu: %T", msg)
                return
        }</span>

        <span class="cov8" title="1">switch msg := msg.(type) </span>{
        case *lnwire.UpdateAddHTLC:<span class="cov8" title="1">
                if l.IsFlushing(Incoming) </span><span class="cov0" title="0">{
                        // This is forbidden by the protocol specification.
                        // The best chance we have to deal with this is to drop
                        // the connection. This should roll back the channel
                        // state to the last CommitSig. If the remote has
                        // already sent a CommitSig we haven't received yet,
                        // channel state will be re-synchronized with a
                        // ChannelReestablish message upon reconnection and the
                        // protocol state that caused us to flush the link will
                        // be rolled back. In the event that there was some
                        // non-deterministic behavior in the remote that caused
                        // them to violate the protocol, we have a decent shot
                        // at correcting it this way, since reconnecting will
                        // put us in the cleanest possible state to try again.
                        //
                        // In addition to the above, it is possible for us to
                        // hit this case in situations where we improperly
                        // handle message ordering due to concurrency choices.
                        // An issue has been filed to address this here:
                        // https://github.com/lightningnetwork/lnd/issues/8393
                        l.failf(
                                LinkFailureError{
                                        code:             ErrInvalidUpdate,
                                        FailureAction:    LinkFailureDisconnect,
                                        PermanentFailure: false,
                                        Warning:          true,
                                },
                                "received add while link is flushing",
                        )

                        return
                }</span>

                // Disallow htlcs with blinding points set if we haven't
                // enabled the feature. This saves us from having to process
                // the onion at all, but will only catch blinded payments
                // where we are a relaying node (as the blinding point will
                // be in the payload when we're the introduction node).
                <span class="cov8" title="1">if msg.BlindingPoint.IsSome() &amp;&amp; l.cfg.DisallowRouteBlinding </span><span class="cov0" title="0">{
                        l.failf(LinkFailureError{code: ErrInvalidUpdate},
                                "blinding point included when route blinding "+
                                        "is disabled")

                        return
                }</span>

                // We have to check the limit here rather than later in the
                // switch because the counterparty can keep sending HTLC's
                // without sending a revoke. This would mean that the switch
                // check would only occur later.
                <span class="cov8" title="1">if l.isOverexposedWithHtlc(msg, true) </span><span class="cov0" title="0">{
                        l.failf(LinkFailureError{code: ErrInternalError},
                                "peer sent us an HTLC that exceeded our max "+
                                        "fee exposure")

                        return
                }</span>

                // We just received an add request from an upstream peer, so we
                // add it to our state machine, then add the HTLC to our
                // "settle" list in the event that we know the preimage.
                <span class="cov8" title="1">index, err := l.channel.ReceiveHTLC(msg)
                if err != nil </span><span class="cov0" title="0">{
                        l.failf(LinkFailureError{code: ErrInvalidUpdate},
                                "unable to handle upstream add HTLC: %v", err)
                        return
                }</span>

                <span class="cov8" title="1">l.log.Tracef("receive upstream htlc with payment hash(%x), "+
                        "assigning index: %v", msg.PaymentHash[:], index)</span>

        case *lnwire.UpdateFulfillHTLC:<span class="cov8" title="1">
                pre := msg.PaymentPreimage
                idx := msg.ID

                // Before we pipeline the settle, we'll check the set of active
                // htlc's to see if the related UpdateAddHTLC has been fully
                // locked-in.
                var lockedin bool
                htlcs := l.channel.ActiveHtlcs()
                for _, add := range htlcs </span><span class="cov8" title="1">{
                        // The HTLC will be outgoing and match idx.
                        if !add.Incoming &amp;&amp; add.HtlcIndex == idx </span><span class="cov8" title="1">{
                                lockedin = true
                                break</span>
                        }
                }

                <span class="cov8" title="1">if !lockedin </span><span class="cov8" title="1">{
                        l.failf(
                                LinkFailureError{code: ErrInvalidUpdate},
                                "unable to handle upstream settle",
                        )
                        return
                }</span>

                <span class="cov8" title="1">if err := l.channel.ReceiveHTLCSettle(pre, idx); err != nil </span><span class="cov0" title="0">{
                        l.failf(
                                LinkFailureError{
                                        code:          ErrInvalidUpdate,
                                        FailureAction: LinkFailureForceClose,
                                },
                                "unable to handle upstream settle HTLC: %v", err,
                        )
                        return
                }</span>

                <span class="cov8" title="1">settlePacket := &amp;htlcPacket{
                        outgoingChanID: l.ShortChanID(),
                        outgoingHTLCID: idx,
                        htlc: &amp;lnwire.UpdateFulfillHTLC{
                                PaymentPreimage: pre,
                        },
                }

                // Add the newly discovered preimage to our growing list of
                // uncommitted preimage. These will be written to the witness
                // cache just before accepting the next commitment signature
                // from the remote peer.
                l.uncommittedPreimages = append(l.uncommittedPreimages, pre)

                // Pipeline this settle, send it to the switch.
                go l.forwardBatch(false, settlePacket)</span>

        case *lnwire.UpdateFailMalformedHTLC:<span class="cov8" title="1">
                // Convert the failure type encoded within the HTLC fail
                // message to the proper generic lnwire error code.
                var failure lnwire.FailureMessage
                switch msg.FailureCode </span>{
                case lnwire.CodeInvalidOnionVersion:<span class="cov8" title="1">
                        failure = &amp;lnwire.FailInvalidOnionVersion{
                                OnionSHA256: msg.ShaOnionBlob,
                        }</span>
                case lnwire.CodeInvalidOnionHmac:<span class="cov0" title="0">
                        failure = &amp;lnwire.FailInvalidOnionHmac{
                                OnionSHA256: msg.ShaOnionBlob,
                        }</span>

                case lnwire.CodeInvalidOnionKey:<span class="cov0" title="0">
                        failure = &amp;lnwire.FailInvalidOnionKey{
                                OnionSHA256: msg.ShaOnionBlob,
                        }</span>

                // Handle malformed errors that are part of a blinded route.
                // This case is slightly different, because we expect every
                // relaying node in the blinded portion of the route to send
                // malformed errors. If we're also a relaying node, we're
                // likely going to switch this error out anyway for our own
                // malformed error, but we handle the case here for
                // completeness.
                case lnwire.CodeInvalidBlinding:<span class="cov0" title="0">
                        failure = &amp;lnwire.FailInvalidBlinding{
                                OnionSHA256: msg.ShaOnionBlob,
                        }</span>

                default:<span class="cov8" title="1">
                        l.log.Warnf("unexpected failure code received in "+
                                "UpdateFailMailformedHTLC: %v", msg.FailureCode)

                        // We don't just pass back the error we received from
                        // our successor. Otherwise we might report a failure
                        // that penalizes us more than needed. If the onion that
                        // we forwarded was correct, the node should have been
                        // able to send back its own failure. The node did not
                        // send back its own failure, so we assume there was a
                        // problem with the onion and report that back. We reuse
                        // the invalid onion key failure because there is no
                        // specific error for this case.
                        failure = &amp;lnwire.FailInvalidOnionKey{
                                OnionSHA256: msg.ShaOnionBlob,
                        }</span>
                }

                // With the error parsed, we'll convert the into it's opaque
                // form.
                <span class="cov8" title="1">var b bytes.Buffer
                if err := lnwire.EncodeFailure(&amp;b, failure, 0); err != nil </span><span class="cov0" title="0">{
                        l.log.Errorf("unable to encode malformed error: %v", err)
                        return
                }</span>

                // If remote side have been unable to parse the onion blob we
                // have sent to it, than we should transform the malformed HTLC
                // message to the usual HTLC fail message.
                <span class="cov8" title="1">err := l.channel.ReceiveFailHTLC(msg.ID, b.Bytes())
                if err != nil </span><span class="cov0" title="0">{
                        l.failf(LinkFailureError{code: ErrInvalidUpdate},
                                "unable to handle upstream fail HTLC: %v", err)
                        return
                }</span>

        case *lnwire.UpdateFailHTLC:<span class="cov8" title="1">
                // Verify that the failure reason is at least 256 bytes plus
                // overhead.
                const minimumFailReasonLength = lnwire.FailureMessageLength +
                        2 + 2 + 32

                if len(msg.Reason) &lt; minimumFailReasonLength </span><span class="cov8" title="1">{
                        // We've received a reason with a non-compliant length.
                        // Older nodes happily relay back these failures that
                        // may originate from a node further downstream.
                        // Therefore we can't just fail the channel.
                        //
                        // We want to be compliant ourselves, so we also can't
                        // pass back the reason unmodified. And we must make
                        // sure that we don't hit the magic length check of 260
                        // bytes in processRemoteSettleFails either.
                        //
                        // Because the reason is unreadable for the payer
                        // anyway, we just replace it by a compliant-length
                        // series of random bytes.
                        msg.Reason = make([]byte, minimumFailReasonLength)
                        _, err := crand.Read(msg.Reason[:])
                        if err != nil </span><span class="cov0" title="0">{
                                l.log.Errorf("Random generation error: %v", err)

                                return
                        }</span>
                }

                // Add fail to the update log.
                <span class="cov8" title="1">idx := msg.ID
                err := l.channel.ReceiveFailHTLC(idx, msg.Reason[:])
                if err != nil </span><span class="cov0" title="0">{
                        l.failf(LinkFailureError{code: ErrInvalidUpdate},
                                "unable to handle upstream fail HTLC: %v", err)
                        return
                }</span>

        case *lnwire.CommitSig:<span class="cov8" title="1">
                // Since we may have learned new preimages for the first time,
                // we'll add them to our preimage cache. By doing this, we
                // ensure any contested contracts watched by any on-chain
                // arbitrators can now sweep this HTLC on-chain. We delay
                // committing the preimages until just before accepting the new
                // remote commitment, as afterwards the peer won't resend the
                // Settle messages on the next channel reestablishment. Doing so
                // allows us to more effectively batch this operation, instead
                // of doing a single write per preimage.
                err := l.cfg.PreimageCache.AddPreimages(
                        l.uncommittedPreimages...,
                )
                if err != nil </span><span class="cov0" title="0">{
                        l.failf(
                                LinkFailureError{code: ErrInternalError},
                                "unable to add preimages=%v to cache: %v",
                                l.uncommittedPreimages, err,
                        )
                        return
                }</span>

                // Instead of truncating the slice to conserve memory
                // allocations, we simply set the uncommitted preimage slice to
                // nil so that a new one will be initialized if any more
                // witnesses are discovered. We do this because the maximum size
                // that the slice can occupy is 15KB, and we want to ensure we
                // release that memory back to the runtime.
                <span class="cov8" title="1">l.uncommittedPreimages = nil

                // We just received a new updates to our local commitment
                // chain, validate this new commitment, closing the link if
                // invalid.
                auxSigBlob, err := msg.CustomRecords.Serialize()
                if err != nil </span><span class="cov0" title="0">{
                        l.failf(
                                LinkFailureError{code: ErrInvalidCommitment},
                                "unable to serialize custom records: %v", err,
                        )

                        return
                }</span>
                <span class="cov8" title="1">err = l.channel.ReceiveNewCommitment(&amp;lnwallet.CommitSigs{
                        CommitSig:  msg.CommitSig,
                        HtlcSigs:   msg.HtlcSigs,
                        PartialSig: msg.PartialSig,
                        AuxSigBlob: auxSigBlob,
                })
                if err != nil </span><span class="cov0" title="0">{
                        // If we were unable to reconstruct their proposed
                        // commitment, then we'll examine the type of error. If
                        // it's an InvalidCommitSigError, then we'll send a
                        // direct error.
                        var sendData []byte
                        switch err.(type) </span>{
                        case *lnwallet.InvalidCommitSigError:<span class="cov0" title="0">
                                sendData = []byte(err.Error())</span>
                        case *lnwallet.InvalidHtlcSigError:<span class="cov0" title="0">
                                sendData = []byte(err.Error())</span>
                        }
                        <span class="cov0" title="0">l.failf(
                                LinkFailureError{
                                        code:          ErrInvalidCommitment,
                                        FailureAction: LinkFailureForceClose,
                                        SendData:      sendData,
                                },
                                "ChannelPoint(%v): unable to accept new "+
                                        "commitment: %v",
                                l.channel.ChannelPoint(), err,
                        )
                        return</span>
                }

                // As we've just accepted a new state, we'll now
                // immediately send the remote peer a revocation for our prior
                // state.
                <span class="cov8" title="1">nextRevocation, currentHtlcs, finalHTLCs, err :=
                        l.channel.RevokeCurrentCommitment()
                if err != nil </span><span class="cov0" title="0">{
                        l.log.Errorf("unable to revoke commitment: %v", err)

                        // We need to fail the channel in case revoking our
                        // local commitment does not succeed. We might have
                        // already advanced our channel state which would lead
                        // us to proceed with an unclean state.
                        //
                        // NOTE: We do not trigger a force close because this
                        // could resolve itself in case our db was just busy
                        // not accepting new transactions.
                        l.failf(
                                LinkFailureError{
                                        code:          ErrInternalError,
                                        Warning:       true,
                                        FailureAction: LinkFailureDisconnect,
                                },
                                "ChannelPoint(%v): unable to accept new "+
                                        "commitment: %v",
                                l.channel.ChannelPoint(), err,
                        )
                        return
                }</span>

                // As soon as we are ready to send our next revocation, we can
                // invoke the incoming commit hooks.
                <span class="cov8" title="1">l.RWMutex.Lock()
                l.incomingCommitHooks.invoke()
                l.RWMutex.Unlock()

                l.cfg.Peer.SendMessage(false, nextRevocation)

                // Notify the incoming htlcs of which the resolutions were
                // locked in.
                for id, settled := range finalHTLCs </span><span class="cov8" title="1">{
                        l.cfg.HtlcNotifier.NotifyFinalHtlcEvent(
                                models.CircuitKey{
                                        ChanID: l.ShortChanID(),
                                        HtlcID: id,
                                },
                                channeldb.FinalHtlcInfo{
                                        Settled:  settled,
                                        Offchain: true,
                                },
                        )
                }</span>

                // Since we just revoked our commitment, we may have a new set
                // of HTLC's on our commitment, so we'll send them using our
                // function closure NotifyContractUpdate.
                <span class="cov8" title="1">newUpdate := &amp;contractcourt.ContractUpdate{
                        HtlcKey: contractcourt.LocalHtlcSet,
                        Htlcs:   currentHtlcs,
                }
                err = l.cfg.NotifyContractUpdate(newUpdate)
                if err != nil </span><span class="cov0" title="0">{
                        l.log.Errorf("unable to notify contract update: %v",
                                err)
                        return
                }</span>

                <span class="cov8" title="1">select </span>{
                case &lt;-l.cg.Done():<span class="cov0" title="0">
                        return</span>
                default:<span class="cov8" title="1"></span>
                }

                // If the remote party initiated the state transition,
                // we'll reply with a signature to provide them with their
                // version of the latest commitment. Otherwise, both commitment
                // chains are fully synced from our PoV, then we don't need to
                // reply with a signature as both sides already have a
                // commitment with the latest accepted.
                <span class="cov8" title="1">if l.channel.OweCommitment() </span><span class="cov8" title="1">{
                        if !l.updateCommitTxOrFail(ctx) </span><span class="cov0" title="0">{
                                return
                        }</span>
                }

                // If we need to send out an Stfu, this would be the time to do
                // so.
                <span class="cov8" title="1">if l.noDanglingUpdates(lntypes.Local) </span><span class="cov8" title="1">{
                        err = l.quiescer.SendOwedStfu()
                        if err != nil </span><span class="cov0" title="0">{
                                l.stfuFailf("sendOwedStfu: %v", err.Error())
                        }</span>
                }

                // Now that we have finished processing the incoming CommitSig
                // and sent out our RevokeAndAck, we invoke the flushHooks if
                // the channel state is clean.
                <span class="cov8" title="1">l.RWMutex.Lock()
                if l.channel.IsChannelClean() </span><span class="cov8" title="1">{
                        l.flushHooks.invoke()
                }</span>
                <span class="cov8" title="1">l.RWMutex.Unlock()</span>

        case *lnwire.RevokeAndAck:<span class="cov8" title="1">
                // We've received a revocation from the remote chain, if valid,
                // this moves the remote chain forward, and expands our
                // revocation window.

                // We now process the message and advance our remote commit
                // chain.
                fwdPkg, remoteHTLCs, err := l.channel.ReceiveRevocation(msg)
                if err != nil </span><span class="cov0" title="0">{
                        // TODO(halseth): force close?
                        l.failf(
                                LinkFailureError{
                                        code:          ErrInvalidRevocation,
                                        FailureAction: LinkFailureDisconnect,
                                },
                                "unable to accept revocation: %v", err,
                        )
                        return
                }</span>

                // The remote party now has a new primary commitment, so we'll
                // update the contract court to be aware of this new set (the
                // prior old remote pending).
                <span class="cov8" title="1">newUpdate := &amp;contractcourt.ContractUpdate{
                        HtlcKey: contractcourt.RemoteHtlcSet,
                        Htlcs:   remoteHTLCs,
                }
                err = l.cfg.NotifyContractUpdate(newUpdate)
                if err != nil </span><span class="cov0" title="0">{
                        l.log.Errorf("unable to notify contract update: %v",
                                err)
                        return
                }</span>

                <span class="cov8" title="1">select </span>{
                case &lt;-l.cg.Done():<span class="cov8" title="1">
                        return</span>
                default:<span class="cov8" title="1"></span>
                }

                // If we have a tower client for this channel type, we'll
                // create a backup for the current state.
                <span class="cov8" title="1">if l.cfg.TowerClient != nil </span><span class="cov0" title="0">{
                        state := l.channel.State()
                        chanID := l.ChanID()

                        err = l.cfg.TowerClient.BackupState(
                                &amp;chanID, state.RemoteCommitment.CommitHeight-1,
                        )
                        if err != nil </span><span class="cov0" title="0">{
                                l.failf(LinkFailureError{
                                        code: ErrInternalError,
                                }, "unable to queue breach backup: %v", err)
                                return
                        }</span>
                }

                // If we can send updates then we can process adds in case we
                // are the exit hop and need to send back resolutions, or in
                // case there are validity issues with the packets. Otherwise
                // we defer the action until resume.
                //
                // We are free to process the settles and fails without this
                // check since processing those can't result in further updates
                // to this channel link.
                <span class="cov8" title="1">if l.quiescer.CanSendUpdates() </span><span class="cov8" title="1">{
                        l.processRemoteAdds(fwdPkg)
                }</span> else<span class="cov8" title="1"> {
                        l.quiescer.OnResume(func() </span><span class="cov0" title="0">{
                                l.processRemoteAdds(fwdPkg)
                        }</span>)
                }
                <span class="cov8" title="1">l.processRemoteSettleFails(fwdPkg)

                // If the link failed during processing the adds, we must
                // return to ensure we won't attempted to update the state
                // further.
                if l.failed </span><span class="cov0" title="0">{
                        return
                }</span>

                // The revocation window opened up. If there are pending local
                // updates, try to update the commit tx. Pending updates could
                // already have been present because of a previously failed
                // update to the commit tx or freshly added in by
                // processRemoteAdds. Also in case there are no local updates,
                // but there are still remote updates that are not in the remote
                // commit tx yet, send out an update.
                <span class="cov8" title="1">if l.channel.OweCommitment() </span><span class="cov8" title="1">{
                        if !l.updateCommitTxOrFail(ctx) </span><span class="cov8" title="1">{
                                return
                        }</span>
                }

                // Now that we have finished processing the RevokeAndAck, we
                // can invoke the flushHooks if the channel state is clean.
                <span class="cov8" title="1">l.RWMutex.Lock()
                if l.channel.IsChannelClean() </span><span class="cov8" title="1">{
                        l.flushHooks.invoke()
                }</span>
                <span class="cov8" title="1">l.RWMutex.Unlock()</span>

        case *lnwire.UpdateFee:<span class="cov8" title="1">
                // Check and see if their proposed fee-rate would make us
                // exceed the fee threshold.
                fee := chainfee.SatPerKWeight(msg.FeePerKw)

                isDust, err := l.exceedsFeeExposureLimit(fee)
                if err != nil </span><span class="cov0" title="0">{
                        // This shouldn't typically happen. If it does, it
                        // indicates something is wrong with our channel state.
                        l.log.Errorf("Unable to determine if fee threshold " +
                                "exceeded")
                        l.failf(LinkFailureError{code: ErrInternalError},
                                "error calculating fee exposure: %v", err)

                        return
                }</span>

                <span class="cov8" title="1">if isDust </span><span class="cov0" title="0">{
                        // The proposed fee-rate makes us exceed the fee
                        // threshold.
                        l.failf(LinkFailureError{code: ErrInternalError},
                                "fee threshold exceeded: %v", err)
                        return
                }</span>

                // We received fee update from peer. If we are the initiator we
                // will fail the channel, if not we will apply the update.
                <span class="cov8" title="1">if err := l.channel.ReceiveUpdateFee(fee); err != nil </span><span class="cov0" title="0">{
                        l.failf(LinkFailureError{code: ErrInvalidUpdate},
                                "error receiving fee update: %v", err)
                        return
                }</span>

                // Update the mailbox's feerate as well.
                <span class="cov8" title="1">l.mailBox.SetFeeRate(fee)</span>

        case *lnwire.Stfu:<span class="cov8" title="1">
                err := l.handleStfu(msg)
                if err != nil </span><span class="cov0" title="0">{
                        l.stfuFailf("handleStfu: %v", err.Error())
                }</span>

        // In the case where we receive a warning message from our peer, just
        // log it and move on. We choose not to disconnect from our peer,
        // although we "MAY" do so according to the specification.
        case *lnwire.Warning:<span class="cov8" title="1">
                l.log.Warnf("received warning message from peer: %v",
                        msg.Warning())</span>

        case *lnwire.Error:<span class="cov0" title="0">
                // Error received from remote, MUST fail channel, but should
                // only print the contents of the error message if all
                // characters are printable ASCII.
                l.failf(
                        LinkFailureError{
                                code: ErrRemoteError,

                                // TODO(halseth): we currently don't fail the
                                // channel permanently, as there are some sync
                                // issues with other implementations that will
                                // lead to them sending an error message, but
                                // we can recover from on next connection. See
                                // https://github.com/ElementsProject/lightning/issues/4212
                                PermanentFailure: false,
                        },
                        "ChannelPoint(%v): received error from peer: %v",
                        l.channel.ChannelPoint(), msg.Error(),
                )</span>
        default:<span class="cov0" title="0">
                l.log.Warnf("received unknown message of type %T", msg)</span>
        }

}

// handleStfu implements the top-level logic for handling the Stfu message from
// our peer.
func (l *channelLink) handleStfu(stfu *lnwire.Stfu) error <span class="cov8" title="1">{
        if !l.noDanglingUpdates(lntypes.Remote) </span><span class="cov0" title="0">{
                return ErrPendingRemoteUpdates
        }</span>
        <span class="cov8" title="1">err := l.quiescer.RecvStfu(*stfu)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // If we can immediately send an Stfu response back, we will.
        <span class="cov8" title="1">if l.noDanglingUpdates(lntypes.Local) </span><span class="cov8" title="1">{
                return l.quiescer.SendOwedStfu()
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// stfuFailf fails the link in the case where the requirements of the quiescence
// protocol are violated. In all cases we opt to drop the connection as only
// link state (as opposed to channel state) is affected.
func (l *channelLink) stfuFailf(format string, args ...interface{}) <span class="cov0" title="0">{
        l.failf(LinkFailureError{
                code:             ErrStfuViolation,
                FailureAction:    LinkFailureDisconnect,
                PermanentFailure: false,
                Warning:          true,
        }, format, args...)
}</span>

// noDanglingUpdates returns true when there are 0 updates that were originally
// issued by whose on either the Local or Remote commitment transaction.
func (l *channelLink) noDanglingUpdates(whose lntypes.ChannelParty) bool <span class="cov8" title="1">{
        pendingOnLocal := l.channel.NumPendingUpdates(
                whose, lntypes.Local,
        )
        pendingOnRemote := l.channel.NumPendingUpdates(
                whose, lntypes.Remote,
        )

        return pendingOnLocal == 0 &amp;&amp; pendingOnRemote == 0
}</span>

// ackDownStreamPackets is responsible for removing htlcs from a link's mailbox
// for packets delivered from server, and cleaning up any circuits closed by
// signing a previous commitment txn. This method ensures that the circuits are
// removed from the circuit map before removing them from the link's mailbox,
// otherwise it could be possible for some circuit to be missed if this link
// flaps.
func (l *channelLink) ackDownStreamPackets() error <span class="cov8" title="1">{
        // First, remove the downstream Add packets that were included in the
        // previous commitment signature. This will prevent the Adds from being
        // replayed if this link disconnects.
        for _, inKey := range l.openedCircuits </span><span class="cov8" title="1">{
                // In order to test the sphinx replay logic of the remote
                // party, unsafe replay does not acknowledge the packets from
                // the mailbox. We can then force a replay of any Add packets
                // held in memory by disconnecting and reconnecting the link.
                if l.cfg.UnsafeReplay </span><span class="cov0" title="0">{
                        continue</span>
                }

                <span class="cov8" title="1">l.log.Debugf("removing Add packet %s from mailbox", inKey)
                l.mailBox.AckPacket(inKey)</span>
        }

        // Now, we will delete all circuits closed by the previous commitment
        // signature, which is the result of downstream Settle/Fail packets. We
        // batch them here to ensure circuits are closed atomically and for
        // performance.
        <span class="cov8" title="1">err := l.cfg.Circuits.DeleteCircuits(l.closedCircuits...)
        switch err </span>{
        case nil:<span class="cov8" title="1"></span>
                // Successful deletion.

        default:<span class="cov0" title="0">
                l.log.Errorf("unable to delete %d circuits: %v",
                        len(l.closedCircuits), err)
                return err</span>
        }

        // With the circuits removed from memory and disk, we now ack any
        // Settle/Fails in the mailbox to ensure they do not get redelivered
        // after startup. If forgive is enabled and we've reached this point,
        // the circuits must have been removed at some point, so it is now safe
        // to un-queue the corresponding Settle/Fails.
        <span class="cov8" title="1">for _, inKey := range l.closedCircuits </span><span class="cov8" title="1">{
                l.log.Debugf("removing Fail/Settle packet %s from mailbox",
                        inKey)
                l.mailBox.AckPacket(inKey)
        }</span>

        // Lastly, reset our buffers to be empty while keeping any acquired
        // growth in the backing array.
        <span class="cov8" title="1">l.openedCircuits = l.openedCircuits[:0]
        l.closedCircuits = l.closedCircuits[:0]

        return nil</span>
}

// updateCommitTxOrFail updates the commitment tx and if that fails, it fails
// the link.
func (l *channelLink) updateCommitTxOrFail(ctx context.Context) bool <span class="cov8" title="1">{
        err := l.updateCommitTx(ctx)
        switch err </span>{
        // No error encountered, success.
        case nil:<span class="cov8" title="1"></span>

        // A duplicate keystone error should be resolved and is not fatal, so
        // we won't send an Error message to the peer.
        case ErrDuplicateKeystone:<span class="cov0" title="0">
                l.failf(LinkFailureError{code: ErrCircuitError},
                        "temporary circuit error: %v", err)
                return false</span>

        // Any other error is treated results in an Error message being sent to
        // the peer.
        default:<span class="cov8" title="1">
                l.failf(LinkFailureError{code: ErrInternalError},
                        "unable to update commitment: %v", err)
                return false</span>
        }

        <span class="cov8" title="1">return true</span>
}

// updateCommitTx signs, then sends an update to the remote peer adding a new
// commitment to their commitment chain which includes all the latest updates
// we've received+processed up to this point.
func (l *channelLink) updateCommitTx(ctx context.Context) error <span class="cov8" title="1">{
        // Preemptively write all pending keystones to disk, just in case the
        // HTLCs we have in memory are included in the subsequent attempt to
        // sign a commitment state.
        err := l.cfg.Circuits.OpenCircuits(l.keystoneBatch...)
        if err != nil </span><span class="cov0" title="0">{
                // If ErrDuplicateKeystone is returned, the caller will catch
                // it.
                return err
        }</span>

        // Reset the batch, but keep the backing buffer to avoid reallocating.
        <span class="cov8" title="1">l.keystoneBatch = l.keystoneBatch[:0]

        // If hodl.Commit mode is active, we will refrain from attempting to
        // commit any in-memory modifications to the channel state. Exiting here
        // permits testing of either the switch or link's ability to trim
        // circuits that have been opened, but unsuccessfully committed.
        if l.cfg.HodlMask.Active(hodl.Commit) </span><span class="cov8" title="1">{
                l.log.Warnf(hodl.Commit.Warning())
                return nil
        }</span>

        <span class="cov8" title="1">ctx, done := l.cg.Create(ctx)
        defer done()

        newCommit, err := l.channel.SignNextCommitment(ctx)
        if err == lnwallet.ErrNoWindow </span><span class="cov8" title="1">{
                l.cfg.PendingCommitTicker.Resume()
                l.log.Trace("PendingCommitTicker resumed")

                n := l.channel.NumPendingUpdates(lntypes.Local, lntypes.Remote)
                l.log.Tracef("revocation window exhausted, unable to send: "+
                        "%v, pend_updates=%v, dangling_closes%v", n,
                        lnutils.SpewLogClosure(l.openedCircuits),
                        lnutils.SpewLogClosure(l.closedCircuits))

                return nil
        }</span> else<span class="cov8" title="1"> if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">if err := l.ackDownStreamPackets(); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">l.cfg.PendingCommitTicker.Pause()
        l.log.Trace("PendingCommitTicker paused after ackDownStreamPackets")

        // The remote party now has a new pending commitment, so we'll update
        // the contract court to be aware of this new set (the prior old remote
        // pending).
        newUpdate := &amp;contractcourt.ContractUpdate{
                HtlcKey: contractcourt.RemotePendingHtlcSet,
                Htlcs:   newCommit.PendingHTLCs,
        }
        err = l.cfg.NotifyContractUpdate(newUpdate)
        if err != nil </span><span class="cov0" title="0">{
                l.log.Errorf("unable to notify contract update: %v", err)
                return err
        }</span>

        <span class="cov8" title="1">select </span>{
        case &lt;-l.cg.Done():<span class="cov8" title="1">
                return ErrLinkShuttingDown</span>
        default:<span class="cov8" title="1"></span>
        }

        <span class="cov8" title="1">auxBlobRecords, err := lnwire.ParseCustomRecords(newCommit.AuxSigBlob)
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("error parsing aux sigs: %w", err)
        }</span>

        <span class="cov8" title="1">commitSig := &amp;lnwire.CommitSig{
                ChanID:        l.ChanID(),
                CommitSig:     newCommit.CommitSig,
                HtlcSigs:      newCommit.HtlcSigs,
                PartialSig:    newCommit.PartialSig,
                CustomRecords: auxBlobRecords,
        }
        l.cfg.Peer.SendMessage(false, commitSig)

        // Now that we have sent out a new CommitSig, we invoke the outgoing set
        // of commit hooks.
        l.RWMutex.Lock()
        l.outgoingCommitHooks.invoke()
        l.RWMutex.Unlock()

        return nil</span>
}

// Peer returns the representation of remote peer with which we have the
// channel link opened.
//
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) PeerPubKey() [33]byte <span class="cov8" title="1">{
        return l.cfg.Peer.PubKey()
}</span>

// ChannelPoint returns the channel outpoint for the channel link.
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) ChannelPoint() wire.OutPoint <span class="cov8" title="1">{
        return l.channel.ChannelPoint()
}</span>

// ShortChanID returns the short channel ID for the channel link. The short
// channel ID encodes the exact location in the main chain that the original
// funding output can be found.
//
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) ShortChanID() lnwire.ShortChannelID <span class="cov8" title="1">{
        l.RLock()
        defer l.RUnlock()

        return l.channel.ShortChanID()
}</span>

// UpdateShortChanID updates the short channel ID for a link. This may be
// required in the event that a link is created before the short chan ID for it
// is known, or a re-org occurs, and the funding transaction changes location
// within the chain.
//
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) UpdateShortChanID() (lnwire.ShortChannelID, error) <span class="cov0" title="0">{
        chanID := l.ChanID()

        // Refresh the channel state's short channel ID by loading it from disk.
        // This ensures that the channel state accurately reflects the updated
        // short channel ID.
        err := l.channel.State().Refresh()
        if err != nil </span><span class="cov0" title="0">{
                l.log.Errorf("unable to refresh short_chan_id for chan_id=%v: "+
                        "%v", chanID, err)
                return hop.Source, err
        }</span>

        <span class="cov0" title="0">return hop.Source, nil</span>
}

// ChanID returns the channel ID for the channel link. The channel ID is a more
// compact representation of a channel's full outpoint.
//
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) ChanID() lnwire.ChannelID <span class="cov8" title="1">{
        return lnwire.NewChanIDFromOutPoint(l.channel.ChannelPoint())
}</span>

// Bandwidth returns the total amount that can flow through the channel link at
// this given instance. The value returned is expressed in millisatoshi and can
// be used by callers when making forwarding decisions to determine if a link
// can accept an HTLC.
//
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) Bandwidth() lnwire.MilliSatoshi <span class="cov8" title="1">{
        // Get the balance available on the channel for new HTLCs. This takes
        // the channel reserve into account so HTLCs up to this value won't
        // violate it.
        return l.channel.AvailableBalance()
}</span>

// MayAddOutgoingHtlc indicates whether we can add an outgoing htlc with the
// amount provided to the link. This check does not reserve a space, since
// forwards or other payments may use the available slot, so it should be
// considered best-effort.
func (l *channelLink) MayAddOutgoingHtlc(amt lnwire.MilliSatoshi) error <span class="cov0" title="0">{
        return l.channel.MayAddOutgoingHtlc(amt)
}</span>

// getDustSum is a wrapper method that calls the underlying channel's dust sum
// method.
//
// NOTE: Part of the dustHandler interface.
func (l *channelLink) getDustSum(whoseCommit lntypes.ChannelParty,
        dryRunFee fn.Option[chainfee.SatPerKWeight]) lnwire.MilliSatoshi <span class="cov8" title="1">{

        return l.channel.GetDustSum(whoseCommit, dryRunFee)
}</span>

// getFeeRate is a wrapper method that retrieves the underlying channel's
// feerate.
//
// NOTE: Part of the dustHandler interface.
func (l *channelLink) getFeeRate() chainfee.SatPerKWeight <span class="cov8" title="1">{
        return l.channel.CommitFeeRate()
}</span>

// getDustClosure returns a closure that can be used by the switch or mailbox
// to evaluate whether a given HTLC is dust.
//
// NOTE: Part of the dustHandler interface.
func (l *channelLink) getDustClosure() dustClosure <span class="cov8" title="1">{
        localDustLimit := l.channel.State().LocalChanCfg.DustLimit
        remoteDustLimit := l.channel.State().RemoteChanCfg.DustLimit
        chanType := l.channel.State().ChanType

        return dustHelper(chanType, localDustLimit, remoteDustLimit)
}</span>

// getCommitFee returns either the local or remote CommitFee in satoshis. This
// is used so that the Switch can have access to the commitment fee without
// needing to have a *LightningChannel. This doesn't include dust.
//
// NOTE: Part of the dustHandler interface.
func (l *channelLink) getCommitFee(remote bool) btcutil.Amount <span class="cov8" title="1">{
        if remote </span><span class="cov8" title="1">{
                return l.channel.State().RemoteCommitment.CommitFee
        }</span>

        <span class="cov8" title="1">return l.channel.State().LocalCommitment.CommitFee</span>
}

// exceedsFeeExposureLimit returns whether or not the new proposed fee-rate
// increases the total dust and fees within the channel past the configured
// fee threshold. It first calculates the dust sum over every update in the
// update log with the proposed fee-rate and taking into account both the local
// and remote dust limits. It uses every update in the update log instead of
// what is actually on the local and remote commitments because it is assumed
// that in a worst-case scenario, every update in the update log could
// theoretically be on either commitment transaction and this needs to be
// accounted for with this fee-rate. It then calculates the local and remote
// commitment fees given the proposed fee-rate. Finally, it tallies the results
// and determines if the fee threshold has been exceeded.
func (l *channelLink) exceedsFeeExposureLimit(
        feePerKw chainfee.SatPerKWeight) (bool, error) <span class="cov8" title="1">{

        dryRunFee := fn.Some[chainfee.SatPerKWeight](feePerKw)

        // Get the sum of dust for both the local and remote commitments using
        // this "dry-run" fee.
        localDustSum := l.getDustSum(lntypes.Local, dryRunFee)
        remoteDustSum := l.getDustSum(lntypes.Remote, dryRunFee)

        // Calculate the local and remote commitment fees using this dry-run
        // fee.
        localFee, remoteFee, err := l.channel.CommitFeeTotalAt(feePerKw)
        if err != nil </span><span class="cov0" title="0">{
                return false, err
        }</span>

        // Finally, check whether the max fee exposure was exceeded on either
        // future commitment transaction with the fee-rate.
        <span class="cov8" title="1">totalLocalDust := localDustSum + lnwire.NewMSatFromSatoshis(localFee)
        if totalLocalDust &gt; l.cfg.MaxFeeExposure </span><span class="cov0" title="0">{
                l.log.Debugf("ChannelLink(%v): exceeds fee exposure limit: "+
                        "local dust: %v, local fee: %v", l.ShortChanID(),
                        totalLocalDust, localFee)

                return true, nil
        }</span>

        <span class="cov8" title="1">totalRemoteDust := remoteDustSum + lnwire.NewMSatFromSatoshis(
                remoteFee,
        )

        if totalRemoteDust &gt; l.cfg.MaxFeeExposure </span><span class="cov0" title="0">{
                l.log.Debugf("ChannelLink(%v): exceeds fee exposure limit: "+
                        "remote dust: %v, remote fee: %v", l.ShortChanID(),
                        totalRemoteDust, remoteFee)

                return true, nil
        }</span>

        <span class="cov8" title="1">return false, nil</span>
}

// isOverexposedWithHtlc calculates whether the proposed HTLC will make the
// channel exceed the fee threshold. It first fetches the largest fee-rate that
// may be on any unrevoked commitment transaction. Then, using this fee-rate,
// determines if the to-be-added HTLC is dust. If the HTLC is dust, it adds to
// the overall dust sum. If it is not dust, it contributes to weight, which
// also adds to the overall dust sum by an increase in fees. If the dust sum on
// either commitment exceeds the configured fee threshold, this function
// returns true.
func (l *channelLink) isOverexposedWithHtlc(htlc *lnwire.UpdateAddHTLC,
        incoming bool) bool <span class="cov8" title="1">{

        dustClosure := l.getDustClosure()

        feeRate := l.channel.WorstCaseFeeRate()

        amount := htlc.Amount.ToSatoshis()

        // See if this HTLC is dust on both the local and remote commitments.
        isLocalDust := dustClosure(feeRate, incoming, lntypes.Local, amount)
        isRemoteDust := dustClosure(feeRate, incoming, lntypes.Remote, amount)

        // Calculate the dust sum for the local and remote commitments.
        localDustSum := l.getDustSum(
                lntypes.Local, fn.None[chainfee.SatPerKWeight](),
        )
        remoteDustSum := l.getDustSum(
                lntypes.Remote, fn.None[chainfee.SatPerKWeight](),
        )

        // Grab the larger of the local and remote commitment fees w/o dust.
        commitFee := l.getCommitFee(false)

        if l.getCommitFee(true) &gt; commitFee </span><span class="cov8" title="1">{
                commitFee = l.getCommitFee(true)
        }</span>

        <span class="cov8" title="1">commitFeeMSat := lnwire.NewMSatFromSatoshis(commitFee)

        localDustSum += commitFeeMSat
        remoteDustSum += commitFeeMSat

        // Calculate the additional fee increase if this is a non-dust HTLC.
        weight := lntypes.WeightUnit(input.HTLCWeight)
        additional := lnwire.NewMSatFromSatoshis(
                feeRate.FeeForWeight(weight),
        )

        if isLocalDust </span><span class="cov8" title="1">{
                // If this is dust, it doesn't contribute to weight but does
                // contribute to the overall dust sum.
                localDustSum += lnwire.NewMSatFromSatoshis(amount)
        }</span> else<span class="cov8" title="1"> {
                // Account for the fee increase that comes with an increase in
                // weight.
                localDustSum += additional
        }</span>

        <span class="cov8" title="1">if localDustSum &gt; l.cfg.MaxFeeExposure </span><span class="cov8" title="1">{
                // The max fee exposure was exceeded.
                l.log.Debugf("ChannelLink(%v): HTLC %v makes the channel "+
                        "overexposed, total local dust: %v (current commit "+
                        "fee: %v)", l.ShortChanID(), htlc, localDustSum)

                return true
        }</span>

        <span class="cov8" title="1">if isRemoteDust </span><span class="cov8" title="1">{
                // If this is dust, it doesn't contribute to weight but does
                // contribute to the overall dust sum.
                remoteDustSum += lnwire.NewMSatFromSatoshis(amount)
        }</span> else<span class="cov8" title="1"> {
                // Account for the fee increase that comes with an increase in
                // weight.
                remoteDustSum += additional
        }</span>

        <span class="cov8" title="1">if remoteDustSum &gt; l.cfg.MaxFeeExposure </span><span class="cov0" title="0">{
                // The max fee exposure was exceeded.
                l.log.Debugf("ChannelLink(%v): HTLC %v makes the channel "+
                        "overexposed, total remote dust: %v (current commit "+
                        "fee: %v)", l.ShortChanID(), htlc, remoteDustSum)

                return true
        }</span>

        <span class="cov8" title="1">return false</span>
}

// dustClosure is a function that evaluates whether an HTLC is dust. It returns
// true if the HTLC is dust. It takes in a feerate, a boolean denoting whether
// the HTLC is incoming (i.e. one that the remote sent), a boolean denoting
// whether to evaluate on the local or remote commit, and finally an HTLC
// amount to test.
type dustClosure func(feerate chainfee.SatPerKWeight, incoming bool,
        whoseCommit lntypes.ChannelParty, amt btcutil.Amount) bool

// dustHelper is used to construct the dustClosure.
func dustHelper(chantype channeldb.ChannelType, localDustLimit,
        remoteDustLimit btcutil.Amount) dustClosure <span class="cov8" title="1">{

        isDust := func(feerate chainfee.SatPerKWeight, incoming bool,
                whoseCommit lntypes.ChannelParty, amt btcutil.Amount) bool </span><span class="cov8" title="1">{

                var dustLimit btcutil.Amount
                if whoseCommit.IsLocal() </span><span class="cov8" title="1">{
                        dustLimit = localDustLimit
                }</span> else<span class="cov8" title="1"> {
                        dustLimit = remoteDustLimit
                }</span>

                <span class="cov8" title="1">return lnwallet.HtlcIsDust(
                        chantype, incoming, whoseCommit, feerate, amt,
                        dustLimit,
                )</span>
        }

        <span class="cov8" title="1">return isDust</span>
}

// zeroConfConfirmed returns whether or not the zero-conf channel has
// confirmed on-chain.
//
// Part of the scidAliasHandler interface.
func (l *channelLink) zeroConfConfirmed() bool <span class="cov8" title="1">{
        return l.channel.State().ZeroConfConfirmed()
}</span>

// confirmedScid returns the confirmed SCID for a zero-conf channel. This
// should not be called for non-zero-conf channels.
//
// Part of the scidAliasHandler interface.
func (l *channelLink) confirmedScid() lnwire.ShortChannelID <span class="cov8" title="1">{
        return l.channel.State().ZeroConfRealScid()
}</span>

// isZeroConf returns whether or not the underlying channel is a zero-conf
// channel.
//
// Part of the scidAliasHandler interface.
func (l *channelLink) isZeroConf() bool <span class="cov8" title="1">{
        return l.channel.State().IsZeroConf()
}</span>

// negotiatedAliasFeature returns whether or not the underlying channel has
// negotiated the option-scid-alias feature bit. This will be true for both
// option-scid-alias and zero-conf channel-types. It will also be true for
// channels with the feature bit but without the above channel-types.
//
// Part of the scidAliasFeature interface.
func (l *channelLink) negotiatedAliasFeature() bool <span class="cov8" title="1">{
        return l.channel.State().NegotiatedAliasFeature()
}</span>

// getAliases returns the set of aliases for the underlying channel.
//
// Part of the scidAliasHandler interface.
func (l *channelLink) getAliases() []lnwire.ShortChannelID <span class="cov8" title="1">{
        return l.cfg.GetAliases(l.ShortChanID())
}</span>

// attachFailAliasUpdate sets the link's FailAliasUpdate function.
//
// Part of the scidAliasHandler interface.
func (l *channelLink) attachFailAliasUpdate(closure func(
        sid lnwire.ShortChannelID, incoming bool) *lnwire.ChannelUpdate1) <span class="cov8" title="1">{

        l.Lock()
        l.cfg.FailAliasUpdate = closure
        l.Unlock()
}</span>

// AttachMailBox updates the current mailbox used by this link, and hooks up
// the mailbox's message and packet outboxes to the link's upstream and
// downstream chans, respectively.
func (l *channelLink) AttachMailBox(mailbox MailBox) <span class="cov8" title="1">{
        l.Lock()
        l.mailBox = mailbox
        l.upstream = mailbox.MessageOutBox()
        l.downstream = mailbox.PacketOutBox()
        l.Unlock()

        // Set the mailbox's fee rate. This may be refreshing a feerate that was
        // never committed.
        l.mailBox.SetFeeRate(l.getFeeRate())

        // Also set the mailbox's dust closure so that it can query whether HTLC's
        // are dust given the current feerate.
        l.mailBox.SetDustClosure(l.getDustClosure())
}</span>

// UpdateForwardingPolicy updates the forwarding policy for the target
// ChannelLink. Once updated, the link will use the new forwarding policy to
// govern if it an incoming HTLC should be forwarded or not. We assume that
// fields that are zero are intentionally set to zero, so we'll use newPolicy to
// update all of the link's FwrdingPolicy's values.
//
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) UpdateForwardingPolicy(
        newPolicy models.ForwardingPolicy) <span class="cov8" title="1">{

        l.Lock()
        defer l.Unlock()

        l.cfg.FwrdingPolicy = newPolicy
}</span>

// CheckHtlcForward should return a nil error if the passed HTLC details
// satisfy the current forwarding policy fo the target link. Otherwise,
// a LinkError with a valid protocol failure message should be returned
// in order to signal to the source of the HTLC, the policy consistency
// issue.
//
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) CheckHtlcForward(payHash [32]byte, incomingHtlcAmt,
        amtToForward lnwire.MilliSatoshi, incomingTimeout,
        outgoingTimeout uint32, inboundFee models.InboundFee,
        heightNow uint32, originalScid lnwire.ShortChannelID,
        customRecords lnwire.CustomRecords) *LinkError <span class="cov8" title="1">{

        l.RLock()
        policy := l.cfg.FwrdingPolicy
        l.RUnlock()

        // Using the outgoing HTLC amount, we'll calculate the outgoing
        // fee this incoming HTLC must carry in order to satisfy the constraints
        // of the outgoing link.
        outFee := ExpectedFee(policy, amtToForward)

        // Then calculate the inbound fee that we charge based on the sum of
        // outgoing HTLC amount and outgoing fee.
        inFee := inboundFee.CalcFee(amtToForward + outFee)

        // Add up both fee components. It is important to calculate both fees
        // separately. An alternative way of calculating is to first determine
        // an aggregate fee and apply that to the outgoing HTLC amount. However,
        // rounding may cause the result to be slightly higher than in the case
        // of separately rounded fee components. This potentially causes failed
        // forwards for senders and is something to be avoided.
        expectedFee := inFee + int64(outFee)

        // If the actual fee is less than our expected fee, then we'll reject
        // this HTLC as it didn't provide a sufficient amount of fees, or the
        // values have been tampered with, or the send used incorrect/dated
        // information to construct the forwarding information for this hop. In
        // any case, we'll cancel this HTLC.
        actualFee := int64(incomingHtlcAmt) - int64(amtToForward)
        if incomingHtlcAmt &lt; amtToForward || actualFee &lt; expectedFee </span><span class="cov8" title="1">{
                l.log.Warnf("outgoing htlc(%x) has insufficient fee: "+
                        "expected %v, got %v: incoming=%v, outgoing=%v, "+
                        "inboundFee=%v",
                        payHash[:], expectedFee, actualFee,
                        incomingHtlcAmt, amtToForward, inboundFee,
                )

                // As part of the returned error, we'll send our latest routing
                // policy so the sending node obtains the most up to date data.
                cb := func(upd *lnwire.ChannelUpdate1) lnwire.FailureMessage </span><span class="cov8" title="1">{
                        return lnwire.NewFeeInsufficient(amtToForward, *upd)
                }</span>
                <span class="cov8" title="1">failure := l.createFailureWithUpdate(false, originalScid, cb)
                return NewLinkError(failure)</span>
        }

        // Check whether the outgoing htlc satisfies the channel policy.
        <span class="cov8" title="1">err := l.canSendHtlc(
                policy, payHash, amtToForward, outgoingTimeout, heightNow,
                originalScid, customRecords,
        )
        if err != nil </span><span class="cov8" title="1">{
                return err
        }</span>

        // Finally, we'll ensure that the time-lock on the outgoing HTLC meets
        // the following constraint: the incoming time-lock minus our time-lock
        // delta should equal the outgoing time lock. Otherwise, whether the
        // sender messed up, or an intermediate node tampered with the HTLC.
        <span class="cov8" title="1">timeDelta := policy.TimeLockDelta
        if incomingTimeout &lt; outgoingTimeout+timeDelta </span><span class="cov8" title="1">{
                l.log.Warnf("incoming htlc(%x) has incorrect time-lock value: "+
                        "expected at least %v block delta, got %v block delta",
                        payHash[:], timeDelta, incomingTimeout-outgoingTimeout)

                // Grab the latest routing policy so the sending node is up to
                // date with our current policy.
                cb := func(upd *lnwire.ChannelUpdate1) lnwire.FailureMessage </span><span class="cov8" title="1">{
                        return lnwire.NewIncorrectCltvExpiry(
                                incomingTimeout, *upd,
                        )
                }</span>
                <span class="cov8" title="1">failure := l.createFailureWithUpdate(false, originalScid, cb)
                return NewLinkError(failure)</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// CheckHtlcTransit should return a nil error if the passed HTLC details
// satisfy the current channel policy.  Otherwise, a LinkError with a
// valid protocol failure message should be returned in order to signal
// the violation. This call is intended to be used for locally initiated
// payments for which there is no corresponding incoming htlc.
func (l *channelLink) CheckHtlcTransit(payHash [32]byte,
        amt lnwire.MilliSatoshi, timeout uint32, heightNow uint32,
        customRecords lnwire.CustomRecords) *LinkError <span class="cov8" title="1">{

        l.RLock()
        policy := l.cfg.FwrdingPolicy
        l.RUnlock()

        // We pass in hop.Source here as this is only used in the Switch when
        // trying to send over a local link. This causes the fallback mechanism
        // to occur.
        return l.canSendHtlc(
                policy, payHash, amt, timeout, heightNow, hop.Source,
                customRecords,
        )
}</span>

// canSendHtlc checks whether the given htlc parameters satisfy
// the channel's amount and time lock constraints.
func (l *channelLink) canSendHtlc(policy models.ForwardingPolicy,
        payHash [32]byte, amt lnwire.MilliSatoshi, timeout uint32,
        heightNow uint32, originalScid lnwire.ShortChannelID,
        customRecords lnwire.CustomRecords) *LinkError <span class="cov8" title="1">{

        // As our first sanity check, we'll ensure that the passed HTLC isn't
        // too small for the next hop. If so, then we'll cancel the HTLC
        // directly.
        if amt &lt; policy.MinHTLCOut </span><span class="cov8" title="1">{
                l.log.Warnf("outgoing htlc(%x) is too small: min_htlc=%v, "+
                        "htlc_value=%v", payHash[:], policy.MinHTLCOut,
                        amt)

                // As part of the returned error, we'll send our latest routing
                // policy so the sending node obtains the most up to date data.
                cb := func(upd *lnwire.ChannelUpdate1) lnwire.FailureMessage </span><span class="cov8" title="1">{
                        return lnwire.NewAmountBelowMinimum(amt, *upd)
                }</span>
                <span class="cov8" title="1">failure := l.createFailureWithUpdate(false, originalScid, cb)
                return NewLinkError(failure)</span>
        }

        // Next, ensure that the passed HTLC isn't too large. If so, we'll
        // cancel the HTLC directly.
        <span class="cov8" title="1">if policy.MaxHTLC != 0 &amp;&amp; amt &gt; policy.MaxHTLC </span><span class="cov8" title="1">{
                l.log.Warnf("outgoing htlc(%x) is too large: max_htlc=%v, "+
                        "htlc_value=%v", payHash[:], policy.MaxHTLC, amt)

                // As part of the returned error, we'll send our latest routing
                // policy so the sending node obtains the most up-to-date data.
                cb := func(upd *lnwire.ChannelUpdate1) lnwire.FailureMessage </span><span class="cov8" title="1">{
                        return lnwire.NewTemporaryChannelFailure(upd)
                }</span>
                <span class="cov8" title="1">failure := l.createFailureWithUpdate(false, originalScid, cb)
                return NewDetailedLinkError(failure, OutgoingFailureHTLCExceedsMax)</span>
        }

        // We want to avoid offering an HTLC which will expire in the near
        // future, so we'll reject an HTLC if the outgoing expiration time is
        // too close to the current height.
        <span class="cov8" title="1">if timeout &lt;= heightNow+l.cfg.OutgoingCltvRejectDelta </span><span class="cov8" title="1">{
                l.log.Warnf("htlc(%x) has an expiry that's too soon: "+
                        "outgoing_expiry=%v, best_height=%v", payHash[:],
                        timeout, heightNow)

                cb := func(upd *lnwire.ChannelUpdate1) lnwire.FailureMessage </span><span class="cov8" title="1">{
                        return lnwire.NewExpiryTooSoon(*upd)
                }</span>
                <span class="cov8" title="1">failure := l.createFailureWithUpdate(false, originalScid, cb)
                return NewLinkError(failure)</span>
        }

        // Check absolute max delta.
        <span class="cov8" title="1">if timeout &gt; l.cfg.MaxOutgoingCltvExpiry+heightNow </span><span class="cov8" title="1">{
                l.log.Warnf("outgoing htlc(%x) has a time lock too far in "+
                        "the future: got %v, but maximum is %v", payHash[:],
                        timeout-heightNow, l.cfg.MaxOutgoingCltvExpiry)

                return NewLinkError(&amp;lnwire.FailExpiryTooFar{})
        }</span>

        // We now check the available bandwidth to see if this HTLC can be
        // forwarded.
        <span class="cov8" title="1">availableBandwidth := l.Bandwidth()
        auxBandwidth, err := fn.MapOptionZ(
                l.cfg.AuxTrafficShaper,
                func(ts AuxTrafficShaper) fn.Result[OptionalBandwidth] </span><span class="cov0" title="0">{
                        var htlcBlob fn.Option[tlv.Blob]
                        blob, err := customRecords.Serialize()
                        if err != nil </span><span class="cov0" title="0">{
                                return fn.Err[OptionalBandwidth](
                                        fmt.Errorf("unable to serialize "+
                                                "custom records: %w", err))
                        }</span>

                        <span class="cov0" title="0">if len(blob) &gt; 0 </span><span class="cov0" title="0">{
                                htlcBlob = fn.Some(blob)
                        }</span>

                        <span class="cov0" title="0">return l.AuxBandwidth(amt, originalScid, htlcBlob, ts)</span>
                },
        ).Unpack()
        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                l.log.Errorf("Unable to determine aux bandwidth: %v", err)
                return NewLinkError(&amp;lnwire.FailTemporaryNodeFailure{})
        }</span>

        <span class="cov8" title="1">if auxBandwidth.IsHandled &amp;&amp; auxBandwidth.Bandwidth.IsSome() </span><span class="cov0" title="0">{
                auxBandwidth.Bandwidth.WhenSome(
                        func(bandwidth lnwire.MilliSatoshi) </span><span class="cov0" title="0">{
                                availableBandwidth = bandwidth
                        }</span>,
                )
        }

        // Check to see if there is enough balance in this channel.
        <span class="cov8" title="1">if amt &gt; availableBandwidth </span><span class="cov8" title="1">{
                l.log.Warnf("insufficient bandwidth to route htlc: %v is "+
                        "larger than %v", amt, availableBandwidth)
                cb := func(upd *lnwire.ChannelUpdate1) lnwire.FailureMessage </span><span class="cov8" title="1">{
                        return lnwire.NewTemporaryChannelFailure(upd)
                }</span>
                <span class="cov8" title="1">failure := l.createFailureWithUpdate(false, originalScid, cb)
                return NewDetailedLinkError(
                        failure, OutgoingFailureInsufficientBalance,
                )</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// AuxBandwidth returns the bandwidth that can be used for a channel, expressed
// in milli-satoshi. This might be different from the regular BTC bandwidth for
// custom channels. This will always return fn.None() for a regular (non-custom)
// channel.
func (l *channelLink) AuxBandwidth(amount lnwire.MilliSatoshi,
        cid lnwire.ShortChannelID, htlcBlob fn.Option[tlv.Blob],
        ts AuxTrafficShaper) fn.Result[OptionalBandwidth] <span class="cov0" title="0">{

        fundingBlob := l.FundingCustomBlob()
        shouldHandle, err := ts.ShouldHandleTraffic(cid, fundingBlob)
        if err != nil </span><span class="cov0" title="0">{
                return fn.Err[OptionalBandwidth](fmt.Errorf("traffic shaper "+
                        "failed to decide whether to handle traffic: %w", err))
        }</span>

        <span class="cov0" title="0">log.Debugf("ShortChannelID=%v: aux traffic shaper is handling "+
                "traffic: %v", cid, shouldHandle)

        // If this channel isn't handled by the aux traffic shaper, we'll return
        // early.
        if !shouldHandle </span><span class="cov0" title="0">{
                return fn.Ok(OptionalBandwidth{
                        IsHandled: false,
                })
        }</span>

        // Ask for a specific bandwidth to be used for the channel.
        <span class="cov0" title="0">commitmentBlob := l.CommitmentCustomBlob()
        auxBandwidth, err := ts.PaymentBandwidth(
                htlcBlob, commitmentBlob, l.Bandwidth(), amount,
        )
        if err != nil </span><span class="cov0" title="0">{
                return fn.Err[OptionalBandwidth](fmt.Errorf("failed to get "+
                        "bandwidth from external traffic shaper: %w", err))
        }</span>

        <span class="cov0" title="0">log.Debugf("ShortChannelID=%v: aux traffic shaper reported available "+
                "bandwidth: %v", cid, auxBandwidth)

        return fn.Ok(OptionalBandwidth{
                IsHandled: true,
                Bandwidth: fn.Some(auxBandwidth),
        })</span>
}

// Stats returns the statistics of channel link.
//
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) Stats() (uint64, lnwire.MilliSatoshi, lnwire.MilliSatoshi) <span class="cov8" title="1">{
        snapshot := l.channel.StateSnapshot()

        return snapshot.ChannelCommitment.CommitHeight,
                snapshot.TotalMSatSent,
                snapshot.TotalMSatReceived
}</span>

// String returns the string representation of channel link.
//
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) String() string <span class="cov0" title="0">{
        return l.channel.ChannelPoint().String()
}</span>

// handleSwitchPacket handles the switch packets. This packets which might be
// forwarded to us from another channel link in case the htlc update came from
// another peer or if the update was created by user
//
// NOTE: Part of the packetHandler interface.
func (l *channelLink) handleSwitchPacket(pkt *htlcPacket) error <span class="cov8" title="1">{
        l.log.Tracef("received switch packet inkey=%v, outkey=%v",
                pkt.inKey(), pkt.outKey())

        return l.mailBox.AddPacket(pkt)
}</span>

// HandleChannelUpdate handles the htlc requests as settle/add/fail which sent
// to us from remote peer we have a channel with.
//
// NOTE: Part of the ChannelLink interface.
func (l *channelLink) HandleChannelUpdate(message lnwire.Message) <span class="cov8" title="1">{
        select </span>{
        case &lt;-l.cg.Done():<span class="cov0" title="0">
                // Return early if the link is already in the process of
                // quitting. It doesn't make sense to hand the message to the
                // mailbox here.
                return</span>
        default:<span class="cov8" title="1"></span>
        }

        <span class="cov8" title="1">err := l.mailBox.AddMessage(message)
        if err != nil </span><span class="cov0" title="0">{
                l.log.Errorf("failed to add Message to mailbox: %v", err)
        }</span>
}

// updateChannelFee updates the commitment fee-per-kw on this channel by
// committing to an update_fee message.
func (l *channelLink) updateChannelFee(ctx context.Context,
        feePerKw chainfee.SatPerKWeight) error <span class="cov8" title="1">{

        l.log.Infof("updating commit fee to %v", feePerKw)

        // We skip sending the UpdateFee message if the channel is not
        // currently eligible to forward messages.
        if !l.eligibleToUpdate() </span><span class="cov0" title="0">{
                l.log.Debugf("skipping fee update for inactive channel")
                return nil
        }</span>

        // Check and see if our proposed fee-rate would make us exceed the fee
        // threshold.
        <span class="cov8" title="1">thresholdExceeded, err := l.exceedsFeeExposureLimit(feePerKw)
        if err != nil </span><span class="cov0" title="0">{
                // This shouldn't typically happen. If it does, it indicates
                // something is wrong with our channel state.
                return err
        }</span>

        <span class="cov8" title="1">if thresholdExceeded </span><span class="cov0" title="0">{
                return fmt.Errorf("link fee threshold exceeded")
        }</span>

        // First, we'll update the local fee on our commitment.
        <span class="cov8" title="1">if err := l.channel.UpdateFee(feePerKw); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // The fee passed the channel's validation checks, so we update the
        // mailbox feerate.
        <span class="cov8" title="1">l.mailBox.SetFeeRate(feePerKw)

        // We'll then attempt to send a new UpdateFee message, and also lock it
        // in immediately by triggering a commitment update.
        msg := lnwire.NewUpdateFee(l.ChanID(), uint32(feePerKw))
        if err := l.cfg.Peer.SendMessage(false, msg); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">return l.updateCommitTx(ctx)</span>
}

// processRemoteSettleFails accepts a batch of settle/fail payment descriptors
// after receiving a revocation from the remote party, and reprocesses them in
// the context of the provided forwarding package. Any settles or fails that
// have already been acknowledged in the forwarding package will not be sent to
// the switch.
func (l *channelLink) processRemoteSettleFails(fwdPkg *channeldb.FwdPkg) <span class="cov8" title="1">{
        if len(fwdPkg.SettleFails) == 0 </span><span class="cov8" title="1">{
                return
        }</span>

        <span class="cov8" title="1">l.log.Debugf("settle-fail-filter: %v", fwdPkg.SettleFailFilter)

        var switchPackets []*htlcPacket
        for i, update := range fwdPkg.SettleFails </span><span class="cov8" title="1">{
                destRef := fwdPkg.DestRef(uint16(i))

                // Skip any settles or fails that have already been
                // acknowledged by the incoming link that originated the
                // forwarded Add.
                if fwdPkg.SettleFailFilter.Contains(uint16(i)) </span><span class="cov0" title="0">{
                        continue</span>
                }

                // TODO(roasbeef): rework log entries to a shared
                // interface.

                <span class="cov8" title="1">switch msg := update.UpdateMsg.(type) </span>{
                // A settle for an HTLC we previously forwarded HTLC has been
                // received. So we'll forward the HTLC to the switch which will
                // handle propagating the settle to the prior hop.
                case *lnwire.UpdateFulfillHTLC:<span class="cov8" title="1">
                        // If hodl.SettleIncoming is requested, we will not
                        // forward the SETTLE to the switch and will not signal
                        // a free slot on the commitment transaction.
                        if l.cfg.HodlMask.Active(hodl.SettleIncoming) </span><span class="cov0" title="0">{
                                l.log.Warnf(hodl.SettleIncoming.Warning())
                                continue</span>
                        }

                        <span class="cov8" title="1">settlePacket := &amp;htlcPacket{
                                outgoingChanID: l.ShortChanID(),
                                outgoingHTLCID: msg.ID,
                                destRef:        &amp;destRef,
                                htlc:           msg,
                        }

                        // Add the packet to the batch to be forwarded, and
                        // notify the overflow queue that a spare spot has been
                        // freed up within the commitment state.
                        switchPackets = append(switchPackets, settlePacket)</span>

                // A failureCode message for a previously forwarded HTLC has
                // been received. As a result a new slot will be freed up in
                // our commitment state, so we'll forward this to the switch so
                // the backwards undo can continue.
                case *lnwire.UpdateFailHTLC:<span class="cov8" title="1">
                        // If hodl.SettleIncoming is requested, we will not
                        // forward the FAIL to the switch and will not signal a
                        // free slot on the commitment transaction.
                        if l.cfg.HodlMask.Active(hodl.FailIncoming) </span><span class="cov0" title="0">{
                                l.log.Warnf(hodl.FailIncoming.Warning())
                                continue</span>
                        }

                        // Fetch the reason the HTLC was canceled so we can
                        // continue to propagate it. This failure originated
                        // from another node, so the linkFailure field is not
                        // set on the packet.
                        <span class="cov8" title="1">failPacket := &amp;htlcPacket{
                                outgoingChanID: l.ShortChanID(),
                                outgoingHTLCID: msg.ID,
                                destRef:        &amp;destRef,
                                htlc:           msg,
                        }

                        l.log.Debugf("Failed to send HTLC with ID=%d", msg.ID)

                        // If the failure message lacks an HMAC (but includes
                        // the 4 bytes for encoding the message and padding
                        // lengths, then this means that we received it as an
                        // UpdateFailMalformedHTLC. As a result, we'll signal
                        // that we need to convert this error within the switch
                        // to an actual error, by encrypting it as if we were
                        // the originating hop.
                        convertedErrorSize := lnwire.FailureMessageLength + 4
                        if len(msg.Reason) == convertedErrorSize </span><span class="cov8" title="1">{
                                failPacket.convertedError = true
                        }</span>

                        // Add the packet to the batch to be forwarded, and
                        // notify the overflow queue that a spare spot has been
                        // freed up within the commitment state.
                        <span class="cov8" title="1">switchPackets = append(switchPackets, failPacket)</span>
                }
        }

        // Only spawn the task forward packets we have a non-zero number.
        <span class="cov8" title="1">if len(switchPackets) &gt; 0 </span><span class="cov8" title="1">{
                go l.forwardBatch(false, switchPackets...)
        }</span>
}

// processRemoteAdds serially processes each of the Add payment descriptors
// which have been "locked-in" by receiving a revocation from the remote party.
// The forwarding package provided instructs how to process this batch,
// indicating whether this is the first time these Adds are being processed, or
// whether we are reprocessing as a result of a failure or restart. Adds that
// have already been acknowledged in the forwarding package will be ignored.
//
//nolint:funlen
func (l *channelLink) processRemoteAdds(fwdPkg *channeldb.FwdPkg) <span class="cov8" title="1">{
        l.log.Tracef("processing %d remote adds for height %d",
                len(fwdPkg.Adds), fwdPkg.Height)

        decodeReqs := make(
                []hop.DecodeHopIteratorRequest, 0, len(fwdPkg.Adds),
        )
        for _, update := range fwdPkg.Adds </span><span class="cov8" title="1">{
                if msg, ok := update.UpdateMsg.(*lnwire.UpdateAddHTLC); ok </span><span class="cov8" title="1">{
                        // Before adding the new htlc to the state machine,
                        // parse the onion object in order to obtain the
                        // routing information with DecodeHopIterator function
                        // which process the Sphinx packet.
                        onionReader := bytes.NewReader(msg.OnionBlob[:])

                        req := hop.DecodeHopIteratorRequest{
                                OnionReader:    onionReader,
                                RHash:          msg.PaymentHash[:],
                                IncomingCltv:   msg.Expiry,
                                IncomingAmount: msg.Amount,
                                BlindingPoint:  msg.BlindingPoint,
                        }

                        decodeReqs = append(decodeReqs, req)
                }</span>
        }

        // Atomically decode the incoming htlcs, simultaneously checking for
        // replay attempts. A particular index in the returned, spare list of
        // channel iterators should only be used if the failure code at the
        // same index is lnwire.FailCodeNone.
        <span class="cov8" title="1">decodeResps, sphinxErr := l.cfg.DecodeHopIterators(
                fwdPkg.ID(), decodeReqs,
        )
        if sphinxErr != nil </span><span class="cov0" title="0">{
                l.failf(LinkFailureError{code: ErrInternalError},
                        "unable to decode hop iterators: %v", sphinxErr)
                return
        }</span>

        <span class="cov8" title="1">var switchPackets []*htlcPacket

        for i, update := range fwdPkg.Adds </span><span class="cov8" title="1">{
                idx := uint16(i)

                //nolint:forcetypeassert
                add := *update.UpdateMsg.(*lnwire.UpdateAddHTLC)
                sourceRef := fwdPkg.SourceRef(idx)

                if fwdPkg.State == channeldb.FwdStateProcessed &amp;&amp;
                        fwdPkg.AckFilter.Contains(idx) </span><span class="cov0" title="0">{

                        // If this index is already found in the ack filter,
                        // the response to this forwarding decision has already
                        // been committed by one of our commitment txns. ADDs
                        // in this state are waiting for the rest of the fwding
                        // package to get acked before being garbage collected.
                        continue</span>
                }

                // An incoming HTLC add has been full-locked in. As a result we
                // can now examine the forwarding details of the HTLC, and the
                // HTLC itself to decide if: we should forward it, cancel it,
                // or are able to settle it (and it adheres to our fee related
                // constraints).

                // Before adding the new htlc to the state machine, parse the
                // onion object in order to obtain the routing information with
                // DecodeHopIterator function which process the Sphinx packet.
                <span class="cov8" title="1">chanIterator, failureCode := decodeResps[i].Result()
                if failureCode != lnwire.CodeNone </span><span class="cov8" title="1">{
                        // If we're unable to process the onion blob then we
                        // should send the malformed htlc error to payment
                        // sender.
                        l.sendMalformedHTLCError(
                                add.ID, failureCode, add.OnionBlob, &amp;sourceRef,
                        )

                        l.log.Errorf("unable to decode onion hop "+
                                "iterator: %v", failureCode)
                        continue</span>
                }

                <span class="cov8" title="1">heightNow := l.cfg.BestHeight()

                pld, routeRole, pldErr := chanIterator.HopPayload()
                if pldErr != nil </span><span class="cov0" title="0">{
                        // If we're unable to process the onion payload, or we
                        // received invalid onion payload failure, then we
                        // should send an error back to the caller so the HTLC
                        // can be canceled.
                        var failedType uint64

                        // We need to get the underlying error value, so we
                        // can't use errors.As as suggested by the linter.
                        //nolint:errorlint
                        if e, ok := pldErr.(hop.ErrInvalidPayload); ok </span><span class="cov0" title="0">{
                                failedType = uint64(e.Type)
                        }</span>

                        // If we couldn't parse the payload, make our best
                        // effort at creating an error encrypter that knows
                        // what blinding type we were, but if we couldn't
                        // parse the payload we have no way of knowing whether
                        // we were the introduction node or not.
                        //
                        //nolint:ll
                        <span class="cov0" title="0">obfuscator, failCode := chanIterator.ExtractErrorEncrypter(
                                l.cfg.ExtractErrorEncrypter,
                                // We need our route role here because we
                                // couldn't parse or validate the payload.
                                routeRole == hop.RouteRoleIntroduction,
                        )
                        if failCode != lnwire.CodeNone </span><span class="cov0" title="0">{
                                l.log.Errorf("could not extract error "+
                                        "encrypter: %v", pldErr)

                                // We can't process this htlc, send back
                                // malformed.
                                l.sendMalformedHTLCError(
                                        add.ID, failureCode, add.OnionBlob,
                                        &amp;sourceRef,
                                )

                                continue</span>
                        }

                        // TODO: currently none of the test unit infrastructure
                        // is setup to handle TLV payloads, so testing this
                        // would require implementing a separate mock iterator
                        // for TLV payloads that also supports injecting invalid
                        // payloads. Deferring this non-trival effort till a
                        // later date
                        <span class="cov0" title="0">failure := lnwire.NewInvalidOnionPayload(failedType, 0)

                        l.sendHTLCError(
                                add, sourceRef, NewLinkError(failure),
                                obfuscator, false,
                        )

                        l.log.Errorf("unable to decode forwarding "+
                                "instructions: %v", pldErr)

                        continue</span>
                }

                // Retrieve onion obfuscator from onion blob in order to
                // produce initial obfuscation of the onion failureCode.
                <span class="cov8" title="1">obfuscator, failureCode := chanIterator.ExtractErrorEncrypter(
                        l.cfg.ExtractErrorEncrypter,
                        routeRole == hop.RouteRoleIntroduction,
                )
                if failureCode != lnwire.CodeNone </span><span class="cov8" title="1">{
                        // If we're unable to process the onion blob than we
                        // should send the malformed htlc error to payment
                        // sender.
                        l.sendMalformedHTLCError(
                                add.ID, failureCode, add.OnionBlob,
                                &amp;sourceRef,
                        )

                        l.log.Errorf("unable to decode onion "+
                                "obfuscator: %v", failureCode)

                        continue</span>
                }

                <span class="cov8" title="1">fwdInfo := pld.ForwardingInfo()

                // Check whether the payload we've just processed uses our
                // node as the introduction point (gave us a blinding key in
                // the payload itself) and fail it back if we don't support
                // route blinding.
                if fwdInfo.NextBlinding.IsSome() &amp;&amp;
                        l.cfg.DisallowRouteBlinding </span><span class="cov0" title="0">{

                        failure := lnwire.NewInvalidBlinding(
                                fn.Some(add.OnionBlob),
                        )

                        l.sendHTLCError(
                                add, sourceRef, NewLinkError(failure),
                                obfuscator, false,
                        )

                        l.log.Error("rejected htlc that uses use as an " +
                                "introduction point when we do not support " +
                                "route blinding")

                        continue</span>
                }

                <span class="cov8" title="1">switch fwdInfo.NextHop </span>{
                case hop.Exit:<span class="cov8" title="1">
                        err := l.processExitHop(
                                add, sourceRef, obfuscator, fwdInfo,
                                heightNow, pld,
                        )
                        if err != nil </span><span class="cov0" title="0">{
                                l.failf(LinkFailureError{
                                        code: ErrInternalError,
                                }, err.Error()) //nolint

                                return
                        }</span>

                // There are additional channels left within this route. So
                // we'll simply do some forwarding package book-keeping.
                default:<span class="cov8" title="1">
                        // If hodl.AddIncoming is requested, we will not
                        // validate the forwarded ADD, nor will we send the
                        // packet to the htlc switch.
                        if l.cfg.HodlMask.Active(hodl.AddIncoming) </span><span class="cov0" title="0">{
                                l.log.Warnf(hodl.AddIncoming.Warning())
                                continue</span>
                        }

                        <span class="cov8" title="1">endorseValue := l.experimentalEndorsement(
                                record.CustomSet(add.CustomRecords),
                        )
                        endorseType := uint64(
                                lnwire.ExperimentalEndorsementType,
                        )

                        switch fwdPkg.State </span>{
                        case channeldb.FwdStateProcessed:<span class="cov0" title="0">
                                // This add was not forwarded on the previous
                                // processing phase, run it through our
                                // validation pipeline to reproduce an error.
                                // This may trigger a different error due to
                                // expiring timelocks, but we expect that an
                                // error will be reproduced.
                                if !fwdPkg.FwdFilter.Contains(idx) </span><span class="cov0" title="0">{
                                        break</span>
                                }

                                // Otherwise, it was already processed, we can
                                // can collect it and continue.
                                <span class="cov0" title="0">outgoingAdd := &amp;lnwire.UpdateAddHTLC{
                                        Expiry:        fwdInfo.OutgoingCTLV,
                                        Amount:        fwdInfo.AmountToForward,
                                        PaymentHash:   add.PaymentHash,
                                        BlindingPoint: fwdInfo.NextBlinding,
                                }

                                endorseValue.WhenSome(func(e byte) </span><span class="cov0" title="0">{
                                        custRecords := map[uint64][]byte{
                                                endorseType: {e},
                                        }

                                        outgoingAdd.CustomRecords = custRecords
                                }</span>)

                                // Finally, we'll encode the onion packet for
                                // the _next_ hop using the hop iterator
                                // decoded for the current hop.
                                <span class="cov0" title="0">buf := bytes.NewBuffer(
                                        outgoingAdd.OnionBlob[0:0],
                                )

                                // We know this cannot fail, as this ADD
                                // was marked forwarded in a previous
                                // round of processing.
                                chanIterator.EncodeNextHop(buf)

                                inboundFee := l.cfg.FwrdingPolicy.InboundFee

                                //nolint:ll
                                updatePacket := &amp;htlcPacket{
                                        incomingChanID:       l.ShortChanID(),
                                        incomingHTLCID:       add.ID,
                                        outgoingChanID:       fwdInfo.NextHop,
                                        sourceRef:            &amp;sourceRef,
                                        incomingAmount:       add.Amount,
                                        amount:               outgoingAdd.Amount,
                                        htlc:                 outgoingAdd,
                                        obfuscator:           obfuscator,
                                        incomingTimeout:      add.Expiry,
                                        outgoingTimeout:      fwdInfo.OutgoingCTLV,
                                        inOnionCustomRecords: pld.CustomRecords(),
                                        inboundFee:           inboundFee,
                                        inWireCustomRecords:  add.CustomRecords.Copy(),
                                }
                                switchPackets = append(
                                        switchPackets, updatePacket,
                                )

                                continue</span>
                        }

                        // TODO(roasbeef): ensure don't accept outrageous
                        // timeout for htlc

                        // With all our forwarding constraints met, we'll
                        // create the outgoing HTLC using the parameters as
                        // specified in the forwarding info.
                        <span class="cov8" title="1">addMsg := &amp;lnwire.UpdateAddHTLC{
                                Expiry:        fwdInfo.OutgoingCTLV,
                                Amount:        fwdInfo.AmountToForward,
                                PaymentHash:   add.PaymentHash,
                                BlindingPoint: fwdInfo.NextBlinding,
                        }

                        endorseValue.WhenSome(func(e byte) </span><span class="cov8" title="1">{
                                addMsg.CustomRecords = map[uint64][]byte{
                                        endorseType: {e},
                                }
                        }</span>)

                        // Finally, we'll encode the onion packet for the
                        // _next_ hop using the hop iterator decoded for the
                        // current hop.
                        <span class="cov8" title="1">buf := bytes.NewBuffer(addMsg.OnionBlob[0:0])
                        err := chanIterator.EncodeNextHop(buf)
                        if err != nil </span><span class="cov0" title="0">{
                                l.log.Errorf("unable to encode the "+
                                        "remaining route %v", err)

                                cb := func(upd *lnwire.ChannelUpdate1) lnwire.FailureMessage </span><span class="cov0" title="0">{ //nolint:ll
                                        return lnwire.NewTemporaryChannelFailure(upd)
                                }</span>

                                <span class="cov0" title="0">failure := l.createFailureWithUpdate(
                                        true, hop.Source, cb,
                                )

                                l.sendHTLCError(
                                        add, sourceRef, NewLinkError(failure),
                                        obfuscator, false,
                                )
                                continue</span>
                        }

                        // Now that this add has been reprocessed, only append
                        // it to our list of packets to forward to the switch
                        // this is the first time processing the add. If the
                        // fwd pkg has already been processed, then we entered
                        // the above section to recreate a previous error.  If
                        // the packet had previously been forwarded, it would
                        // have been added to switchPackets at the top of this
                        // section.
                        <span class="cov8" title="1">if fwdPkg.State == channeldb.FwdStateLockedIn </span><span class="cov8" title="1">{
                                inboundFee := l.cfg.FwrdingPolicy.InboundFee

                                //nolint:ll
                                updatePacket := &amp;htlcPacket{
                                        incomingChanID:       l.ShortChanID(),
                                        incomingHTLCID:       add.ID,
                                        outgoingChanID:       fwdInfo.NextHop,
                                        sourceRef:            &amp;sourceRef,
                                        incomingAmount:       add.Amount,
                                        amount:               addMsg.Amount,
                                        htlc:                 addMsg,
                                        obfuscator:           obfuscator,
                                        incomingTimeout:      add.Expiry,
                                        outgoingTimeout:      fwdInfo.OutgoingCTLV,
                                        inOnionCustomRecords: pld.CustomRecords(),
                                        inboundFee:           inboundFee,
                                        inWireCustomRecords:  add.CustomRecords.Copy(),
                                }

                                fwdPkg.FwdFilter.Set(idx)
                                switchPackets = append(switchPackets,
                                        updatePacket)
                        }</span>
                }
        }

        // Commit the htlcs we are intending to forward if this package has not
        // been fully processed.
        <span class="cov8" title="1">if fwdPkg.State == channeldb.FwdStateLockedIn </span><span class="cov8" title="1">{
                err := l.channel.SetFwdFilter(fwdPkg.Height, fwdPkg.FwdFilter)
                if err != nil </span><span class="cov0" title="0">{
                        l.failf(LinkFailureError{code: ErrInternalError},
                                "unable to set fwd filter: %v", err)
                        return
                }</span>
        }

        <span class="cov8" title="1">if len(switchPackets) == 0 </span><span class="cov8" title="1">{
                return
        }</span>

        <span class="cov8" title="1">replay := fwdPkg.State != channeldb.FwdStateLockedIn

        l.log.Debugf("forwarding %d packets to switch: replay=%v",
                len(switchPackets), replay)

        // NOTE: This call is made synchronous so that we ensure all circuits
        // are committed in the exact order that they are processed in the link.
        // Failing to do this could cause reorderings/gaps in the range of
        // opened circuits, which violates assumptions made by the circuit
        // trimming.
        l.forwardBatch(replay, switchPackets...)</span>
}

// experimentalEndorsement returns the value to set for our outgoing
// experimental endorsement field, and a boolean indicating whether it should
// be populated on the outgoing htlc.
func (l *channelLink) experimentalEndorsement(
        customUpdateAdd record.CustomSet) fn.Option[byte] <span class="cov8" title="1">{

        // Only relay experimental signal if we are within the experiment
        // period.
        if !l.cfg.ShouldFwdExpEndorsement() </span><span class="cov0" title="0">{
                return fn.None[byte]()
        }</span>

        // If we don't have any custom records or the experimental field is
        // not set, just forward a zero value.
        <span class="cov8" title="1">if len(customUpdateAdd) == 0 </span><span class="cov8" title="1">{
                return fn.Some[byte](lnwire.ExperimentalUnendorsed)
        }</span>

        <span class="cov0" title="0">t := uint64(lnwire.ExperimentalEndorsementType)
        value, set := customUpdateAdd[t]
        if !set </span><span class="cov0" title="0">{
                return fn.Some[byte](lnwire.ExperimentalUnendorsed)
        }</span>

        // We expect at least one byte for this field, consider it invalid if
        // it has no data and just forward a zero value.
        <span class="cov0" title="0">if len(value) == 0 </span><span class="cov0" title="0">{
                return fn.Some[byte](lnwire.ExperimentalUnendorsed)
        }</span>

        // Only forward endorsed if the incoming link is endorsed.
        <span class="cov0" title="0">if value[0] == lnwire.ExperimentalEndorsed </span><span class="cov0" title="0">{
                return fn.Some[byte](lnwire.ExperimentalEndorsed)
        }</span>

        // Forward as unendorsed otherwise, including cases where we've
        // received an invalid value that uses more than 3 bits of information.
        <span class="cov0" title="0">return fn.Some[byte](lnwire.ExperimentalUnendorsed)</span>
}

// processExitHop handles an htlc for which this link is the exit hop. It
// returns a boolean indicating whether the commitment tx needs an update.
func (l *channelLink) processExitHop(add lnwire.UpdateAddHTLC,
        sourceRef channeldb.AddRef, obfuscator hop.ErrorEncrypter,
        fwdInfo hop.ForwardingInfo, heightNow uint32,
        payload invoices.Payload) error <span class="cov8" title="1">{

        // If hodl.ExitSettle is requested, we will not validate the final hop's
        // ADD, nor will we settle the corresponding invoice or respond with the
        // preimage.
        if l.cfg.HodlMask.Active(hodl.ExitSettle) </span><span class="cov8" title="1">{
                l.log.Warnf("%s for htlc(rhash=%x,htlcIndex=%v)",
                        hodl.ExitSettle.Warning(), add.PaymentHash, add.ID)

                return nil
        }</span>

        // In case the traffic shaper is active, we'll check if the HTLC has
        // custom records and skip the amount check in the onion payload below.
        <span class="cov8" title="1">isCustomHTLC := fn.MapOptionZ(
                l.cfg.AuxTrafficShaper,
                func(ts AuxTrafficShaper) bool </span><span class="cov0" title="0">{
                        return ts.IsCustomHTLC(add.CustomRecords)
                }</span>,
        )

        // As we're the exit hop, we'll double check the hop-payload included in
        // the HTLC to ensure that it was crafted correctly by the sender and
        // is compatible with the HTLC we were extended. If an external
        // validator is active we might bypass the amount check.
        <span class="cov8" title="1">if !isCustomHTLC &amp;&amp; add.Amount &lt; fwdInfo.AmountToForward </span><span class="cov8" title="1">{
                l.log.Errorf("onion payload of incoming htlc(%x) has "+
                        "incompatible value: expected &lt;=%v, got %v",
                        add.PaymentHash, add.Amount, fwdInfo.AmountToForward)

                failure := NewLinkError(
                        lnwire.NewFinalIncorrectHtlcAmount(add.Amount),
                )
                l.sendHTLCError(add, sourceRef, failure, obfuscator, true)

                return nil
        }</span>

        // We'll also ensure that our time-lock value has been computed
        // correctly.
        <span class="cov8" title="1">if add.Expiry &lt; fwdInfo.OutgoingCTLV </span><span class="cov8" title="1">{
                l.log.Errorf("onion payload of incoming htlc(%x) has "+
                        "incompatible time-lock: expected &lt;=%v, got %v",
                        add.PaymentHash, add.Expiry, fwdInfo.OutgoingCTLV)

                failure := NewLinkError(
                        lnwire.NewFinalIncorrectCltvExpiry(add.Expiry),
                )

                l.sendHTLCError(add, sourceRef, failure, obfuscator, true)

                return nil
        }</span>

        // Notify the invoiceRegistry of the exit hop htlc. If we crash right
        // after this, this code will be re-executed after restart. We will
        // receive back a resolution event.
        <span class="cov8" title="1">invoiceHash := lntypes.Hash(add.PaymentHash)

        circuitKey := models.CircuitKey{
                ChanID: l.ShortChanID(),
                HtlcID: add.ID,
        }

        event, err := l.cfg.Registry.NotifyExitHopHtlc(
                invoiceHash, add.Amount, add.Expiry, int32(heightNow),
                circuitKey, l.hodlQueue.ChanIn(), add.CustomRecords, payload,
        )
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Create a hodlHtlc struct and decide either resolved now or later.
        <span class="cov8" title="1">htlc := hodlHtlc{
                add:        add,
                sourceRef:  sourceRef,
                obfuscator: obfuscator,
        }

        // If the event is nil, the invoice is being held, so we save payment
        // descriptor for future reference.
        if event == nil </span><span class="cov8" title="1">{
                l.hodlMap[circuitKey] = htlc
                return nil
        }</span>

        // Process the received resolution.
        <span class="cov8" title="1">return l.processHtlcResolution(event, htlc)</span>
}

// settleHTLC settles the HTLC on the channel.
func (l *channelLink) settleHTLC(preimage lntypes.Preimage,
        htlcIndex uint64, sourceRef channeldb.AddRef) error <span class="cov8" title="1">{

        hash := preimage.Hash()

        l.log.Infof("settling htlc %v as exit hop", hash)

        err := l.channel.SettleHTLC(
                preimage, htlcIndex, &amp;sourceRef, nil, nil,
        )
        if err != nil </span><span class="cov0" title="0">{
                return fmt.Errorf("unable to settle htlc: %w", err)
        }</span>

        // If the link is in hodl.BogusSettle mode, replace the preimage with a
        // fake one before sending it to the peer.
        <span class="cov8" title="1">if l.cfg.HodlMask.Active(hodl.BogusSettle) </span><span class="cov0" title="0">{
                l.log.Warnf(hodl.BogusSettle.Warning())
                preimage = [32]byte{}
                copy(preimage[:], bytes.Repeat([]byte{2}, 32))
        }</span>

        // HTLC was successfully settled locally send notification about it
        // remote peer.
        <span class="cov8" title="1">l.cfg.Peer.SendMessage(false, &amp;lnwire.UpdateFulfillHTLC{
                ChanID:          l.ChanID(),
                ID:              htlcIndex,
                PaymentPreimage: preimage,
        })

        // Once we have successfully settled the htlc, notify a settle event.
        l.cfg.HtlcNotifier.NotifySettleEvent(
                HtlcKey{
                        IncomingCircuit: models.CircuitKey{
                                ChanID: l.ShortChanID(),
                                HtlcID: htlcIndex,
                        },
                },
                preimage,
                HtlcEventTypeReceive,
        )

        return nil</span>
}

// forwardBatch forwards the given htlcPackets to the switch, and waits on the
// err chan for the individual responses. This method is intended to be spawned
// as a goroutine so the responses can be handled in the background.
func (l *channelLink) forwardBatch(replay bool, packets ...*htlcPacket) <span class="cov8" title="1">{
        // Don't forward packets for which we already have a response in our
        // mailbox. This could happen if a packet fails and is buffered in the
        // mailbox, and the incoming link flaps.
        var filteredPkts = make([]*htlcPacket, 0, len(packets))
        for _, pkt := range packets </span><span class="cov8" title="1">{
                if l.mailBox.HasPacket(pkt.inKey()) </span><span class="cov0" title="0">{
                        continue</span>
                }

                <span class="cov8" title="1">filteredPkts = append(filteredPkts, pkt)</span>
        }

        <span class="cov8" title="1">err := l.cfg.ForwardPackets(l.cg.Done(), replay, filteredPkts...)
        if err != nil </span><span class="cov8" title="1">{
                log.Errorf("Unhandled error while reforwarding htlc "+
                        "settle/fail over htlcswitch: %v", err)
        }</span>
}

// sendHTLCError functions cancels HTLC and send cancel message back to the
// peer from which HTLC was received.
func (l *channelLink) sendHTLCError(add lnwire.UpdateAddHTLC,
        sourceRef channeldb.AddRef, failure *LinkError,
        e hop.ErrorEncrypter, isReceive bool) <span class="cov8" title="1">{

        reason, err := e.EncryptFirstHop(failure.WireMessage())
        if err != nil </span><span class="cov0" title="0">{
                l.log.Errorf("unable to obfuscate error: %v", err)
                return
        }</span>

        <span class="cov8" title="1">err = l.channel.FailHTLC(add.ID, reason, &amp;sourceRef, nil, nil)
        if err != nil </span><span class="cov0" title="0">{
                l.log.Errorf("unable cancel htlc: %v", err)
                return
        }</span>

        // Send the appropriate failure message depending on whether we're
        // in a blinded route or not.
        <span class="cov8" title="1">if err := l.sendIncomingHTLCFailureMsg(
                add.ID, e, reason,
        ); err != nil </span><span class="cov0" title="0">{
                l.log.Errorf("unable to send HTLC failure: %v", err)
                return
        }</span>

        // Notify a link failure on our incoming link. Outgoing htlc information
        // is not available at this point, because we have not decrypted the
        // onion, so it is excluded.
        <span class="cov8" title="1">var eventType HtlcEventType
        if isReceive </span><span class="cov8" title="1">{
                eventType = HtlcEventTypeReceive
        }</span> else<span class="cov0" title="0"> {
                eventType = HtlcEventTypeForward
        }</span>

        <span class="cov8" title="1">l.cfg.HtlcNotifier.NotifyLinkFailEvent(
                HtlcKey{
                        IncomingCircuit: models.CircuitKey{
                                ChanID: l.ShortChanID(),
                                HtlcID: add.ID,
                        },
                },
                HtlcInfo{
                        IncomingTimeLock: add.Expiry,
                        IncomingAmt:      add.Amount,
                },
                eventType,
                failure,
                true,
        )</span>
}

// sendPeerHTLCFailure handles sending a HTLC failure message back to the
// peer from which the HTLC was received. This function is primarily used to
// handle the special requirements of route blinding, specifically:
// - Forwarding nodes must switch out any errors with MalformedFailHTLC
// - Introduction nodes should return regular HTLC failure messages.
//
// It accepts the original opaque failure, which will be used in the case
// that we're not part of a blinded route and an error encrypter that'll be
// used if we are the introduction node and need to present an error as if
// we're the failing party.
func (l *channelLink) sendIncomingHTLCFailureMsg(htlcIndex uint64,
        e hop.ErrorEncrypter,
        originalFailure lnwire.OpaqueReason) error <span class="cov8" title="1">{

        var msg lnwire.Message
        switch </span>{
        // Our circuit's error encrypter will be nil if this was a locally
        // initiated payment. We can only hit a blinded error for a locally
        // initiated payment if we allow ourselves to be picked as the
        // introduction node for our own payments and in that case we
        // shouldn't reach this code. To prevent the HTLC getting stuck,
        // we fail it back and log an error.
        // code.
        case e == nil:<span class="cov0" title="0">
                msg = &amp;lnwire.UpdateFailHTLC{
                        ChanID: l.ChanID(),
                        ID:     htlcIndex,
                        Reason: originalFailure,
                }

                l.log.Errorf("Unexpected blinded failure when "+
                        "we are the sending node, incoming htlc: %v(%v)",
                        l.ShortChanID(), htlcIndex)</span>

        // For cleartext hops (ie, non-blinded/normal) we don't need any
        // transformation on the error message and can just send the original.
        case !e.Type().IsBlinded():<span class="cov8" title="1">
                msg = &amp;lnwire.UpdateFailHTLC{
                        ChanID: l.ChanID(),
                        ID:     htlcIndex,
                        Reason: originalFailure,
                }</span>

        // When we're the introduction node, we need to convert the error to
        // a UpdateFailHTLC.
        case e.Type() == hop.EncrypterTypeIntroduction:<span class="cov0" title="0">
                l.log.Debugf("Introduction blinded node switching out failure "+
                        "error: %v", htlcIndex)

                // The specification does not require that we set the onion
                // blob.
                failureMsg := lnwire.NewInvalidBlinding(
                        fn.None[[lnwire.OnionPacketSize]byte](),
                )
                reason, err := e.EncryptFirstHop(failureMsg)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov0" title="0">msg = &amp;lnwire.UpdateFailHTLC{
                        ChanID: l.ChanID(),
                        ID:     htlcIndex,
                        Reason: reason,
                }</span>

        // If we are a relaying node, we need to switch out any error that
        // we've received to a malformed HTLC error.
        case e.Type() == hop.EncrypterTypeRelaying:<span class="cov0" title="0">
                l.log.Debugf("Relaying blinded node switching out malformed "+
                        "error: %v", htlcIndex)

                msg = &amp;lnwire.UpdateFailMalformedHTLC{
                        ChanID:      l.ChanID(),
                        ID:          htlcIndex,
                        FailureCode: lnwire.CodeInvalidBlinding,
                }</span>

        default:<span class="cov0" title="0">
                return fmt.Errorf("unexpected encrypter: %d", e)</span>
        }

        <span class="cov8" title="1">if err := l.cfg.Peer.SendMessage(false, msg); err != nil </span><span class="cov0" title="0">{
                l.log.Warnf("Send update fail failed: %v", err)
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// sendMalformedHTLCError helper function which sends the malformed HTLC update
// to the payment sender.
func (l *channelLink) sendMalformedHTLCError(htlcIndex uint64,
        code lnwire.FailCode, onionBlob [lnwire.OnionPacketSize]byte,
        sourceRef *channeldb.AddRef) <span class="cov8" title="1">{

        shaOnionBlob := sha256.Sum256(onionBlob[:])
        err := l.channel.MalformedFailHTLC(htlcIndex, code, shaOnionBlob, sourceRef)
        if err != nil </span><span class="cov0" title="0">{
                l.log.Errorf("unable cancel htlc: %v", err)
                return
        }</span>

        <span class="cov8" title="1">l.cfg.Peer.SendMessage(false, &amp;lnwire.UpdateFailMalformedHTLC{
                ChanID:       l.ChanID(),
                ID:           htlcIndex,
                ShaOnionBlob: shaOnionBlob,
                FailureCode:  code,
        })</span>
}

// failf is a function which is used to encapsulate the action necessary for
// properly failing the link. It takes a LinkFailureError, which will be passed
// to the OnChannelFailure closure, in order for it to determine if we should
// force close the channel, and if we should send an error message to the
// remote peer.
func (l *channelLink) failf(linkErr LinkFailureError, format string,
        a ...interface{}) <span class="cov8" title="1">{

        reason := fmt.Errorf(format, a...)

        // Return if we have already notified about a failure.
        if l.failed </span><span class="cov0" title="0">{
                l.log.Warnf("ignoring link failure (%v), as link already "+
                        "failed", reason)
                return
        }</span>

        <span class="cov8" title="1">l.log.Errorf("failing link: %s with error: %v", reason, linkErr)

        // Set failed, such that we won't process any more updates, and notify
        // the peer about the failure.
        l.failed = true
        l.cfg.OnChannelFailure(l.ChanID(), l.ShortChanID(), linkErr)</span>
}

// FundingCustomBlob returns the custom funding blob of the channel that this
// link is associated with. The funding blob represents static information about
// the channel that was created at channel funding time.
func (l *channelLink) FundingCustomBlob() fn.Option[tlv.Blob] <span class="cov0" title="0">{
        if l.channel == nil </span><span class="cov0" title="0">{
                return fn.None[tlv.Blob]()
        }</span>

        <span class="cov0" title="0">if l.channel.State() == nil </span><span class="cov0" title="0">{
                return fn.None[tlv.Blob]()
        }</span>

        <span class="cov0" title="0">return l.channel.State().CustomBlob</span>
}

// CommitmentCustomBlob returns the custom blob of the current local commitment
// of the channel that this link is associated with.
func (l *channelLink) CommitmentCustomBlob() fn.Option[tlv.Blob] <span class="cov0" title="0">{
        if l.channel == nil </span><span class="cov0" title="0">{
                return fn.None[tlv.Blob]()
        }</span>

        <span class="cov0" title="0">return l.channel.LocalCommitmentBlob()</span>
}
</pre>
		
		<pre class="file" id="file9" style="display: none">package htlcswitch

import "github.com/go-errors/errors"

var (
        // ErrLinkShuttingDown signals that the link is shutting down.
        ErrLinkShuttingDown = errors.New("link shutting down")

        // ErrLinkFailedShutdown signals that a requested shutdown failed.
        ErrLinkFailedShutdown = errors.New("link failed to shutdown")
)

// errorCode encodes the possible types of errors that will make us fail the
// current link.
type errorCode uint8

const (
        // ErrInternalError indicates that something internal in the link
        // failed. In this case we will send a generic error to our peer.
        ErrInternalError errorCode = iota

        // ErrRemoteError indicates that our peer sent an error, prompting up
        // to fail the link.
        ErrRemoteError

        // ErrRemoteUnresponsive indicates that our peer took too long to
        // complete a commitment dance.
        ErrRemoteUnresponsive

        // ErrSyncError indicates that we failed synchronizing the state of the
        // channel with our peer.
        ErrSyncError

        // ErrInvalidUpdate indicates that the peer send us an invalid update.
        ErrInvalidUpdate

        // ErrInvalidCommitment indicates that the remote peer sent us an
        // invalid commitment signature.
        ErrInvalidCommitment

        // ErrInvalidRevocation indicates that the remote peer send us an
        // invalid revocation message.
        ErrInvalidRevocation

        // ErrRecoveryError the channel was unable to be resumed, we need the
        // remote party to force close the channel out on chain now as a
        // result.
        ErrRecoveryError

        // ErrCircuitError indicates a duplicate keystone error was hit in the
        // circuit map. This is non-fatal and will resolve itself (usually
        // within several minutes).
        ErrCircuitError

        // ErrStfuViolation indicates that the quiescence protocol has been
        // violated, either because Stfu has been sent/received at an invalid
        // time, or that an update has been sent/received while the channel is
        // quiesced.
        ErrStfuViolation
)

// LinkFailureAction is an enum-like type that describes the action that should
// be taken in response to a link failure.
type LinkFailureAction uint8

const (
        // LinkFailureForceNone indicates no action is to be taken.
        LinkFailureForceNone LinkFailureAction = iota

        // LinkFailureForceClose indicates that the channel should be force
        // closed.
        LinkFailureForceClose

        // LinkFailureDisconnect indicates that we should disconnect in an
        // attempt to recycle the connection. This can be useful if we think a
        // TCP connection or state machine is stalled.
        LinkFailureDisconnect
)

// LinkFailureError encapsulates an error that will make us fail the current
// link. It contains the necessary information needed to determine if we should
// force close the channel in the process, and if any error data should be sent
// to the peer.
type LinkFailureError struct {
        // code is the type of error this LinkFailureError encapsulates.
        code errorCode

        // FailureAction describes what we should do to fail the channel.
        FailureAction LinkFailureAction

        // PermanentFailure indicates whether this failure is permanent, and
        // the channel should not be attempted loaded again.
        PermanentFailure bool

        // Warning denotes if this is a non-terminal error that doesn't warrant
        // failing the channel all together.
        Warning bool

        // SendData is a byte slice that will be sent to the peer. If nil a
        // generic error will be sent.
        SendData []byte
}

// A compile time check to ensure LinkFailureError implements the error
// interface.
var _ error = (*LinkFailureError)(nil)

// Error returns a generic error for the LinkFailureError.
//
// NOTE: Part of the error interface.
func (e LinkFailureError) Error() string <span class="cov0" title="0">{
        switch e.code </span>{
        case ErrInternalError:<span class="cov0" title="0">
                return "internal error"</span>
        case ErrRemoteError:<span class="cov0" title="0">
                return "remote error"</span>
        case ErrRemoteUnresponsive:<span class="cov0" title="0">
                return "remote unresponsive"</span>
        case ErrSyncError:<span class="cov0" title="0">
                return "sync error"</span>
        case ErrInvalidUpdate:<span class="cov0" title="0">
                return "invalid update"</span>
        case ErrInvalidCommitment:<span class="cov0" title="0">
                return "invalid commitment"</span>
        case ErrInvalidRevocation:<span class="cov0" title="0">
                return "invalid revocation"</span>
        case ErrRecoveryError:<span class="cov0" title="0">
                return "unable to resume channel, recovery required"</span>
        case ErrCircuitError:<span class="cov0" title="0">
                return "non-fatal circuit map error"</span>
        case ErrStfuViolation:<span class="cov0" title="0">
                return "quiescence protocol executed improperly"</span>
        default:<span class="cov0" title="0">
                return "unknown error"</span>
        }
}

// ShouldSendToPeer indicates whether we should send an error to the peer if
// the link fails with this LinkFailureError.
func (e LinkFailureError) ShouldSendToPeer() bool <span class="cov0" title="0">{
        switch e.code </span>{

        // Since sending an error can lead some nodes to force close the
        // channel, create a whitelist of the failures we want to send so that
        // newly added error codes aren't automatically sent to the remote peer.
        case
                ErrInternalError,
                ErrRemoteError,
                ErrSyncError,
                ErrInvalidUpdate,
                ErrInvalidCommitment,
                ErrInvalidRevocation,
                ErrRecoveryError:<span class="cov0" title="0">

                return true</span>

        // In all other cases we will not attempt to send our peer an error.
        default:<span class="cov0" title="0">
                return false</span>
        }
}
</pre>
		
		<pre class="file" id="file10" style="display: none">package htlcswitch

import (
        "github.com/btcsuite/btclog/v2"
        "github.com/lightningnetwork/lnd/build"
        "github.com/lightningnetwork/lnd/htlcswitch/hop"
)

// log is a logger that is initialized with no output filters.  This
// means the package will not perform any logging by default until the caller
// requests it.
var log btclog.Logger

// The default amount of logging is none.
func init() <span class="cov8" title="1">{
        logger := build.NewSubLogger("HSWC", nil)

        UseLogger(logger)
}</span>

// DisableLog disables all library log output.  Logging output is disabled
// by default until UseLogger is called.
func DisableLog() <span class="cov0" title="0">{
        UseLogger(btclog.Disabled)
}</span>

// UseLogger uses a specified Logger to output package logging info.
// This should be used in preference to SetLogWriter if the caller is also
// using btclog.
func UseLogger(logger btclog.Logger) <span class="cov8" title="1">{
        log = logger
        hop.UseLogger(logger)
}</span>
</pre>
		
		<pre class="file" id="file11" style="display: none">package htlcswitch

import (
        "bytes"
        "container/list"
        "errors"
        "fmt"
        "sync"
        "time"

        "github.com/lightningnetwork/lnd/clock"
        "github.com/lightningnetwork/lnd/lntypes"
        "github.com/lightningnetwork/lnd/lnwallet/chainfee"
        "github.com/lightningnetwork/lnd/lnwire"
)

var (
        // ErrMailBoxShuttingDown is returned when the mailbox is interrupted by
        // a shutdown request.
        ErrMailBoxShuttingDown = errors.New("mailbox is shutting down")

        // ErrPacketAlreadyExists signals that an attempt to add a packet failed
        // because it already exists in the mailbox.
        ErrPacketAlreadyExists = errors.New("mailbox already has packet")
)

// MailBox is an interface which represents a concurrent-safe, in-order
// delivery queue for messages from the network and also from the main switch.
// This struct serves as a buffer between incoming messages, and messages to
// the handled by the link. Each of the mutating methods within this interface
// should be implemented in a non-blocking manner.
type MailBox interface {
        // AddMessage appends a new message to the end of the message queue.
        AddMessage(msg lnwire.Message) error

        // AddPacket appends a new message to the end of the packet queue.
        AddPacket(pkt *htlcPacket) error

        // HasPacket queries the packets for a circuit key, this is used to drop
        // packets bound for the switch that already have a queued response.
        HasPacket(CircuitKey) bool

        // AckPacket removes a packet from the mailboxes in-memory replay
        // buffer. This will prevent a packet from being delivered after a link
        // restarts if the switch has remained online. The returned boolean
        // indicates whether or not a packet with the passed incoming circuit
        // key was removed.
        AckPacket(CircuitKey) bool

        // FailAdd fails an UpdateAddHTLC that exists within the mailbox,
        // removing it from the in-memory replay buffer. This will prevent the
        // packet from being delivered after the link restarts if the switch has
        // remained online. The generated LinkError will show an
        // OutgoingFailureDownstreamHtlcAdd FailureDetail.
        FailAdd(pkt *htlcPacket)

        // MessageOutBox returns a channel that any new messages ready for
        // delivery will be sent on.
        MessageOutBox() chan lnwire.Message

        // PacketOutBox returns a channel that any new packets ready for
        // delivery will be sent on.
        PacketOutBox() chan *htlcPacket

        // Clears any pending wire messages from the inbox.
        ResetMessages() error

        // Reset the packet head to point at the first element in the list.
        ResetPackets() error

        // SetDustClosure takes in a closure that is used to evaluate whether
        // mailbox HTLC's are dust.
        SetDustClosure(isDust dustClosure)

        // SetFeeRate sets the feerate to be used when evaluating dust.
        SetFeeRate(feerate chainfee.SatPerKWeight)

        // DustPackets returns the dust sum for Adds in the mailbox for the
        // local and remote commitments.
        DustPackets() (lnwire.MilliSatoshi, lnwire.MilliSatoshi)

        // Start starts the mailbox and any goroutines it needs to operate
        // properly.
        Start()

        // Stop signals the mailbox and its goroutines for a graceful shutdown.
        Stop()
}

type mailBoxConfig struct {
        // shortChanID is the short channel id of the channel this mailbox
        // belongs to.
        shortChanID lnwire.ShortChannelID

        // forwardPackets send a varidic number of htlcPackets to the switch to
        // be routed. A quit channel should be provided so that the call can
        // properly exit during shutdown.
        forwardPackets func(&lt;-chan struct{}, ...*htlcPacket) error

        // clock is a time source for the mailbox.
        clock clock.Clock

        // expiry is the interval after which Adds will be cancelled if they
        // have not been yet been delivered. The computed deadline will expiry
        // this long after the Adds are added via AddPacket.
        expiry time.Duration

        // failMailboxUpdate is used to fail an expired HTLC and use the
        // correct SCID if the underlying channel uses aliases.
        failMailboxUpdate func(outScid,
                mailboxScid lnwire.ShortChannelID) lnwire.FailureMessage
}

// memoryMailBox is an implementation of the MailBox struct backed by purely
// in-memory queues.
//
// TODO(morehouse): use typed lists instead of list.Lists to avoid type asserts.
type memoryMailBox struct {
        started sync.Once
        stopped sync.Once

        cfg *mailBoxConfig

        wireMessages *list.List
        wireMtx      sync.Mutex
        wireCond     *sync.Cond

        messageOutbox chan lnwire.Message
        msgReset      chan chan struct{}

        // repPkts is a queue for reply packets, e.g. Settles and Fails.
        repPkts  *list.List
        repIndex map[CircuitKey]*list.Element
        repHead  *list.Element

        // addPkts is a dedicated queue for Adds.
        addPkts  *list.List
        addIndex map[CircuitKey]*list.Element
        addHead  *list.Element

        pktMtx  sync.Mutex
        pktCond *sync.Cond

        pktOutbox chan *htlcPacket
        pktReset  chan chan struct{}

        wireShutdown chan struct{}
        pktShutdown  chan struct{}
        quit         chan struct{}

        // feeRate is set when the link receives or sends out fee updates. It
        // is refreshed when AttachMailBox is called in case a fee update did
        // not get committed. In some cases it may be out of sync with the
        // channel's feerate, but it should eventually get back in sync.
        feeRate chainfee.SatPerKWeight

        // isDust is set when AttachMailBox is called and serves to evaluate
        // the outstanding dust in the memoryMailBox given the current set
        // feeRate.
        isDust dustClosure
}

// newMemoryMailBox creates a new instance of the memoryMailBox.
func newMemoryMailBox(cfg *mailBoxConfig) *memoryMailBox <span class="cov8" title="1">{
        box := &amp;memoryMailBox{
                cfg:           cfg,
                wireMessages:  list.New(),
                repPkts:       list.New(),
                addPkts:       list.New(),
                messageOutbox: make(chan lnwire.Message),
                pktOutbox:     make(chan *htlcPacket),
                msgReset:      make(chan chan struct{}, 1),
                pktReset:      make(chan chan struct{}, 1),
                repIndex:      make(map[CircuitKey]*list.Element),
                addIndex:      make(map[CircuitKey]*list.Element),
                wireShutdown:  make(chan struct{}),
                pktShutdown:   make(chan struct{}),
                quit:          make(chan struct{}),
        }
        box.wireCond = sync.NewCond(&amp;box.wireMtx)
        box.pktCond = sync.NewCond(&amp;box.pktMtx)

        return box
}</span>

// A compile time assertion to ensure that memoryMailBox meets the MailBox
// interface.
var _ MailBox = (*memoryMailBox)(nil)

// courierType is an enum that reflects the distinct types of messages a
// MailBox can handle. Each type will be placed in an isolated mail box and
// will have a dedicated goroutine for delivering the messages.
type courierType uint8

const (
        // wireCourier is a type of courier that handles wire messages.
        wireCourier courierType = iota

        // pktCourier is a type of courier that handles htlc packets.
        pktCourier
)

// Start starts the mailbox and any goroutines it needs to operate properly.
//
// NOTE: This method is part of the MailBox interface.
func (m *memoryMailBox) Start() <span class="cov8" title="1">{
        m.started.Do(func() </span><span class="cov8" title="1">{
                go m.wireMailCourier()
                go m.pktMailCourier()
        }</span>)
}

// ResetMessages blocks until all buffered wire messages are cleared.
func (m *memoryMailBox) ResetMessages() error <span class="cov8" title="1">{
        msgDone := make(chan struct{})
        select </span>{
        case m.msgReset &lt;- msgDone:<span class="cov8" title="1">
                return m.signalUntilReset(wireCourier, msgDone)</span>
        case &lt;-m.quit:<span class="cov0" title="0">
                return ErrMailBoxShuttingDown</span>
        }
}

// ResetPackets blocks until the head of packets buffer is reset, causing the
// packets to be redelivered in order.
func (m *memoryMailBox) ResetPackets() error <span class="cov8" title="1">{
        pktDone := make(chan struct{})
        select </span>{
        case m.pktReset &lt;- pktDone:<span class="cov8" title="1">
                return m.signalUntilReset(pktCourier, pktDone)</span>
        case &lt;-m.quit:<span class="cov8" title="1">
                return ErrMailBoxShuttingDown</span>
        }
}

// signalUntilReset strobes the condition variable for the specified inbox type
// until receiving a response that the mailbox has processed a reset.
func (m *memoryMailBox) signalUntilReset(cType courierType,
        done chan struct{}) error <span class="cov8" title="1">{

        for </span><span class="cov8" title="1">{

                switch cType </span>{
                case wireCourier:<span class="cov8" title="1">
                        m.wireCond.Signal()</span>
                case pktCourier:<span class="cov8" title="1">
                        m.pktCond.Signal()</span>
                }

                <span class="cov8" title="1">select </span>{
                case &lt;-time.After(time.Millisecond):<span class="cov8" title="1">
                        continue</span>
                case &lt;-done:<span class="cov8" title="1">
                        return nil</span>
                case &lt;-m.quit:<span class="cov8" title="1">
                        return ErrMailBoxShuttingDown</span>
                }
        }
}

// AckPacket removes the packet identified by it's incoming circuit key from the
// queue of packets to be delivered. The returned boolean indicates whether or
// not a packet with the passed incoming circuit key was removed.
//
// NOTE: It is safe to call this method multiple times for the same circuit key.
func (m *memoryMailBox) AckPacket(inKey CircuitKey) bool <span class="cov8" title="1">{
        m.pktCond.L.Lock()
        defer m.pktCond.L.Unlock()

        if entry, ok := m.repIndex[inKey]; ok </span><span class="cov8" title="1">{
                // Check whether we are removing the head of the queue. If so,
                // we must advance the head to the next packet before removing.
                // It's possible that the courier has already advanced the
                // repHead, so this check prevents the repHead from getting
                // desynchronized.
                if entry == m.repHead </span><span class="cov8" title="1">{
                        m.repHead = entry.Next()
                }</span>
                <span class="cov8" title="1">m.repPkts.Remove(entry)
                delete(m.repIndex, inKey)

                return true</span>
        }

        <span class="cov8" title="1">if entry, ok := m.addIndex[inKey]; ok </span><span class="cov8" title="1">{
                // Check whether we are removing the head of the queue. If so,
                // we must advance the head to the next add before removing.
                // It's possible that the courier has already advanced the
                // addHead, so this check prevents the addHead from getting
                // desynchronized.
                //
                // NOTE: While this event is rare for Settles or Fails, it could
                // be very common for Adds since the mailbox has the ability to
                // cancel Adds before they are delivered. When that occurs, the
                // head of addPkts has only been peeked and we expect to be
                // removing the head of the queue.
                if entry == m.addHead </span><span class="cov8" title="1">{
                        m.addHead = entry.Next()
                }</span>

                <span class="cov8" title="1">m.addPkts.Remove(entry)
                delete(m.addIndex, inKey)

                return true</span>
        }

        <span class="cov8" title="1">return false</span>
}

// HasPacket queries the packets for a circuit key, this is used to drop packets
// bound for the switch that already have a queued response.
func (m *memoryMailBox) HasPacket(inKey CircuitKey) bool <span class="cov8" title="1">{
        m.pktCond.L.Lock()
        _, ok := m.repIndex[inKey]
        m.pktCond.L.Unlock()

        return ok
}</span>

// Stop signals the mailbox and its goroutines for a graceful shutdown.
//
// NOTE: This method is part of the MailBox interface.
func (m *memoryMailBox) Stop() <span class="cov8" title="1">{
        m.stopped.Do(func() </span><span class="cov8" title="1">{
                close(m.quit)

                m.signalUntilShutdown(wireCourier)
                m.signalUntilShutdown(pktCourier)
        }</span>)
}

// signalUntilShutdown strobes the condition variable of the passed courier
// type, blocking until the worker has exited.
func (m *memoryMailBox) signalUntilShutdown(cType courierType) <span class="cov8" title="1">{
        var (
                cond     *sync.Cond
                shutdown chan struct{}
        )

        switch cType </span>{
        case wireCourier:<span class="cov8" title="1">
                cond = m.wireCond
                shutdown = m.wireShutdown</span>
        case pktCourier:<span class="cov8" title="1">
                cond = m.pktCond
                shutdown = m.pktShutdown</span>
        }

        <span class="cov8" title="1">for </span><span class="cov8" title="1">{
                select </span>{
                case &lt;-time.After(time.Millisecond):<span class="cov8" title="1">
                        cond.Signal()</span>
                case &lt;-shutdown:<span class="cov8" title="1">
                        return</span>
                }
        }
}

// pktWithExpiry wraps an incoming packet and records the time at which it it
// should be canceled from the mailbox. This will be used to detect if it gets
// stuck in the mailbox and inform when to cancel back.
type pktWithExpiry struct {
        pkt    *htlcPacket
        expiry time.Time
}

func (p *pktWithExpiry) deadline(clock clock.Clock) &lt;-chan time.Time <span class="cov8" title="1">{
        return clock.TickAfter(p.expiry.Sub(clock.Now()))
}</span>

// wireMailCourier is a dedicated goroutine whose job is to reliably deliver
// wire messages.
func (m *memoryMailBox) wireMailCourier() <span class="cov8" title="1">{
        defer close(m.wireShutdown)

        for </span><span class="cov8" title="1">{
                // First, we'll check our condition. If our mailbox is empty,
                // then we'll wait until a new item is added.
                m.wireCond.L.Lock()
                for m.wireMessages.Front() == nil </span><span class="cov8" title="1">{
                        m.wireCond.Wait()

                        select </span>{
                        case msgDone := &lt;-m.msgReset:<span class="cov8" title="1">
                                m.wireMessages.Init()
                                close(msgDone)</span>
                        case &lt;-m.quit:<span class="cov8" title="1">
                                m.wireCond.L.Unlock()
                                return</span>
                        default:<span class="cov8" title="1"></span>
                        }
                }

                // Grab the datum off the front of the queue, shifting the
                // slice's reference down one in order to remove the datum from
                // the queue.
                <span class="cov8" title="1">entry := m.wireMessages.Front()

                //nolint:forcetypeassert
                nextMsg := m.wireMessages.Remove(entry).(lnwire.Message)

                // Now that we're done with the condition, we can unlock it to
                // allow any callers to append to the end of our target queue.
                m.wireCond.L.Unlock()

                // With the next message obtained, we'll now select to attempt
                // to deliver the message. If we receive a kill signal, then
                // we'll bail out.
                select </span>{
                case m.messageOutbox &lt;- nextMsg:<span class="cov8" title="1"></span>
                case msgDone := &lt;-m.msgReset:<span class="cov0" title="0">
                        m.wireCond.L.Lock()
                        m.wireMessages.Init()
                        m.wireCond.L.Unlock()

                        close(msgDone)</span>
                case &lt;-m.quit:<span class="cov0" title="0">
                        return</span>
                }
        }
}

// pktMailCourier is a dedicated goroutine whose job is to reliably deliver
// packet messages.
func (m *memoryMailBox) pktMailCourier() <span class="cov8" title="1">{
        defer close(m.pktShutdown)

        for </span><span class="cov8" title="1">{
                // First, we'll check our condition. If our mailbox is empty,
                // then we'll wait until a new item is added.
                m.pktCond.L.Lock()
                for m.repHead == nil &amp;&amp; m.addHead == nil </span><span class="cov8" title="1">{
                        m.pktCond.Wait()

                        select </span>{
                        // Resetting the packet queue means just moving our
                        // pointer to the front. This ensures that any un-ACK'd
                        // messages are re-delivered upon reconnect.
                        case pktDone := &lt;-m.pktReset:<span class="cov8" title="1">
                                m.repHead = m.repPkts.Front()
                                m.addHead = m.addPkts.Front()

                                close(pktDone)</span>

                        case &lt;-m.quit:<span class="cov8" title="1">
                                m.pktCond.L.Unlock()
                                return</span>
                        default:<span class="cov8" title="1"></span>
                        }
                }

                <span class="cov8" title="1">var (
                        nextRep   *htlcPacket
                        nextRepEl *list.Element
                        nextAdd   *pktWithExpiry
                        nextAddEl *list.Element
                )
                // For packets, we actually never remove an item until it has
                // been ACK'd by the link. This ensures that if a read packet
                // doesn't make it into a commitment, then it'll be
                // re-delivered once the link comes back online.

                // Peek at the head of the Settle/Fails and Add queues. We peak
                // both even if there is a Settle/Fail present because we need
                // to set a deadline for the next pending Add if it's present.
                // Due to clock monotonicity, we know that the head of the Adds
                // is the next to expire.
                if m.repHead != nil </span><span class="cov8" title="1">{
                        //nolint:forcetypeassert
                        nextRep = m.repHead.Value.(*htlcPacket)
                        nextRepEl = m.repHead
                }</span>
                <span class="cov8" title="1">if m.addHead != nil </span><span class="cov8" title="1">{
                        //nolint:forcetypeassert
                        nextAdd = m.addHead.Value.(*pktWithExpiry)
                        nextAddEl = m.addHead
                }</span>

                // Now that we're done with the condition, we can unlock it to
                // allow any callers to append to the end of our target queue.
                <span class="cov8" title="1">m.pktCond.L.Unlock()

                var (
                        pktOutbox chan *htlcPacket
                        addOutbox chan *htlcPacket
                        add       *htlcPacket
                        deadline  &lt;-chan time.Time
                )

                // Prioritize delivery of Settle/Fail packets over Adds. This
                // ensures that we actively clear the commitment of existing
                // HTLCs before trying to add new ones. This can help to improve
                // forwarding performance since the time to sign a commitment is
                // linear in the number of HTLCs manifested on the commitments.
                //
                // NOTE: Both types are eventually delivered over the same
                // channel, but we can control which is delivered by exclusively
                // making one nil and the other non-nil. We know from our loop
                // condition that at least one nextRep and nextAdd are non-nil.
                if nextRep != nil </span><span class="cov8" title="1">{
                        pktOutbox = m.pktOutbox
                }</span> else<span class="cov8" title="1"> {
                        addOutbox = m.pktOutbox
                }</span>

                // If we have a pending Add, we'll also construct the deadline
                // so we can fail it back if we are unable to deliver any
                // message in time. We also dereference the nextAdd's packet,
                // since we will need access to it in the case we are delivering
                // it and/or if the deadline expires.
                //
                // NOTE: It's possible after this point for add to be nil, but
                // this can only occur when addOutbox is also nil, hence we
                // won't accidentally deliver a nil packet.
                <span class="cov8" title="1">if nextAdd != nil </span><span class="cov8" title="1">{
                        add = nextAdd.pkt
                        deadline = nextAdd.deadline(m.cfg.clock)
                }</span>

                <span class="cov8" title="1">select </span>{
                case pktOutbox &lt;- nextRep:<span class="cov8" title="1">
                        m.pktCond.L.Lock()
                        // Only advance the repHead if this Settle or Fail is
                        // still at the head of the queue.
                        if m.repHead != nil &amp;&amp; m.repHead == nextRepEl </span><span class="cov8" title="1">{
                                m.repHead = m.repHead.Next()
                        }</span>
                        <span class="cov8" title="1">m.pktCond.L.Unlock()</span>

                case addOutbox &lt;- add:<span class="cov8" title="1">
                        m.pktCond.L.Lock()
                        // Only advance the addHead if this Add is still at the
                        // head of the queue.
                        if m.addHead != nil &amp;&amp; m.addHead == nextAddEl </span><span class="cov8" title="1">{
                                m.addHead = m.addHead.Next()
                        }</span>
                        <span class="cov8" title="1">m.pktCond.L.Unlock()</span>

                case &lt;-deadline:<span class="cov8" title="1">
                        log.Debugf("Expiring add htlc with "+
                                "keystone=%v", add.keystone())
                        m.FailAdd(add)</span>

                case pktDone := &lt;-m.pktReset:<span class="cov8" title="1">
                        m.pktCond.L.Lock()
                        m.repHead = m.repPkts.Front()
                        m.addHead = m.addPkts.Front()
                        m.pktCond.L.Unlock()

                        close(pktDone)</span>

                case &lt;-m.quit:<span class="cov8" title="1">
                        return</span>
                }
        }
}

// AddMessage appends a new message to the end of the message queue.
//
// NOTE: This method is safe for concrete use and part of the MailBox
// interface.
func (m *memoryMailBox) AddMessage(msg lnwire.Message) error <span class="cov8" title="1">{
        // First, we'll lock the condition, and add the message to the end of
        // the wire message inbox.
        m.wireCond.L.Lock()
        m.wireMessages.PushBack(msg)
        m.wireCond.L.Unlock()

        // With the message added, we signal to the mailCourier that there are
        // additional messages to deliver.
        m.wireCond.Signal()

        return nil
}</span>

// AddPacket appends a new message to the end of the packet queue.
//
// NOTE: This method is safe for concrete use and part of the MailBox
// interface.
func (m *memoryMailBox) AddPacket(pkt *htlcPacket) error <span class="cov8" title="1">{
        m.pktCond.L.Lock()
        switch htlc := pkt.htlc.(type) </span>{
        // Split off Settle/Fail packets into the repPkts queue.
        case *lnwire.UpdateFulfillHTLC, *lnwire.UpdateFailHTLC:<span class="cov8" title="1">
                if _, ok := m.repIndex[pkt.inKey()]; ok </span><span class="cov8" title="1">{
                        m.pktCond.L.Unlock()
                        return ErrPacketAlreadyExists
                }</span>

                <span class="cov8" title="1">entry := m.repPkts.PushBack(pkt)
                m.repIndex[pkt.inKey()] = entry
                if m.repHead == nil </span><span class="cov8" title="1">{
                        m.repHead = entry
                }</span>

        // Split off Add packets into the addPkts queue.
        case *lnwire.UpdateAddHTLC:<span class="cov8" title="1">
                if _, ok := m.addIndex[pkt.inKey()]; ok </span><span class="cov8" title="1">{
                        m.pktCond.L.Unlock()
                        return ErrPacketAlreadyExists
                }</span>

                <span class="cov8" title="1">entry := m.addPkts.PushBack(&amp;pktWithExpiry{
                        pkt:    pkt,
                        expiry: m.cfg.clock.Now().Add(m.cfg.expiry),
                })
                m.addIndex[pkt.inKey()] = entry
                if m.addHead == nil </span><span class="cov8" title="1">{
                        m.addHead = entry
                }</span>

        default:<span class="cov0" title="0">
                m.pktCond.L.Unlock()
                return fmt.Errorf("unknown htlc type: %T", htlc)</span>
        }
        <span class="cov8" title="1">m.pktCond.L.Unlock()

        // With the packet added, we signal to the mailCourier that there are
        // additional packets to consume.
        m.pktCond.Signal()

        return nil</span>
}

// SetFeeRate sets the memoryMailBox's feerate for use in DustPackets.
func (m *memoryMailBox) SetFeeRate(feeRate chainfee.SatPerKWeight) <span class="cov8" title="1">{
        m.pktCond.L.Lock()
        defer m.pktCond.L.Unlock()

        m.feeRate = feeRate
}</span>

// SetDustClosure sets the memoryMailBox's dustClosure for use in DustPackets.
func (m *memoryMailBox) SetDustClosure(isDust dustClosure) <span class="cov8" title="1">{
        m.pktCond.L.Lock()
        defer m.pktCond.L.Unlock()

        m.isDust = isDust
}</span>

// DustPackets returns the dust sum for add packets in the mailbox. The first
// return value is the local dust sum and the second is the remote dust sum.
// This will keep track of a given dust HTLC from the time it is added via
// AddPacket until it is removed via AckPacket.
func (m *memoryMailBox) DustPackets() (lnwire.MilliSatoshi,
        lnwire.MilliSatoshi) <span class="cov8" title="1">{

        m.pktCond.L.Lock()
        defer m.pktCond.L.Unlock()

        var (
                localDustSum  lnwire.MilliSatoshi
                remoteDustSum lnwire.MilliSatoshi
        )

        // Run through the map of HTLC's and determine the dust sum with calls
        // to the memoryMailBox's isDust closure. Note that all mailbox packets
        // are outgoing so the second argument to isDust will be false.
        for _, e := range m.addIndex </span><span class="cov8" title="1">{
                addPkt := e.Value.(*pktWithExpiry).pkt

                // Evaluate whether this HTLC is dust on the local commitment.
                if m.isDust(
                        m.feeRate, false, lntypes.Local,
                        addPkt.amount.ToSatoshis(),
                ) </span><span class="cov8" title="1">{

                        localDustSum += addPkt.amount
                }</span>

                // Evaluate whether this HTLC is dust on the remote commitment.
                <span class="cov8" title="1">if m.isDust(
                        m.feeRate, false, lntypes.Remote,
                        addPkt.amount.ToSatoshis(),
                ) </span><span class="cov8" title="1">{

                        remoteDustSum += addPkt.amount
                }</span>
        }

        <span class="cov8" title="1">return localDustSum, remoteDustSum</span>
}

// FailAdd fails an UpdateAddHTLC that exists within the mailbox, removing it
// from the in-memory replay buffer. This will prevent the packet from being
// delivered after the link restarts if the switch has remained online. The
// generated LinkError will show an OutgoingFailureDownstreamHtlcAdd
// FailureDetail.
func (m *memoryMailBox) FailAdd(pkt *htlcPacket) <span class="cov8" title="1">{
        // First, remove the packet from mailbox. If we didn't find the packet
        // because it has already been acked, we'll exit early to avoid sending
        // a duplicate fail message through the switch.
        if !m.AckPacket(pkt.inKey()) </span><span class="cov8" title="1">{
                return
        }</span>

        <span class="cov8" title="1">var (
                localFailure = false
                reason       lnwire.OpaqueReason
        )

        // Create a temporary channel failure which we will send back to our
        // peer if this is a forward, or report to the user if the failed
        // payment was locally initiated.
        failure := m.cfg.failMailboxUpdate(
                pkt.originalOutgoingChanID, m.cfg.shortChanID,
        )

        // If the payment was locally initiated (which is indicated by a nil
        // obfuscator), we do not need to encrypt it back to the sender.
        if pkt.obfuscator == nil </span><span class="cov8" title="1">{
                var b bytes.Buffer
                err := lnwire.EncodeFailure(&amp;b, failure, 0)
                if err != nil </span><span class="cov0" title="0">{
                        log.Errorf("Unable to encode failure: %v", err)
                        return
                }</span>
                <span class="cov8" title="1">reason = lnwire.OpaqueReason(b.Bytes())
                localFailure = true</span>
        } else<span class="cov8" title="1"> {
                // If the packet is part of a forward, (identified by a non-nil
                // obfuscator) we need to encrypt the error back to the source.
                var err error
                reason, err = pkt.obfuscator.EncryptFirstHop(failure)
                if err != nil </span><span class="cov0" title="0">{
                        log.Errorf("Unable to obfuscate error: %v", err)
                        return
                }</span>
        }

        // Create a link error containing the temporary channel failure and a
        // detail which indicates the we failed to add the htlc.
        <span class="cov8" title="1">linkError := NewDetailedLinkError(
                failure, OutgoingFailureDownstreamHtlcAdd,
        )

        failPkt := &amp;htlcPacket{
                incomingChanID: pkt.incomingChanID,
                incomingHTLCID: pkt.incomingHTLCID,
                circuit:        pkt.circuit,
                sourceRef:      pkt.sourceRef,
                hasSource:      true,
                localFailure:   localFailure,
                obfuscator:     pkt.obfuscator,
                linkFailure:    linkError,
                htlc: &amp;lnwire.UpdateFailHTLC{
                        Reason: reason,
                },
        }

        if err := m.cfg.forwardPackets(m.quit, failPkt); err != nil </span><span class="cov0" title="0">{
                log.Errorf("Unhandled error while reforwarding packets "+
                        "settle/fail over htlcswitch: %v", err)
        }</span>
}

// MessageOutBox returns a channel that any new messages ready for delivery
// will be sent on.
//
// NOTE: This method is part of the MailBox interface.
func (m *memoryMailBox) MessageOutBox() chan lnwire.Message <span class="cov8" title="1">{
        return m.messageOutbox
}</span>

// PacketOutBox returns a channel that any new packets ready for delivery will
// be sent on.
//
// NOTE: This method is part of the MailBox interface.
func (m *memoryMailBox) PacketOutBox() chan *htlcPacket <span class="cov8" title="1">{
        return m.pktOutbox
}</span>

// mailOrchestrator is responsible for coordinating the creation and lifecycle
// of mailboxes used within the switch. It supports the ability to create
// mailboxes, reassign their short channel id's, deliver htlc packets, and
// queue packets for mailboxes that have not been created due to a link's late
// registration.
type mailOrchestrator struct {
        mu sync.RWMutex

        cfg *mailOrchConfig

        // mailboxes caches exactly one mailbox for all known channels.
        mailboxes map[lnwire.ChannelID]MailBox

        // liveIndex maps a live short chan id to the primary mailbox key.
        // An index in liveIndex map is only entered under two conditions:
        //   1. A link has a non-zero short channel id at time of AddLink.
        //   2. A link receives a non-zero short channel via UpdateShortChanID.
        liveIndex map[lnwire.ShortChannelID]lnwire.ChannelID

        // TODO(conner): add another pair of indexes:
        //   chan_id -&gt; short_chan_id
        //   short_chan_id -&gt; mailbox
        // so that Deliver can lookup mailbox directly once live,
        // but still queryable by channel_id.

        // unclaimedPackets maps a live short chan id to queue of packets if no
        // mailbox has been created.
        unclaimedPackets map[lnwire.ShortChannelID][]*htlcPacket
}

type mailOrchConfig struct {
        // forwardPackets send a varidic number of htlcPackets to the switch to
        // be routed. A quit channel should be provided so that the call can
        // properly exit during shutdown.
        forwardPackets func(&lt;-chan struct{}, ...*htlcPacket) error

        // clock is a time source for the generated mailboxes.
        clock clock.Clock

        // expiry is the interval after which Adds will be cancelled if they
        // have not been yet been delivered. The computed deadline will expiry
        // this long after the Adds are added to a mailbox via AddPacket.
        expiry time.Duration

        // failMailboxUpdate is used to fail an expired HTLC and use the
        // correct SCID if the underlying channel uses aliases.
        failMailboxUpdate func(outScid,
                mailboxScid lnwire.ShortChannelID) lnwire.FailureMessage
}

// newMailOrchestrator initializes a fresh mailOrchestrator.
func newMailOrchestrator(cfg *mailOrchConfig) *mailOrchestrator <span class="cov8" title="1">{
        return &amp;mailOrchestrator{
                cfg:              cfg,
                mailboxes:        make(map[lnwire.ChannelID]MailBox),
                liveIndex:        make(map[lnwire.ShortChannelID]lnwire.ChannelID),
                unclaimedPackets: make(map[lnwire.ShortChannelID][]*htlcPacket),
        }
}</span>

// Stop instructs the orchestrator to stop all active mailboxes.
func (mo *mailOrchestrator) Stop() <span class="cov8" title="1">{
        for _, mailbox := range mo.mailboxes </span><span class="cov8" title="1">{
                mailbox.Stop()
        }</span>
}

// GetOrCreateMailBox returns an existing mailbox belonging to `chanID`, or
// creates and returns a new mailbox if none is found.
func (mo *mailOrchestrator) GetOrCreateMailBox(chanID lnwire.ChannelID,
        shortChanID lnwire.ShortChannelID) MailBox <span class="cov8" title="1">{

        // First, try lookup the mailbox directly using only the shared mutex.
        mo.mu.RLock()
        mailbox, ok := mo.mailboxes[chanID]
        if ok </span><span class="cov8" title="1">{
                mo.mu.RUnlock()
                return mailbox
        }</span>
        <span class="cov8" title="1">mo.mu.RUnlock()

        // Otherwise, we will try again with exclusive lock, creating a mailbox
        // if one still has not been created.
        mo.mu.Lock()
        mailbox = mo.exclusiveGetOrCreateMailBox(chanID, shortChanID)
        mo.mu.Unlock()

        return mailbox</span>
}

// exclusiveGetOrCreateMailBox checks for the existence of a mailbox for the
// given channel id. If none is found, a new one is creates, started, and
// recorded.
//
// NOTE: This method MUST be invoked with the mailOrchestrator's exclusive lock.
func (mo *mailOrchestrator) exclusiveGetOrCreateMailBox(
        chanID lnwire.ChannelID, shortChanID lnwire.ShortChannelID) MailBox <span class="cov8" title="1">{

        mailbox, ok := mo.mailboxes[chanID]
        if !ok </span><span class="cov8" title="1">{
                mailbox = newMemoryMailBox(&amp;mailBoxConfig{
                        shortChanID:       shortChanID,
                        forwardPackets:    mo.cfg.forwardPackets,
                        clock:             mo.cfg.clock,
                        expiry:            mo.cfg.expiry,
                        failMailboxUpdate: mo.cfg.failMailboxUpdate,
                })
                mailbox.Start()
                mo.mailboxes[chanID] = mailbox
        }</span>

        <span class="cov8" title="1">return mailbox</span>
}

// BindLiveShortChanID registers that messages bound for a particular short
// channel id should be forwarded to the mailbox corresponding to the given
// channel id. This method also checks to see if there are any unclaimed
// packets for this short_chan_id. If any are found, they are delivered to the
// mailbox and removed (marked as claimed).
func (mo *mailOrchestrator) BindLiveShortChanID(mailbox MailBox,
        cid lnwire.ChannelID, sid lnwire.ShortChannelID) <span class="cov8" title="1">{

        mo.mu.Lock()
        // Update the mapping from short channel id to mailbox's channel id.
        mo.liveIndex[sid] = cid

        // Retrieve any unclaimed packets destined for this mailbox.
        pkts := mo.unclaimedPackets[sid]
        delete(mo.unclaimedPackets, sid)
        mo.mu.Unlock()

        // Deliver the unclaimed packets.
        for _, pkt := range pkts </span><span class="cov8" title="1">{
                mailbox.AddPacket(pkt)
        }</span>
}

// Deliver lookups the target mailbox using the live index from short_chan_id
// to channel_id. If the mailbox is found, the message is delivered directly.
// Otherwise the packet is recorded as unclaimed, and will be delivered to the
// mailbox upon the subsequent call to BindLiveShortChanID.
func (mo *mailOrchestrator) Deliver(
        sid lnwire.ShortChannelID, pkt *htlcPacket) error <span class="cov8" title="1">{

        var (
                mailbox MailBox
                found   bool
        )

        // First, try to find the channel id for the target short_chan_id. If
        // the link is live, we will also look up the created mailbox.
        mo.mu.RLock()
        chanID, isLive := mo.liveIndex[sid]
        if isLive </span><span class="cov8" title="1">{
                mailbox, found = mo.mailboxes[chanID]
        }</span>
        <span class="cov8" title="1">mo.mu.RUnlock()

        // The link is live and target mailbox was found, deliver immediately.
        if isLive &amp;&amp; found </span><span class="cov8" title="1">{
                return mailbox.AddPacket(pkt)
        }</span>

        // If we detected that the link has not been made live, we will acquire
        // the exclusive lock preemptively in order to queue this packet in the
        // list of unclaimed packets.
        <span class="cov8" title="1">mo.mu.Lock()

        // Double check to see if the mailbox has been not made live since the
        // release of the shared lock.
        //
        // NOTE: Checking again with the exclusive lock held prevents a race
        // condition where BindLiveShortChanID is interleaved between the
        // release of the shared lock, and acquiring the exclusive lock. The
        // result would be stuck packets, as they wouldn't be redelivered until
        // the next call to BindLiveShortChanID, which is expected to occur
        // infrequently.
        chanID, isLive = mo.liveIndex[sid]
        if isLive </span><span class="cov0" title="0">{
                // Reaching this point indicates the mailbox is actually live.
                // We'll try to load the mailbox using the fresh channel id.
                //
                // NOTE: This should never create a new mailbox, as the live
                // index should only be set if the mailbox had been initialized
                // beforehand.  However, this does ensure that this case is
                // handled properly in the event that it could happen.
                mailbox = mo.exclusiveGetOrCreateMailBox(chanID, sid)
                mo.mu.Unlock()

                // Deliver the packet to the mailbox if it was found or created.
                return mailbox.AddPacket(pkt)
        }</span>

        // Finally, if the channel id is still not found in the live index,
        // we'll add this to the list of unclaimed packets. These will be
        // delivered upon the next call to BindLiveShortChanID.
        <span class="cov8" title="1">mo.unclaimedPackets[sid] = append(mo.unclaimedPackets[sid], pkt)
        mo.mu.Unlock()

        return nil</span>
}
</pre>
		
		<pre class="file" id="file12" style="display: none">package htlcswitch

import (
        "bytes"
        "context"
        "crypto/sha256"
        "encoding/binary"
        "fmt"
        "io"
        "net"
        "path/filepath"
        "sync"
        "sync/atomic"
        "testing"
        "time"

        "github.com/btcsuite/btcd/btcec/v2"
        "github.com/btcsuite/btcd/btcec/v2/ecdsa"
        "github.com/btcsuite/btcd/btcutil"
        "github.com/btcsuite/btcd/wire"
        "github.com/go-errors/errors"
        sphinx "github.com/lightningnetwork/lightning-onion"
        "github.com/lightningnetwork/lnd/chainntnfs"
        "github.com/lightningnetwork/lnd/channeldb"
        "github.com/lightningnetwork/lnd/clock"
        "github.com/lightningnetwork/lnd/contractcourt"
        "github.com/lightningnetwork/lnd/fn/v2"
        "github.com/lightningnetwork/lnd/graph/db/models"
        "github.com/lightningnetwork/lnd/htlcswitch/hop"
        "github.com/lightningnetwork/lnd/invoices"
        "github.com/lightningnetwork/lnd/lnpeer"
        "github.com/lightningnetwork/lnd/lntest/mock"
        "github.com/lightningnetwork/lnd/lntypes"
        "github.com/lightningnetwork/lnd/lnwallet/chainfee"
        "github.com/lightningnetwork/lnd/lnwire"
        "github.com/lightningnetwork/lnd/ticker"
        "github.com/lightningnetwork/lnd/tlv"
)

func isAlias(scid lnwire.ShortChannelID) bool <span class="cov8" title="1">{
        return scid.BlockHeight &gt;= 16_000_000 &amp;&amp; scid.BlockHeight &lt; 16_250_000
}</span>

type mockPreimageCache struct {
        sync.Mutex
        preimageMap map[lntypes.Hash]lntypes.Preimage
}

func newMockPreimageCache() *mockPreimageCache <span class="cov8" title="1">{
        return &amp;mockPreimageCache{
                preimageMap: make(map[lntypes.Hash]lntypes.Preimage),
        }
}</span>

func (m *mockPreimageCache) LookupPreimage(
        hash lntypes.Hash) (lntypes.Preimage, bool) <span class="cov8" title="1">{

        m.Lock()
        defer m.Unlock()

        p, ok := m.preimageMap[hash]
        return p, ok
}</span>

func (m *mockPreimageCache) AddPreimages(preimages ...lntypes.Preimage) error <span class="cov8" title="1">{
        m.Lock()
        defer m.Unlock()

        for _, preimage := range preimages </span><span class="cov8" title="1">{
                m.preimageMap[preimage.Hash()] = preimage
        }</span>

        <span class="cov8" title="1">return nil</span>
}

func (m *mockPreimageCache) SubscribeUpdates(
        chanID lnwire.ShortChannelID, htlc *channeldb.HTLC,
        payload *hop.Payload,
        nextHopOnionBlob []byte) (*contractcourt.WitnessSubscription, error) <span class="cov0" title="0">{

        return nil, nil
}</span>

// TODO(yy): replace it with chainfee.MockEstimator.
type mockFeeEstimator struct {
        byteFeeIn chan chainfee.SatPerKWeight
        relayFee  chan chainfee.SatPerKWeight

        quit chan struct{}
}

func newMockFeeEstimator() *mockFeeEstimator <span class="cov8" title="1">{
        return &amp;mockFeeEstimator{
                byteFeeIn: make(chan chainfee.SatPerKWeight),
                relayFee:  make(chan chainfee.SatPerKWeight),
                quit:      make(chan struct{}),
        }
}</span>

func (m *mockFeeEstimator) EstimateFeePerKW(
        numBlocks uint32) (chainfee.SatPerKWeight, error) <span class="cov8" title="1">{

        select </span>{
        case feeRate := &lt;-m.byteFeeIn:<span class="cov8" title="1">
                return feeRate, nil</span>
        case &lt;-m.quit:<span class="cov0" title="0">
                return 0, fmt.Errorf("exiting")</span>
        }
}

func (m *mockFeeEstimator) RelayFeePerKW() chainfee.SatPerKWeight <span class="cov8" title="1">{
        select </span>{
        case feeRate := &lt;-m.relayFee:<span class="cov8" title="1">
                return feeRate</span>
        case &lt;-m.quit:<span class="cov0" title="0">
                return 0</span>
        }
}

func (m *mockFeeEstimator) Start() error <span class="cov0" title="0">{
        return nil
}</span>
func (m *mockFeeEstimator) Stop() error <span class="cov8" title="1">{
        close(m.quit)
        return nil
}</span>

var _ chainfee.Estimator = (*mockFeeEstimator)(nil)

type mockForwardingLog struct {
        sync.Mutex

        events map[time.Time]channeldb.ForwardingEvent
}

func (m *mockForwardingLog) AddForwardingEvents(events []channeldb.ForwardingEvent) error <span class="cov8" title="1">{
        m.Lock()
        defer m.Unlock()

        for _, event := range events </span><span class="cov8" title="1">{
                m.events[event.Timestamp] = event
        }</span>

        <span class="cov8" title="1">return nil</span>
}

type mockServer struct {
        started  int32 // To be used atomically.
        shutdown int32 // To be used atomically.
        wg       sync.WaitGroup
        quit     chan struct{}

        t testing.TB

        name             string
        messages         chan lnwire.Message
        protocolTraceMtx sync.Mutex
        protocolTrace    []lnwire.Message

        id         [33]byte
        htlcSwitch *Switch

        registry         *mockInvoiceRegistry
        pCache           *mockPreimageCache
        interceptorFuncs []messageInterceptor
}

var _ lnpeer.Peer = (*mockServer)(nil)

func initSwitchWithDB(startingHeight uint32, db *channeldb.DB) (*Switch, error) <span class="cov8" title="1">{
        signAliasUpdate := func(u *lnwire.ChannelUpdate1) (*ecdsa.Signature,
                error) </span><span class="cov8" title="1">{

                return testSig, nil
        }</span>

        <span class="cov8" title="1">cfg := Config{
                DB:                   db,
                FetchAllOpenChannels: db.ChannelStateDB().FetchAllOpenChannels,
                FetchAllChannels:     db.ChannelStateDB().FetchAllChannels,
                FetchClosedChannels:  db.ChannelStateDB().FetchClosedChannels,
                SwitchPackager:       channeldb.NewSwitchPackager(),
                FwdingLog: &amp;mockForwardingLog{
                        events: make(map[time.Time]channeldb.ForwardingEvent),
                },
                FetchLastChannelUpdate: func(scid lnwire.ShortChannelID) (
                        *lnwire.ChannelUpdate1, error) </span><span class="cov8" title="1">{

                        return &amp;lnwire.ChannelUpdate1{
                                ShortChannelID: scid,
                        }, nil
                }</span>,
                Notifier: &amp;mock.ChainNotifier{
                        SpendChan: make(chan *chainntnfs.SpendDetail),
                        EpochChan: make(chan *chainntnfs.BlockEpoch),
                        ConfChan:  make(chan *chainntnfs.TxConfirmation),
                },
                FwdEventTicker: ticker.NewForce(
                        DefaultFwdEventInterval,
                ),
                LogEventTicker:         ticker.NewForce(DefaultLogInterval),
                AckEventTicker:         ticker.NewForce(DefaultAckInterval),
                HtlcNotifier:           &amp;mockHTLCNotifier{},
                Clock:                  clock.NewDefaultClock(),
                MailboxDeliveryTimeout: time.Hour,
                MaxFeeExposure:         DefaultMaxFeeExposure,
                SignAliasUpdate:        signAliasUpdate,
                IsAlias:                isAlias,
        }

        <span class="cov8" title="1">return New(cfg, startingHeight)</span>
}

func initSwitchWithTempDB(t testing.TB, startingHeight uint32) (*Switch,
        error) <span class="cov8" title="1">{

        tempPath := filepath.Join(t.TempDir(), "switchdb")
        db := channeldb.OpenForTesting(t, tempPath)

        s, err := initSwitchWithDB(startingHeight, db)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return s, nil</span>
}

func newMockServer(t testing.TB, name string, startingHeight uint32,
        db *channeldb.DB, defaultDelta uint32) (*mockServer, error) <span class="cov8" title="1">{

        var id [33]byte
        h := sha256.Sum256([]byte(name))
        copy(id[:], h[:])

        pCache := newMockPreimageCache()

        var (
                htlcSwitch *Switch
                err        error
        )
        if db == nil </span><span class="cov8" title="1">{
                htlcSwitch, err = initSwitchWithTempDB(t, startingHeight)
        }</span> else<span class="cov8" title="1"> {
                htlcSwitch, err = initSwitchWithDB(startingHeight, db)
        }</span>
        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">t.Cleanup(func() </span><span class="cov8" title="1">{ _ = htlcSwitch.Stop() }</span>)

        <span class="cov8" title="1">registry := newMockRegistry(t)

        return &amp;mockServer{
                t:                t,
                id:               id,
                name:             name,
                messages:         make(chan lnwire.Message, 3000),
                quit:             make(chan struct{}),
                registry:         registry,
                htlcSwitch:       htlcSwitch,
                pCache:           pCache,
                interceptorFuncs: make([]messageInterceptor, 0),
        }, nil</span>
}

func (s *mockServer) Start() error <span class="cov8" title="1">{
        if !atomic.CompareAndSwapInt32(&amp;s.started, 0, 1) </span><span class="cov0" title="0">{
                return errors.New("mock server already started")
        }</span>

        <span class="cov8" title="1">if err := s.htlcSwitch.Start(); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">s.wg.Add(1)
        go func() </span><span class="cov8" title="1">{
                defer s.wg.Done()

                defer func() </span><span class="cov8" title="1">{
                        s.htlcSwitch.Stop()
                }</span>()

                <span class="cov8" title="1">for </span><span class="cov8" title="1">{
                        select </span>{
                        case msg := &lt;-s.messages:<span class="cov8" title="1">
                                s.protocolTraceMtx.Lock()
                                s.protocolTrace = append(s.protocolTrace, msg)
                                s.protocolTraceMtx.Unlock()

                                var shouldSkip bool

                                for _, interceptor := range s.interceptorFuncs </span><span class="cov8" title="1">{
                                        skip, err := interceptor(msg)
                                        if err != nil </span><span class="cov0" title="0">{
                                                s.t.Fatalf("%v: error in the "+
                                                        "interceptor: %v", s.name, err)
                                                return
                                        }</span>
                                        <span class="cov8" title="1">shouldSkip = shouldSkip || skip</span>
                                }

                                <span class="cov8" title="1">if shouldSkip </span><span class="cov8" title="1">{
                                        continue</span>
                                }

                                <span class="cov8" title="1">if err := s.readHandler(msg); err != nil </span><span class="cov0" title="0">{
                                        s.t.Fatal(err)
                                        return
                                }</span>
                        case &lt;-s.quit:<span class="cov8" title="1">
                                return</span>
                        }
                }
        }()

        <span class="cov8" title="1">return nil</span>
}

func (s *mockServer) QuitSignal() &lt;-chan struct{} <span class="cov0" title="0">{
        return s.quit
}</span>

// mockHopIterator represents the test version of hop iterator which instead
// of encrypting the path in onion blob just stores the path as a list of hops.
type mockHopIterator struct {
        hops []*hop.Payload
}

func newMockHopIterator(hops ...*hop.Payload) hop.Iterator <span class="cov8" title="1">{
        return &amp;mockHopIterator{hops: hops}
}</span>

func (r *mockHopIterator) HopPayload() (*hop.Payload, hop.RouteRole, error) <span class="cov8" title="1">{
        h := r.hops[0]
        r.hops = r.hops[1:]
        return h, hop.RouteRoleCleartext, nil
}</span>

func (r *mockHopIterator) ExtraOnionBlob() []byte <span class="cov0" title="0">{
        return nil
}</span>

func (r *mockHopIterator) ExtractErrorEncrypter(
        extracter hop.ErrorEncrypterExtracter, _ bool) (hop.ErrorEncrypter,
        lnwire.FailCode) <span class="cov8" title="1">{

        return extracter(nil)
}</span>

func (r *mockHopIterator) EncodeNextHop(w io.Writer) error <span class="cov8" title="1">{
        var hopLength [4]byte
        binary.BigEndian.PutUint32(hopLength[:], uint32(len(r.hops)))

        if _, err := w.Write(hopLength[:]); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">for _, hop := range r.hops </span><span class="cov8" title="1">{
                fwdInfo := hop.ForwardingInfo()
                if err := encodeFwdInfo(w, &amp;fwdInfo); err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>
        }

        <span class="cov8" title="1">return nil</span>
}

func encodeFwdInfo(w io.Writer, f *hop.ForwardingInfo) error <span class="cov8" title="1">{
        if err := binary.Write(w, binary.BigEndian, f.NextHop); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">if err := binary.Write(w, binary.BigEndian, f.AmountToForward); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">if err := binary.Write(w, binary.BigEndian, f.OutgoingCTLV); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">return nil</span>
}

var _ hop.Iterator = (*mockHopIterator)(nil)

// mockObfuscator mock implementation of the failure obfuscator which only
// encodes the failure and do not makes any onion obfuscation.
type mockObfuscator struct {
        ogPacket *sphinx.OnionPacket
        failure  lnwire.FailureMessage
}

// NewMockObfuscator initializes a dummy mockObfuscator used for testing.
func NewMockObfuscator() hop.ErrorEncrypter <span class="cov8" title="1">{
        return &amp;mockObfuscator{}
}</span>

func (o *mockObfuscator) OnionPacket() *sphinx.OnionPacket <span class="cov0" title="0">{
        return o.ogPacket
}</span>

func (o *mockObfuscator) Type() hop.EncrypterType <span class="cov8" title="1">{
        return hop.EncrypterTypeMock
}</span>

func (o *mockObfuscator) Encode(w io.Writer) error <span class="cov8" title="1">{
        return nil
}</span>

func (o *mockObfuscator) Decode(r io.Reader) error <span class="cov8" title="1">{
        return nil
}</span>

func (o *mockObfuscator) Reextract(
        extracter hop.ErrorEncrypterExtracter) error <span class="cov8" title="1">{

        return nil
}</span>

var fakeHmac = []byte("hmachmachmachmachmachmachmachmac")

func (o *mockObfuscator) EncryptFirstHop(failure lnwire.FailureMessage) (
        lnwire.OpaqueReason, error) <span class="cov8" title="1">{

        o.failure = failure

        var b bytes.Buffer
        b.Write(fakeHmac)

        if err := lnwire.EncodeFailure(&amp;b, failure, 0); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">return b.Bytes(), nil</span>
}

func (o *mockObfuscator) IntermediateEncrypt(reason lnwire.OpaqueReason) lnwire.OpaqueReason <span class="cov8" title="1">{
        return reason
}</span>

func (o *mockObfuscator) EncryptMalformedError(reason lnwire.OpaqueReason) lnwire.OpaqueReason <span class="cov8" title="1">{
        var b bytes.Buffer
        b.Write(fakeHmac)

        b.Write(reason)

        return b.Bytes()
}</span>

// mockDeobfuscator mock implementation of the failure deobfuscator which
// only decodes the failure do not makes any onion obfuscation.
type mockDeobfuscator struct{}

func newMockDeobfuscator() ErrorDecrypter <span class="cov8" title="1">{
        return &amp;mockDeobfuscator{}
}</span>

func (o *mockDeobfuscator) DecryptError(reason lnwire.OpaqueReason) (
        *ForwardingError, error) <span class="cov8" title="1">{

        if !bytes.Equal(reason[:32], fakeHmac) </span><span class="cov0" title="0">{
                return nil, errors.New("fake decryption error")
        }</span>
        <span class="cov8" title="1">reason = reason[32:]

        r := bytes.NewReader(reason)
        failure, err := lnwire.DecodeFailure(r, 0)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return NewForwardingError(failure, 1), nil</span>
}

var _ ErrorDecrypter = (*mockDeobfuscator)(nil)

// mockIteratorDecoder test version of hop iterator decoder which decodes the
// encoded array of hops.
type mockIteratorDecoder struct {
        mu sync.RWMutex

        responses map[[32]byte][]hop.DecodeHopIteratorResponse

        decodeFail bool
}

func newMockIteratorDecoder() *mockIteratorDecoder <span class="cov8" title="1">{
        return &amp;mockIteratorDecoder{
                responses: make(map[[32]byte][]hop.DecodeHopIteratorResponse),
        }
}</span>

func (p *mockIteratorDecoder) DecodeHopIterator(r io.Reader, rHash []byte,
        cltv uint32) (hop.Iterator, lnwire.FailCode) <span class="cov8" title="1">{

        var b [4]byte
        _, err := r.Read(b[:])
        if err != nil </span><span class="cov0" title="0">{
                return nil, lnwire.CodeTemporaryChannelFailure
        }</span>
        <span class="cov8" title="1">hopLength := binary.BigEndian.Uint32(b[:])

        hops := make([]*hop.Payload, hopLength)
        for i := uint32(0); i &lt; hopLength; i++ </span><span class="cov8" title="1">{
                var f hop.ForwardingInfo
                if err := decodeFwdInfo(r, &amp;f); err != nil </span><span class="cov0" title="0">{
                        return nil, lnwire.CodeTemporaryChannelFailure
                }</span>

                <span class="cov8" title="1">var nextHopBytes [8]byte
                binary.BigEndian.PutUint64(nextHopBytes[:], f.NextHop.ToUint64())

                hops[i] = hop.NewLegacyPayload(&amp;sphinx.HopData{
                        Realm:         [1]byte{}, // hop.BitcoinNetwork
                        NextAddress:   nextHopBytes,
                        ForwardAmount: uint64(f.AmountToForward),
                        OutgoingCltv:  f.OutgoingCTLV,
                })</span>
        }

        <span class="cov8" title="1">return newMockHopIterator(hops...), lnwire.CodeNone</span>
}

func (p *mockIteratorDecoder) DecodeHopIterators(id []byte,
        reqs []hop.DecodeHopIteratorRequest) (
        []hop.DecodeHopIteratorResponse, error) <span class="cov8" title="1">{

        idHash := sha256.Sum256(id)

        p.mu.RLock()
        if resps, ok := p.responses[idHash]; ok </span><span class="cov0" title="0">{
                p.mu.RUnlock()
                return resps, nil
        }</span>
        <span class="cov8" title="1">p.mu.RUnlock()

        batchSize := len(reqs)

        resps := make([]hop.DecodeHopIteratorResponse, 0, batchSize)
        for _, req := range reqs </span><span class="cov8" title="1">{
                iterator, failcode := p.DecodeHopIterator(
                        req.OnionReader, req.RHash, req.IncomingCltv,
                )

                if p.decodeFail </span><span class="cov8" title="1">{
                        failcode = lnwire.CodeTemporaryChannelFailure
                }</span>

                <span class="cov8" title="1">resp := hop.DecodeHopIteratorResponse{
                        HopIterator: iterator,
                        FailCode:    failcode,
                }
                resps = append(resps, resp)</span>
        }

        <span class="cov8" title="1">p.mu.Lock()
        p.responses[idHash] = resps
        p.mu.Unlock()

        return resps, nil</span>
}

func decodeFwdInfo(r io.Reader, f *hop.ForwardingInfo) error <span class="cov8" title="1">{
        if err := binary.Read(r, binary.BigEndian, &amp;f.NextHop); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">if err := binary.Read(r, binary.BigEndian, &amp;f.AmountToForward); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">if err := binary.Read(r, binary.BigEndian, &amp;f.OutgoingCTLV); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// messageInterceptor is function that handles the incoming peer messages and
// may decide should the peer skip the message or not.
type messageInterceptor func(m lnwire.Message) (bool, error)

// Record is used to set the function which will be triggered when new
// lnwire message was received.
func (s *mockServer) intersect(f messageInterceptor) <span class="cov8" title="1">{
        s.interceptorFuncs = append(s.interceptorFuncs, f)
}</span>

func (s *mockServer) SendMessage(sync bool, msgs ...lnwire.Message) error <span class="cov8" title="1">{

        for _, msg := range msgs </span><span class="cov8" title="1">{
                select </span>{
                case s.messages &lt;- msg:<span class="cov8" title="1"></span>
                case &lt;-s.quit:<span class="cov0" title="0">
                        return errors.New("server is stopped")</span>
                }
        }

        <span class="cov8" title="1">return nil</span>
}

func (s *mockServer) SendMessageLazy(sync bool, msgs ...lnwire.Message) error <span class="cov0" title="0">{
        panic("not implemented")</span>
}

func (s *mockServer) readHandler(message lnwire.Message) error <span class="cov8" title="1">{
        var targetChan lnwire.ChannelID

        switch msg := message.(type) </span>{
        case *lnwire.UpdateAddHTLC:<span class="cov8" title="1">
                targetChan = msg.ChanID</span>
        case *lnwire.UpdateFulfillHTLC:<span class="cov8" title="1">
                targetChan = msg.ChanID</span>
        case *lnwire.UpdateFailHTLC:<span class="cov8" title="1">
                targetChan = msg.ChanID</span>
        case *lnwire.UpdateFailMalformedHTLC:<span class="cov8" title="1">
                targetChan = msg.ChanID</span>
        case *lnwire.RevokeAndAck:<span class="cov8" title="1">
                targetChan = msg.ChanID</span>
        case *lnwire.CommitSig:<span class="cov8" title="1">
                targetChan = msg.ChanID</span>
        case *lnwire.ChannelReady:<span class="cov8" title="1">
                // Ignore
                return nil</span>
        case *lnwire.ChannelReestablish:<span class="cov8" title="1">
                targetChan = msg.ChanID</span>
        case *lnwire.UpdateFee:<span class="cov8" title="1">
                targetChan = msg.ChanID</span>
        case *lnwire.Stfu:<span class="cov8" title="1">
                targetChan = msg.ChanID</span>
        default:<span class="cov0" title="0">
                return fmt.Errorf("unknown message type: %T", msg)</span>
        }

        // Dispatch the commitment update message to the proper channel link
        // dedicated to this channel. If the link is not found, we will discard
        // the message.
        <span class="cov8" title="1">link, err := s.htlcSwitch.GetLink(targetChan)
        if err != nil </span><span class="cov0" title="0">{
                return nil
        }</span>

        // Create goroutine for this, in order to be able to properly stop
        // the server when handler stacked (server unavailable)
        <span class="cov8" title="1">link.HandleChannelUpdate(message)

        return nil</span>
}

func (s *mockServer) PubKey() [33]byte <span class="cov8" title="1">{
        return s.id
}</span>

func (s *mockServer) IdentityKey() *btcec.PublicKey <span class="cov0" title="0">{
        pubkey, _ := btcec.ParsePubKey(s.id[:])
        return pubkey
}</span>

func (s *mockServer) Address() net.Addr <span class="cov0" title="0">{
        return nil
}</span>

func (s *mockServer) AddNewChannel(channel *lnpeer.NewChannel,
        cancel &lt;-chan struct{}) error <span class="cov0" title="0">{

        return nil
}</span>

func (s *mockServer) AddPendingChannel(_ lnwire.ChannelID,
        cancel &lt;-chan struct{}) error <span class="cov0" title="0">{

        return nil
}</span>

func (s *mockServer) RemovePendingChannel(_ lnwire.ChannelID) error <span class="cov0" title="0">{
        return nil
}</span>

func (s *mockServer) WipeChannel(*wire.OutPoint) {<span class="cov0" title="0">}</span>

func (s *mockServer) LocalFeatures() *lnwire.FeatureVector <span class="cov0" title="0">{
        return nil
}</span>

func (s *mockServer) RemoteFeatures() *lnwire.FeatureVector <span class="cov0" title="0">{
        return nil
}</span>

func (s *mockServer) Disconnect(err error) {<span class="cov0" title="0">}</span>

func (s *mockServer) Stop() error <span class="cov8" title="1">{
        if !atomic.CompareAndSwapInt32(&amp;s.shutdown, 0, 1) </span><span class="cov8" title="1">{
                return nil
        }</span>

        <span class="cov8" title="1">close(s.quit)
        s.wg.Wait()

        return nil</span>
}

func (s *mockServer) String() string <span class="cov0" title="0">{
        return s.name
}</span>

type mockChannelLink struct {
        htlcSwitch *Switch

        shortChanID lnwire.ShortChannelID

        // Only used for zero-conf channels.
        realScid lnwire.ShortChannelID

        aliases []lnwire.ShortChannelID

        chanID lnwire.ChannelID

        peer lnpeer.Peer

        mailBox MailBox

        packets chan *htlcPacket

        eligible bool

        unadvertised bool

        zeroConf bool

        optionFeature bool

        htlcID uint64

        checkHtlcTransitResult *LinkError

        checkHtlcForwardResult *LinkError

        failAliasUpdate func(sid lnwire.ShortChannelID,
                incoming bool) *lnwire.ChannelUpdate1

        confirmedZC bool
}

// completeCircuit is a helper method for adding the finalized payment circuit
// to the switch's circuit map. In testing, this should be executed after
// receiving an htlc from the downstream packets channel.
func (f *mockChannelLink) completeCircuit(pkt *htlcPacket) error <span class="cov8" title="1">{
        switch htlc := pkt.htlc.(type) </span>{
        case *lnwire.UpdateAddHTLC:<span class="cov8" title="1">
                pkt.outgoingChanID = f.shortChanID
                pkt.outgoingHTLCID = f.htlcID
                htlc.ID = f.htlcID

                keystone := Keystone{pkt.inKey(), pkt.outKey()}
                err := f.htlcSwitch.circuits.OpenCircuits(keystone)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov8" title="1">f.htlcID++</span>

        case *lnwire.UpdateFulfillHTLC, *lnwire.UpdateFailHTLC:<span class="cov8" title="1">
                if pkt.circuit != nil </span><span class="cov8" title="1">{
                        err := f.htlcSwitch.teardownCircuit(pkt)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }
        }

        <span class="cov8" title="1">f.mailBox.AckPacket(pkt.inKey())

        return nil</span>
}

func (f *mockChannelLink) deleteCircuit(pkt *htlcPacket) error <span class="cov8" title="1">{
        return f.htlcSwitch.circuits.DeleteCircuits(pkt.inKey())
}</span>

func newMockChannelLink(htlcSwitch *Switch, chanID lnwire.ChannelID,
        shortChanID, realScid lnwire.ShortChannelID, peer lnpeer.Peer,
        eligible, unadvertised, zeroConf, optionFeature bool,
) *mockChannelLink <span class="cov8" title="1">{

        aliases := make([]lnwire.ShortChannelID, 0)
        var realConfirmed bool

        if zeroConf </span><span class="cov8" title="1">{
                aliases = append(aliases, shortChanID)
        }</span>

        <span class="cov8" title="1">if realScid != hop.Source </span><span class="cov8" title="1">{
                realConfirmed = true
        }</span>

        <span class="cov8" title="1">return &amp;mockChannelLink{
                htlcSwitch:    htlcSwitch,
                chanID:        chanID,
                shortChanID:   shortChanID,
                realScid:      realScid,
                peer:          peer,
                eligible:      eligible,
                unadvertised:  unadvertised,
                zeroConf:      zeroConf,
                optionFeature: optionFeature,
                aliases:       aliases,
                confirmedZC:   realConfirmed,
        }</span>
}

// addAlias is not part of any interface method.
func (f *mockChannelLink) addAlias(alias lnwire.ShortChannelID) <span class="cov8" title="1">{
        f.aliases = append(f.aliases, alias)
}</span>

func (f *mockChannelLink) handleSwitchPacket(pkt *htlcPacket) error <span class="cov8" title="1">{
        f.mailBox.AddPacket(pkt)
        return nil
}</span>

func (f *mockChannelLink) getDustSum(whoseCommit lntypes.ChannelParty,
        dryRunFee fn.Option[chainfee.SatPerKWeight]) lnwire.MilliSatoshi <span class="cov8" title="1">{

        return 0
}</span>

func (f *mockChannelLink) getFeeRate() chainfee.SatPerKWeight <span class="cov8" title="1">{
        return 0
}</span>

func (f *mockChannelLink) getDustClosure() dustClosure <span class="cov8" title="1">{
        dustLimit := btcutil.Amount(400)
        return dustHelper(
                channeldb.SingleFunderTweaklessBit, dustLimit, dustLimit,
        )
}</span>

func (f *mockChannelLink) getCommitFee(remote bool) btcutil.Amount <span class="cov0" title="0">{
        return 0
}</span>

func (f *mockChannelLink) HandleChannelUpdate(lnwire.Message) {<span class="cov0" title="0">
}</span>

func (f *mockChannelLink) UpdateForwardingPolicy(_ models.ForwardingPolicy) {<span class="cov0" title="0">
}</span>
func (f *mockChannelLink) CheckHtlcForward([32]byte, lnwire.MilliSatoshi,
        lnwire.MilliSatoshi, uint32, uint32, models.InboundFee, uint32,
        lnwire.ShortChannelID, lnwire.CustomRecords) *LinkError <span class="cov8" title="1">{

        return f.checkHtlcForwardResult
}</span>

func (f *mockChannelLink) CheckHtlcTransit(payHash [32]byte,
        amt lnwire.MilliSatoshi, timeout uint32,
        heightNow uint32, _ lnwire.CustomRecords) *LinkError <span class="cov8" title="1">{

        return f.checkHtlcTransitResult
}</span>

func (f *mockChannelLink) Stats() (
        uint64, lnwire.MilliSatoshi, lnwire.MilliSatoshi) <span class="cov8" title="1">{

        return 0, 0, 0
}</span>

func (f *mockChannelLink) AttachMailBox(mailBox MailBox) <span class="cov8" title="1">{
        f.mailBox = mailBox
        f.packets = mailBox.PacketOutBox()
        mailBox.SetDustClosure(f.getDustClosure())
}</span>

func (f *mockChannelLink) attachFailAliasUpdate(closure func(
        sid lnwire.ShortChannelID, incoming bool) *lnwire.ChannelUpdate1) <span class="cov8" title="1">{

        f.failAliasUpdate = closure
}</span>

func (f *mockChannelLink) getAliases() []lnwire.ShortChannelID <span class="cov8" title="1">{
        return f.aliases
}</span>

func (f *mockChannelLink) isZeroConf() bool <span class="cov8" title="1">{
        return f.zeroConf
}</span>

func (f *mockChannelLink) negotiatedAliasFeature() bool <span class="cov8" title="1">{
        return f.optionFeature
}</span>

func (f *mockChannelLink) confirmedScid() lnwire.ShortChannelID <span class="cov8" title="1">{
        return f.realScid
}</span>

func (f *mockChannelLink) zeroConfConfirmed() bool <span class="cov8" title="1">{
        return f.confirmedZC
}</span>

func (f *mockChannelLink) Start() error <span class="cov8" title="1">{
        f.mailBox.ResetMessages()
        f.mailBox.ResetPackets()
        return nil
}</span>

func (f *mockChannelLink) ChanID() lnwire.ChannelID <span class="cov8" title="1">{
        return f.chanID
}</span>

func (f *mockChannelLink) ShortChanID() lnwire.ShortChannelID <span class="cov8" title="1">{
        return f.shortChanID
}</span>

func (f *mockChannelLink) Bandwidth() lnwire.MilliSatoshi <span class="cov0" title="0">{
        return 99999999
}</span>

func (f *mockChannelLink) PeerPubKey() [33]byte <span class="cov8" title="1">{
        return f.peer.PubKey()
}</span>

func (f *mockChannelLink) ChannelPoint() wire.OutPoint <span class="cov0" title="0">{
        return wire.OutPoint{}
}</span>

func (f *mockChannelLink) Stop()                                        {<span class="cov8" title="1">}</span>
func (f *mockChannelLink) EligibleToForward() bool                      <span class="cov8" title="1">{ return f.eligible }</span>
func (f *mockChannelLink) MayAddOutgoingHtlc(lnwire.MilliSatoshi) error <span class="cov0" title="0">{ return nil }</span>
func (f *mockChannelLink) setLiveShortChanID(sid lnwire.ShortChannelID) <span class="cov0" title="0">{ f.shortChanID = sid }</span>
func (f *mockChannelLink) IsUnadvertised() bool                         <span class="cov8" title="1">{ return f.unadvertised }</span>
func (f *mockChannelLink) UpdateShortChanID() (lnwire.ShortChannelID, error) <span class="cov8" title="1">{
        f.eligible = true
        return f.shortChanID, nil
}</span>

func (f *mockChannelLink) EnableAdds(linkDirection LinkDirection) bool <span class="cov0" title="0">{
        // TODO(proofofkeags): Implement
        return true
}</span>

func (f *mockChannelLink) DisableAdds(linkDirection LinkDirection) bool <span class="cov0" title="0">{
        // TODO(proofofkeags): Implement
        return true
}</span>
func (f *mockChannelLink) IsFlushing(linkDirection LinkDirection) bool <span class="cov0" title="0">{
        // TODO(proofofkeags): Implement
        return false
}</span>
func (f *mockChannelLink) OnFlushedOnce(func()) {<span class="cov0" title="0">
        // TODO(proofofkeags): Implement
}</span>
func (f *mockChannelLink) OnCommitOnce(LinkDirection, func()) {<span class="cov0" title="0">
        // TODO(proofofkeags): Implement
}</span>
func (f *mockChannelLink) InitStfu() &lt;-chan fn.Result[lntypes.ChannelParty] <span class="cov0" title="0">{
        // TODO(proofofkeags): Implement
        c := make(chan fn.Result[lntypes.ChannelParty], 1)

        c &lt;- fn.Errf[lntypes.ChannelParty]("InitStfu not implemented")

        return c
}</span>

func (f *mockChannelLink) FundingCustomBlob() fn.Option[tlv.Blob] <span class="cov0" title="0">{
        return fn.None[tlv.Blob]()
}</span>

func (f *mockChannelLink) CommitmentCustomBlob() fn.Option[tlv.Blob] <span class="cov0" title="0">{
        return fn.None[tlv.Blob]()
}</span>

// AuxBandwidth returns the bandwidth that can be used for a channel,
// expressed in milli-satoshi. This might be different from the regular
// BTC bandwidth for custom channels. This will always return fn.None()
// for a regular (non-custom) channel.
func (f *mockChannelLink) AuxBandwidth(lnwire.MilliSatoshi,
        lnwire.ShortChannelID,
        fn.Option[tlv.Blob], AuxTrafficShaper) fn.Result[OptionalBandwidth] <span class="cov0" title="0">{

        return fn.Ok(OptionalBandwidth{})
}</span>

var _ ChannelLink = (*mockChannelLink)(nil)

const testInvoiceCltvExpiry = 6

type mockInvoiceRegistry struct {
        settleChan chan lntypes.Hash

        registry *invoices.InvoiceRegistry
}

type mockChainNotifier struct {
        chainntnfs.ChainNotifier
}

// RegisterBlockEpochNtfn mocks a successful call to register block
// notifications.
func (m *mockChainNotifier) RegisterBlockEpochNtfn(*chainntnfs.BlockEpoch) (
        *chainntnfs.BlockEpochEvent, error) <span class="cov8" title="1">{

        return &amp;chainntnfs.BlockEpochEvent{
                Cancel: func() </span>{<span class="cov0" title="0">}</span>,
        }, nil
}

func newMockRegistry(t testing.TB) *mockInvoiceRegistry <span class="cov8" title="1">{
        cdb := channeldb.OpenForTesting(t, t.TempDir())

        modifierMock := &amp;invoices.MockHtlcModifier{}
        registry := invoices.NewRegistry(
                cdb,
                invoices.NewInvoiceExpiryWatcher(
                        clock.NewDefaultClock(), 0, 0, nil,
                        &amp;mockChainNotifier{},
                ),
                &amp;invoices.RegistryConfig{
                        FinalCltvRejectDelta: 5,
                        HtlcInterceptor:      modifierMock,
                },
        )
        registry.Start()

        return &amp;mockInvoiceRegistry{
                registry: registry,
        }
}</span>

func (i *mockInvoiceRegistry) LookupInvoice(ctx context.Context,
        rHash lntypes.Hash) (invoices.Invoice, error) <span class="cov8" title="1">{

        return i.registry.LookupInvoice(ctx, rHash)
}</span>

func (i *mockInvoiceRegistry) SettleHodlInvoice(
        ctx context.Context, preimage lntypes.Preimage) error <span class="cov8" title="1">{

        return i.registry.SettleHodlInvoice(ctx, preimage)
}</span>

func (i *mockInvoiceRegistry) NotifyExitHopHtlc(rhash lntypes.Hash,
        amt lnwire.MilliSatoshi, expiry uint32, currentHeight int32,
        circuitKey models.CircuitKey, hodlChan chan&lt;- interface{},
        wireCustomRecords lnwire.CustomRecords,
        payload invoices.Payload) (invoices.HtlcResolution, error) <span class="cov8" title="1">{

        event, err := i.registry.NotifyExitHopHtlc(
                rhash, amt, expiry, currentHeight, circuitKey,
                hodlChan, wireCustomRecords, payload,
        )
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>
        <span class="cov8" title="1">if i.settleChan != nil </span><span class="cov8" title="1">{
                i.settleChan &lt;- rhash
        }</span>

        <span class="cov8" title="1">return event, nil</span>
}

func (i *mockInvoiceRegistry) CancelInvoice(ctx context.Context,
        payHash lntypes.Hash) error <span class="cov8" title="1">{

        return i.registry.CancelInvoice(ctx, payHash)
}</span>

func (i *mockInvoiceRegistry) AddInvoice(ctx context.Context,
        invoice invoices.Invoice, paymentHash lntypes.Hash) error <span class="cov8" title="1">{

        _, err := i.registry.AddInvoice(ctx, &amp;invoice, paymentHash)
        return err
}</span>

func (i *mockInvoiceRegistry) HodlUnsubscribeAll(
        subscriber chan&lt;- interface{}) <span class="cov8" title="1">{

        i.registry.HodlUnsubscribeAll(subscriber)
}</span>

var _ InvoiceDatabase = (*mockInvoiceRegistry)(nil)

type mockCircuitMap struct {
        lookup chan *PaymentCircuit
}

var _ CircuitMap = (*mockCircuitMap)(nil)

func (m *mockCircuitMap) OpenCircuits(...Keystone) error <span class="cov0" title="0">{
        return nil
}</span>

func (m *mockCircuitMap) TrimOpenCircuits(chanID lnwire.ShortChannelID,
        start uint64) error <span class="cov0" title="0">{
        return nil
}</span>

func (m *mockCircuitMap) DeleteCircuits(inKeys ...CircuitKey) error <span class="cov0" title="0">{
        return nil
}</span>

func (m *mockCircuitMap) CommitCircuits(
        circuit ...*PaymentCircuit) (*CircuitFwdActions, error) <span class="cov0" title="0">{

        return nil, nil
}</span>

func (m *mockCircuitMap) CloseCircuit(outKey CircuitKey) (*PaymentCircuit,
        error) <span class="cov0" title="0">{
        return nil, nil
}</span>

func (m *mockCircuitMap) FailCircuit(inKey CircuitKey) (*PaymentCircuit,
        error) <span class="cov0" title="0">{
        return nil, nil
}</span>

func (m *mockCircuitMap) LookupCircuit(inKey CircuitKey) *PaymentCircuit <span class="cov8" title="1">{
        return &lt;-m.lookup
}</span>

func (m *mockCircuitMap) LookupOpenCircuit(outKey CircuitKey) *PaymentCircuit <span class="cov0" title="0">{
        return nil
}</span>

func (m *mockCircuitMap) LookupByPaymentHash(hash [32]byte) []*PaymentCircuit <span class="cov0" title="0">{
        return nil
}</span>

func (m *mockCircuitMap) NumPending() int <span class="cov0" title="0">{
        return 0
}</span>

func (m *mockCircuitMap) NumOpen() int <span class="cov0" title="0">{
        return 0
}</span>

type mockOnionErrorDecryptor struct {
        sourceIdx int
        message   []byte
        err       error
}

func (m *mockOnionErrorDecryptor) DecryptError(encryptedData []byte) (
        *sphinx.DecryptedError, error) <span class="cov8" title="1">{

        return &amp;sphinx.DecryptedError{
                SenderIdx: m.sourceIdx,
                Message:   m.message,
        }, m.err
}</span>

var _ htlcNotifier = (*mockHTLCNotifier)(nil)

type mockHTLCNotifier struct {
        htlcNotifier //nolint:unused
}

func (h *mockHTLCNotifier) NotifyForwardingEvent(key HtlcKey, info HtlcInfo,
        eventType HtlcEventType) {<span class="cov8" title="1">

}</span>

func (h *mockHTLCNotifier) NotifyLinkFailEvent(key HtlcKey, info HtlcInfo,
        eventType HtlcEventType, linkErr *LinkError,
        incoming bool) {<span class="cov8" title="1">

}</span>

func (h *mockHTLCNotifier) NotifyForwardingFailEvent(key HtlcKey,
        eventType HtlcEventType) {<span class="cov8" title="1">

}</span>

func (h *mockHTLCNotifier) NotifySettleEvent(key HtlcKey,
        preimage lntypes.Preimage, eventType HtlcEventType) {<span class="cov8" title="1">

}</span>

func (h *mockHTLCNotifier) NotifyFinalHtlcEvent(key models.CircuitKey,
        info channeldb.FinalHtlcInfo) {<span class="cov8" title="1">

}</span>
</pre>
		
		<pre class="file" id="file13" style="display: none">package htlcswitch

import (
        "fmt"

        "github.com/lightningnetwork/lnd/channeldb"
        "github.com/lightningnetwork/lnd/graph/db/models"
        "github.com/lightningnetwork/lnd/htlcswitch/hop"
        "github.com/lightningnetwork/lnd/lnwire"
        "github.com/lightningnetwork/lnd/record"
)

// htlcPacket is a wrapper around htlc lnwire update, which adds additional
// information which is needed by this package.
type htlcPacket struct {
        // incomingChanID is the ID of the channel that we have received an incoming
        // HTLC on.
        incomingChanID lnwire.ShortChannelID

        // outgoingChanID is the ID of the channel that we have offered or will
        // offer an outgoing HTLC on.
        outgoingChanID lnwire.ShortChannelID

        // incomingHTLCID is the ID of the HTLC that we have received from the peer
        // on the incoming channel.
        incomingHTLCID uint64

        // outgoingHTLCID is the ID of the HTLC that we offered to the peer on the
        // outgoing channel.
        outgoingHTLCID uint64

        // sourceRef is used by forwarded htlcPackets to locate incoming Add
        // entry in a fwdpkg owned by the incoming link. This value can be nil
        // if there is no such entry, e.g. switch initiated payments.
        sourceRef *channeldb.AddRef

        // destRef is used to locate a settle/fail entry in the outgoing link's
        // fwdpkg. If sourceRef is non-nil, this reference should be to a
        // settle/fail in response to the sourceRef.
        destRef *channeldb.SettleFailRef

        // incomingAmount is the value in milli-satoshis that arrived on an
        // incoming link.
        incomingAmount lnwire.MilliSatoshi

        // amount is the value of the HTLC that is being created or modified.
        amount lnwire.MilliSatoshi

        // htlc lnwire message type of which depends on switch request type.
        htlc lnwire.Message

        // obfuscator contains the necessary state to allow the switch to wrap
        // any forwarded errors in an additional layer of encryption.
        obfuscator hop.ErrorEncrypter

        // localFailure is set to true if an HTLC fails for a local payment before
        // the first hop. In this case, the failure reason is simply encoded, not
        // encrypted with any shared secret.
        localFailure bool

        // linkFailure is non-nil for htlcs that fail at our node. This may
        // occur for our own payments which fail on the outgoing link,
        // or for forwards which fail in the switch or on the outgoing link.
        linkFailure *LinkError

        // convertedError is set to true if this is an HTLC fail that was
        // created using an UpdateFailMalformedHTLC from the remote party. If
        // this is true, then when forwarding this failure packet, we'll need
        // to wrap it as if we were the first hop if it's a multi-hop HTLC. If
        // it's a direct HTLC, then we'll decode the error as no encryption has
        // taken place.
        convertedError bool

        // hasSource is set to true if the incomingChanID and incomingHTLCID
        // fields of a forwarded fail packet are already set and do not need to
        // be looked up in the circuit map.
        hasSource bool

        // isResolution is set to true if this packet was actually an incoming
        // resolution message from an outside sub-system. We'll treat these as
        // if they emanated directly from the switch. As a result, we'll
        // encrypt all errors related to this packet as if we were the first
        // hop.
        isResolution bool

        // circuit holds a reference to an Add's circuit which is persisted in
        // the switch during successful forwarding.
        circuit *PaymentCircuit

        // incomingTimeout is the timeout that the incoming HTLC carried. This
        // is the timeout of the HTLC applied to the incoming link.
        incomingTimeout uint32

        // outgoingTimeout is the timeout of the proposed outgoing HTLC. This
        // will be extracted from the hop payload received by the incoming
        // link.
        outgoingTimeout uint32

        // inOnionCustomRecords are user-defined records in the custom type
        // range that were included in the onion payload.
        inOnionCustomRecords record.CustomSet

        // inWireCustomRecords are custom type range TLVs that are included
        // in the incoming update_add_htlc wire message.
        inWireCustomRecords lnwire.CustomRecords

        // originalOutgoingChanID is used when sending back failure messages.
        // It is only used for forwarded Adds on option_scid_alias channels.
        // This is to avoid possible confusion if a payer uses the public SCID
        // but receives a channel_update with the alias SCID. Instead, the
        // payer should receive a channel_update with the public SCID.
        originalOutgoingChanID lnwire.ShortChannelID

        // inboundFee is the fee schedule of the incoming channel.
        inboundFee models.InboundFee
}

// inKey returns the circuit key used to identify the incoming htlc.
func (p *htlcPacket) inKey() CircuitKey <span class="cov8" title="1">{
        return CircuitKey{
                ChanID: p.incomingChanID,
                HtlcID: p.incomingHTLCID,
        }
}</span>

// outKey returns the circuit key used to identify the outgoing, forwarded htlc.
func (p *htlcPacket) outKey() CircuitKey <span class="cov8" title="1">{
        return CircuitKey{
                ChanID: p.outgoingChanID,
                HtlcID: p.outgoingHTLCID,
        }
}</span>

// keystone returns a tuple containing the incoming and outgoing circuit keys.
func (p *htlcPacket) keystone() Keystone <span class="cov8" title="1">{
        return Keystone{
                InKey:  p.inKey(),
                OutKey: p.outKey(),
        }
}</span>

// String returns a human-readable description of the packet.
func (p *htlcPacket) String() string <span class="cov8" title="1">{
        return fmt.Sprintf("keystone=%v, sourceRef=%v, destRef=%v, "+
                "incomingAmount=%v, amount=%v, localFailure=%v, hasSource=%v "+
                "isResolution=%v", p.keystone(), p.sourceRef, p.destRef,
                p.incomingAmount, p.amount, p.localFailure, p.hasSource,
                p.isResolution)
}</span>
</pre>
		
		<pre class="file" id="file14" style="display: none">package htlcswitch

import (
        "bytes"
        "encoding/binary"
        "errors"
        "io"
        "sync"

        "github.com/lightningnetwork/lnd/channeldb"
        "github.com/lightningnetwork/lnd/kvdb"
        "github.com/lightningnetwork/lnd/lnwire"
        "github.com/lightningnetwork/lnd/multimutex"
)

var (

        // networkResultStoreBucketKey is used for the root level bucket that
        // stores the network result for each payment ID.
        networkResultStoreBucketKey = []byte("network-result-store-bucket")

        // ErrPaymentIDNotFound is an error returned if the given paymentID is
        // not found.
        ErrPaymentIDNotFound = errors.New("paymentID not found")

        // ErrPaymentIDAlreadyExists is returned if we try to write a pending
        // payment whose paymentID already exists.
        ErrPaymentIDAlreadyExists = errors.New("paymentID already exists")
)

// PaymentResult wraps a decoded result received from the network after a
// payment attempt was made. This is what is eventually handed to the router
// for processing.
type PaymentResult struct {
        // Preimage is set by the switch in case a sent HTLC was settled.
        Preimage [32]byte

        // Error is non-nil in case a HTLC send failed, and the HTLC is now
        // irrevocably canceled. If the payment failed during forwarding, this
        // error will be a *ForwardingError.
        Error error
}

// networkResult is the raw result received from the network after a payment
// attempt has been made. Since the switch doesn't always have the necessary
// data to decode the raw message, we store it together with some meta data,
// and decode it when the router query for the final result.
type networkResult struct {
        // msg is the received result. This should be of type UpdateFulfillHTLC
        // or UpdateFailHTLC.
        msg lnwire.Message

        // unencrypted indicates whether the failure encoded in the message is
        // unencrypted, and hence doesn't need to be decrypted.
        unencrypted bool

        // isResolution indicates whether this is a resolution message, in
        // which the failure reason might not be included.
        isResolution bool
}

// serializeNetworkResult serializes the networkResult.
func serializeNetworkResult(w io.Writer, n *networkResult) error <span class="cov8" title="1">{
        return channeldb.WriteElements(w, n.msg, n.unencrypted, n.isResolution)
}</span>

// deserializeNetworkResult deserializes the networkResult.
func deserializeNetworkResult(r io.Reader) (*networkResult, error) <span class="cov8" title="1">{
        n := &amp;networkResult{}

        if err := channeldb.ReadElements(r,
                &amp;n.msg, &amp;n.unencrypted, &amp;n.isResolution,
        ); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return n, nil</span>
}

// networkResultStore is a persistent store that stores any results of HTLCs in
// flight on the network. Since payment results are inherently asynchronous, it
// is used as a common access point for senders of HTLCs, to know when a result
// is back. The Switch will checkpoint any received result to the store, and
// the store will keep results and notify the callers about them.
type networkResultStore struct {
        backend kvdb.Backend

        // results is a map from paymentIDs to channels where subscribers to
        // payment results will be notified.
        results    map[uint64][]chan *networkResult
        resultsMtx sync.Mutex

        // attemptIDMtx is a multimutex used to make sure the database and
        // result subscribers map is consistent for each attempt ID in case of
        // concurrent callers.
        attemptIDMtx *multimutex.Mutex[uint64]
}

func newNetworkResultStore(db kvdb.Backend) *networkResultStore <span class="cov8" title="1">{
        return &amp;networkResultStore{
                backend:      db,
                results:      make(map[uint64][]chan *networkResult),
                attemptIDMtx: multimutex.NewMutex[uint64](),
        }
}</span>

// storeResult stores the networkResult for the given attemptID, and notifies
// any subscribers.
func (store *networkResultStore) storeResult(attemptID uint64,
        result *networkResult) error <span class="cov8" title="1">{

        // We get a mutex for this attempt ID. This is needed to ensure
        // consistency between the database state and the subscribers in case
        // of concurrent calls.
        store.attemptIDMtx.Lock(attemptID)
        defer store.attemptIDMtx.Unlock(attemptID)

        log.Debugf("Storing result for attemptID=%v", attemptID)

        // Serialize the payment result.
        var b bytes.Buffer
        if err := serializeNetworkResult(&amp;b, result); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">var attemptIDBytes [8]byte
        binary.BigEndian.PutUint64(attemptIDBytes[:], attemptID)

        err := kvdb.Batch(store.backend, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                networkResults, err := tx.CreateTopLevelBucket(
                        networkResultStoreBucketKey,
                )
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov8" title="1">return networkResults.Put(attemptIDBytes[:], b.Bytes())</span>
        })
        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Now that the result is stored in the database, we can notify any
        // active subscribers.
        <span class="cov8" title="1">store.resultsMtx.Lock()
        for _, res := range store.results[attemptID] </span><span class="cov8" title="1">{
                res &lt;- result
        }</span>
        <span class="cov8" title="1">delete(store.results, attemptID)
        store.resultsMtx.Unlock()

        return nil</span>
}

// subscribeResult is used to get the HTLC attempt result for the given attempt
// ID.  It returns a channel on which the result will be delivered when ready.
func (store *networkResultStore) subscribeResult(attemptID uint64) (
        &lt;-chan *networkResult, error) <span class="cov8" title="1">{

        // We get a mutex for this payment ID. This is needed to ensure
        // consistency between the database state and the subscribers in case
        // of concurrent calls.
        store.attemptIDMtx.Lock(attemptID)
        defer store.attemptIDMtx.Unlock(attemptID)

        log.Debugf("Subscribing to result for attemptID=%v", attemptID)

        var (
                result     *networkResult
                resultChan = make(chan *networkResult, 1)
        )

        err := kvdb.View(store.backend, func(tx kvdb.RTx) error </span><span class="cov8" title="1">{
                var err error
                result, err = fetchResult(tx, attemptID)
                switch </span>{

                // Result not yet available, we will notify once a result is
                // available.
                case err == ErrPaymentIDNotFound:<span class="cov8" title="1">
                        return nil</span>

                case err != nil:<span class="cov0" title="0">
                        return err</span>

                // The result was found, and will be returned immediately.
                default:<span class="cov8" title="1">
                        return nil</span>
                }
        }, func() <span class="cov8" title="1">{
                result = nil
        }</span>)
        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // If the result was found, we can send it on the result channel
        // imemdiately.
        <span class="cov8" title="1">if result != nil </span><span class="cov8" title="1">{
                resultChan &lt;- result
                return resultChan, nil
        }</span>

        // Otherwise we store the result channel for when the result is
        // available.
        <span class="cov8" title="1">store.resultsMtx.Lock()
        store.results[attemptID] = append(
                store.results[attemptID], resultChan,
        )
        store.resultsMtx.Unlock()

        return resultChan, nil</span>
}

// getResult attempts to immediately fetch the result for the given pid from
// the store. If no result is available, ErrPaymentIDNotFound is returned.
func (store *networkResultStore) getResult(pid uint64) (
        *networkResult, error) <span class="cov8" title="1">{

        var result *networkResult
        err := kvdb.View(store.backend, func(tx kvdb.RTx) error </span><span class="cov8" title="1">{
                var err error
                result, err = fetchResult(tx, pid)
                return err
        }</span>, func() <span class="cov8" title="1">{
                result = nil
        }</span>)
        <span class="cov8" title="1">if err != nil </span><span class="cov8" title="1">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return result, nil</span>
}

func fetchResult(tx kvdb.RTx, pid uint64) (*networkResult, error) <span class="cov8" title="1">{
        var attemptIDBytes [8]byte
        binary.BigEndian.PutUint64(attemptIDBytes[:], pid)

        networkResults := tx.ReadBucket(networkResultStoreBucketKey)
        if networkResults == nil </span><span class="cov8" title="1">{
                return nil, ErrPaymentIDNotFound
        }</span>

        // Check whether a result is already available.
        <span class="cov8" title="1">resultBytes := networkResults.Get(attemptIDBytes[:])
        if resultBytes == nil </span><span class="cov8" title="1">{
                return nil, ErrPaymentIDNotFound
        }</span>

        // Decode the result we found.
        <span class="cov8" title="1">r := bytes.NewReader(resultBytes)

        return deserializeNetworkResult(r)</span>
}

// cleanStore removes all entries from the store, except the payment IDs given.
// NOTE: Since every result not listed in the keep map will be deleted, care
// should be taken to ensure no new payment attempts are being made
// concurrently while this process is ongoing, as its result might end up being
// deleted.
func (store *networkResultStore) cleanStore(keep map[uint64]struct{}) error <span class="cov8" title="1">{
        return kvdb.Update(store.backend, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                networkResults, err := tx.CreateTopLevelBucket(
                        networkResultStoreBucketKey,
                )
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                // Iterate through the bucket, deleting all items not in the
                // keep map.
                <span class="cov8" title="1">var toClean [][]byte
                if err := networkResults.ForEach(func(k, _ []byte) error </span><span class="cov8" title="1">{
                        pid := binary.BigEndian.Uint64(k)
                        if _, ok := keep[pid]; ok </span><span class="cov8" title="1">{
                                return nil
                        }</span>

                        <span class="cov8" title="1">toClean = append(toClean, k)
                        return nil</span>
                }); err != nil <span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov8" title="1">for _, k := range toClean </span><span class="cov8" title="1">{
                        err := networkResults.Delete(k)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>
                }

                <span class="cov8" title="1">if len(toClean) &gt; 0 </span><span class="cov8" title="1">{
                        log.Infof("Removed %d stale entries from network "+
                                "result store", len(toClean))
                }</span>

                <span class="cov8" title="1">return nil</span>
        }, func() {<span class="cov8" title="1">}</span>)
}
</pre>
		
		<pre class="file" id="file15" style="display: none">package htlcswitch

import (
        "fmt"
        "sync"
        "time"

        "github.com/btcsuite/btclog/v2"
        "github.com/lightningnetwork/lnd/fn/v2"
        "github.com/lightningnetwork/lnd/lntypes"
        "github.com/lightningnetwork/lnd/lnwire"
)

var (
        // ErrInvalidStfu indicates that the Stfu we have received is invalid.
        // This can happen in instances where we have not sent Stfu but we have
        // received one with the initiator field set to false.
        ErrInvalidStfu = fmt.Errorf("stfu received is invalid")

        // ErrStfuAlreadySent indicates that this channel has already sent an
        // Stfu message for this negotiation.
        ErrStfuAlreadySent = fmt.Errorf("stfu already sent")

        // ErrStfuAlreadyRcvd indicates that this channel has already received
        // an Stfu message for this negotiation.
        ErrStfuAlreadyRcvd = fmt.Errorf("stfu already received")

        // ErrNoQuiescenceInitiator indicates that the caller has requested the
        // quiescence initiator for a channel that is not yet quiescent.
        ErrNoQuiescenceInitiator = fmt.Errorf(
                "indeterminate quiescence initiator: channel is not quiescent",
        )

        // ErrPendingRemoteUpdates indicates that we have received an Stfu while
        // the remote party has issued updates that are not yet bilaterally
        // committed.
        ErrPendingRemoteUpdates = fmt.Errorf(
                "stfu received with pending remote updates",
        )

        // ErrPendingLocalUpdates indicates that we are attempting to send an
        // Stfu while we have issued updates that are not yet bilaterally
        // committed.
        ErrPendingLocalUpdates = fmt.Errorf(
                "stfu send attempted with pending local updates",
        )

        // ErrQuiescenceTimeout indicates that the quiescer has been quiesced
        // beyond the allotted time.
        ErrQuiescenceTimeout = fmt.Errorf(
                "quiescence timeout",
        )
)

const defaultQuiescenceTimeout = 30 * time.Second

type StfuReq = fn.Req[fn.Unit, fn.Result[lntypes.ChannelParty]]

// Quiescer is the public interface of the quiescence mechanism. Callers of the
// quiescence API should not need any methods besides the ones detailed here.
type Quiescer interface {
        // IsQuiescent returns true if the state machine has been driven all the
        // way to completion. If this returns true, processes that depend on
        // channel quiescence may proceed.
        IsQuiescent() bool

        // QuiescenceInitiator determines which ChannelParty is the initiator of
        // quiescence for the purposes of downstream protocols. If the channel
        // is not currently quiescent, this method will return
        // ErrNoDownstreamLeader.
        QuiescenceInitiator() fn.Result[lntypes.ChannelParty]

        // InitStfu instructs the quiescer that we intend to begin a quiescence
        // negotiation where we are the initiator. We don't yet send stfu yet
        // because we need to wait for the link to give us a valid opportunity
        // to do so.
        InitStfu(req StfuReq)

        // RecvStfu is called when we receive an Stfu message from the remote.
        RecvStfu(stfu lnwire.Stfu) error

        // CanRecvUpdates returns true if we haven't yet received an Stfu which
        // would mark the end of the remote's ability to send updates.
        CanRecvUpdates() bool

        // CanSendUpdates returns true if we haven't yet sent an Stfu which
        // would mark the end of our ability to send updates.
        CanSendUpdates() bool

        // SendOwedStfu sends Stfu if it owes one. It returns an error if the
        // state machine is in an invalid state.
        SendOwedStfu() error

        // OnResume accepts a no return closure that will run when the quiescer
        // is resumed.
        OnResume(hook func())

        // Resume runs all of the deferred actions that have accumulated while
        // the channel has been quiescent and then resets the quiescer state to
        // its initial state.
        Resume()
}

// QuiescerCfg is a config structure used to initialize a quiescer giving it the
// appropriate functionality to interact with the channel state that the
// quiescer must syncrhonize with.
type QuiescerCfg struct {
        // chanID marks what channel we are managing the state machine for. This
        // is important because the quiescer needs to know the ChannelID to
        // construct the Stfu message.
        chanID lnwire.ChannelID

        // channelInitiator indicates which ChannelParty originally opened the
        // channel. This is used to break ties when both sides of the channel
        // send Stfu claiming to be the initiator.
        channelInitiator lntypes.ChannelParty

        // sendMsg is a function that can be used to send an Stfu message over
        // the wire.
        sendMsg func(lnwire.Stfu) error

        // timeoutDuration is the Duration that we will wait from the moment the
        // channel is considered quiescent before we call the onTimeout function
        timeoutDuration time.Duration

        // onTimeout is a function that will be called in the event that the
        // Quiescer has not been resumed before the timeout is reached. If
        // Quiescer.Resume is called before the timeout has been raeached, then
        // onTimeout will not be called until the quiescer reaches a quiescent
        // state again.
        onTimeout func()
}

// QuiescerLive is a state machine that tracks progression through the
// quiescence protocol.
type QuiescerLive struct {
        cfg QuiescerCfg

        // log is a quiescer-scoped logging instance.
        log btclog.Logger

        // localInit indicates whether our path through this state machine was
        // initiated by our node. This can be true or false independently of
        // remoteInit.
        localInit bool

        // remoteInit indicates whether we received Stfu from our peer where the
        // message indicated that the remote node believes it was the initiator.
        // This can be true or false independently of localInit.
        remoteInit bool

        // sent tracks whether or not we have emitted Stfu for sending.
        sent bool

        // received tracks whether or not we have received Stfu from our peer.
        received bool

        // activeQuiescenceRequest is a possibly None Request that we should
        // resolve when we complete quiescence.
        activeQuiescenceReq fn.Option[StfuReq]

        // resumeQueue is a slice of hooks that will be called when the quiescer
        // is resumed. These are actions that needed to be deferred while the
        // channel was quiescent.
        resumeQueue []func()

        // timeoutTimer is a field that is used to hold onto the timeout job
        // when we reach quiescence.
        timeoutTimer *time.Timer

        sync.RWMutex
}

// NewQuiescer creates a new quiescer for the given channel.
func NewQuiescer(cfg QuiescerCfg) Quiescer <span class="cov8" title="1">{
        logPrefix := fmt.Sprintf("Quiescer(%v):", cfg.chanID)

        return &amp;QuiescerLive{
                cfg: cfg,
                log: log.WithPrefix(logPrefix),
        }
}</span>

// RecvStfu is called when we receive an Stfu message from the remote.
func (q *QuiescerLive) RecvStfu(msg lnwire.Stfu) error <span class="cov8" title="1">{
        q.Lock()
        defer q.Unlock()

        return q.recvStfu(msg)
}</span>

// recvStfu is called when we receive an Stfu message from the remote.
func (q *QuiescerLive) recvStfu(msg lnwire.Stfu) error <span class="cov8" title="1">{
        // At the time of this writing, this check that we have already received
        // an Stfu is not strictly necessary, according to the specification.
        // However, it is fishy if we do and it is unclear how we should handle
        // such a case so we will err on the side of caution.
        if q.received </span><span class="cov8" title="1">{
                return fmt.Errorf("%w for channel %v", ErrStfuAlreadyRcvd,
                        q.cfg.chanID)
        }</span>

        // We need to check that the Stfu we are receiving is valid.
        <span class="cov8" title="1">if !q.sent &amp;&amp; !msg.Initiator </span><span class="cov0" title="0">{
                return fmt.Errorf("%w for channel %v", ErrInvalidStfu,
                        q.cfg.chanID)
        }</span>

        <span class="cov8" title="1">if !q.canRecvStfu() </span><span class="cov0" title="0">{
                return fmt.Errorf("%w for channel %v", ErrPendingRemoteUpdates,
                        q.cfg.chanID)
        }</span>

        <span class="cov8" title="1">q.received = true

        // If the remote party sets the initiator bit to true then we will
        // remember that they are making a claim to the initiator role. This
        // does not necessarily mean they will get it, though.
        q.remoteInit = msg.Initiator

        // Since we just received an Stfu, we may have a newly quiesced state.
        // If so, we will try to resolve any outstanding StfuReqs.
        q.tryResolveStfuReq()

        if q.isQuiescent() </span><span class="cov8" title="1">{
                q.startTimeout()
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// MakeStfu is called when we are ready to send an Stfu message. It returns the
// Stfu message to be sent.
func (q *QuiescerLive) MakeStfu() fn.Result[lnwire.Stfu] <span class="cov8" title="1">{
        q.RLock()
        defer q.RUnlock()

        return q.makeStfu()
}</span>

// makeStfu is called when we are ready to send an Stfu message. It returns the
// Stfu message to be sent.
func (q *QuiescerLive) makeStfu() fn.Result[lnwire.Stfu] <span class="cov8" title="1">{
        if q.sent </span><span class="cov8" title="1">{
                return fn.Errf[lnwire.Stfu]("%w for channel %v",
                        ErrStfuAlreadySent, q.cfg.chanID)
        }</span>

        <span class="cov8" title="1">if !q.canSendStfu() </span><span class="cov0" title="0">{
                return fn.Errf[lnwire.Stfu]("%w for channel %v",
                        ErrPendingLocalUpdates, q.cfg.chanID)
        }</span>

        <span class="cov8" title="1">stfu := lnwire.Stfu{
                ChanID:    q.cfg.chanID,
                Initiator: q.localInit,
        }

        return fn.Ok(stfu)</span>
}

// OweStfu returns true if we owe the other party an Stfu. We owe the remote an
// Stfu when we have received but not yet sent an Stfu, or we are the initiator
// but have not yet sent an Stfu.
func (q *QuiescerLive) OweStfu() bool <span class="cov0" title="0">{
        q.RLock()
        defer q.RUnlock()

        return q.oweStfu()
}</span>

// oweStfu returns true if we owe the other party an Stfu. We owe the remote an
// Stfu when we have received but not yet sent an Stfu, or we are the initiator
// but have not yet sent an Stfu.
func (q *QuiescerLive) oweStfu() bool <span class="cov8" title="1">{
        return (q.received || q.localInit) &amp;&amp; !q.sent
}</span>

// NeedStfu returns true if the remote owes us an Stfu. They owe us an Stfu when
// we have sent but not yet received an Stfu.
func (q *QuiescerLive) NeedStfu() bool <span class="cov8" title="1">{
        q.RLock()
        defer q.RUnlock()

        return q.needStfu()
}</span>

// needStfu returns true if the remote owes us an Stfu. They owe us an Stfu when
// we have sent but not yet received an Stfu.
func (q *QuiescerLive) needStfu() bool <span class="cov8" title="1">{
        q.RLock()
        defer q.RUnlock()

        return q.sent &amp;&amp; !q.received
}</span>

// IsQuiescent returns true if the state machine has been driven all the way to
// completion. If this returns true, processes that depend on channel quiescence
// may proceed.
func (q *QuiescerLive) IsQuiescent() bool <span class="cov8" title="1">{
        q.RLock()
        defer q.RUnlock()

        return q.isQuiescent()
}</span>

// isQuiescent returns true if the state machine has been driven all the way to
// completion. If this returns true, processes that depend on channel quiescence
// may proceed.
func (q *QuiescerLive) isQuiescent() bool <span class="cov8" title="1">{
        return q.sent &amp;&amp; q.received
}</span>

// QuiescenceInitiator determines which ChannelParty is the initiator of
// quiescence for the purposes of downstream protocols. If the channel is not
// currently quiescent, this method will return ErrNoQuiescenceInitiator.
func (q *QuiescerLive) QuiescenceInitiator() fn.Result[lntypes.ChannelParty] <span class="cov8" title="1">{
        q.RLock()
        defer q.RUnlock()

        return q.quiescenceInitiator()
}</span>

// quiescenceInitiator determines which ChannelParty is the initiator of
// quiescence for the purposes of downstream protocols. If the channel is not
// currently quiescent, this method will return ErrNoQuiescenceInitiator.
func (q *QuiescerLive) quiescenceInitiator() fn.Result[lntypes.ChannelParty] <span class="cov8" title="1">{
        switch </span>{
        case !q.isQuiescent():<span class="cov8" title="1">
                return fn.Err[lntypes.ChannelParty](ErrNoQuiescenceInitiator)</span>

        case q.localInit &amp;&amp; q.remoteInit:<span class="cov8" title="1">
                // In the case of a tie, the channel initiator wins.
                return fn.Ok(q.cfg.channelInitiator)</span>

        case q.localInit:<span class="cov8" title="1">
                return fn.Ok(lntypes.Local)</span>

        case q.remoteInit:<span class="cov8" title="1">
                return fn.Ok(lntypes.Remote)</span>
        }

        // unreachable
        <span class="cov0" title="0">return fn.Err[lntypes.ChannelParty](ErrNoQuiescenceInitiator)</span>
}

// CanSendUpdates returns true if we haven't yet sent an Stfu which would mark
// the end of our ability to send updates.
func (q *QuiescerLive) CanSendUpdates() bool <span class="cov8" title="1">{
        q.RLock()
        defer q.RUnlock()

        return q.canSendUpdates()
}</span>

// canSendUpdates returns true if we haven't yet sent an Stfu which would mark
// the end of our ability to send updates.
func (q *QuiescerLive) canSendUpdates() bool <span class="cov8" title="1">{
        return !q.sent &amp;&amp; !q.localInit
}</span>

// CanRecvUpdates returns true if we haven't yet received an Stfu which would
// mark the end of the remote's ability to send updates.
func (q *QuiescerLive) CanRecvUpdates() bool <span class="cov8" title="1">{
        q.RLock()
        defer q.RUnlock()

        return q.canRecvUpdates()
}</span>

// canRecvUpdates returns true if we haven't yet received an Stfu which would
// mark the end of the remote's ability to send updates.
func (q *QuiescerLive) canRecvUpdates() bool <span class="cov8" title="1">{
        return !q.received
}</span>

// CanSendStfu returns true if we can send an Stfu.
func (q *QuiescerLive) CanSendStfu(numPendingLocalUpdates uint64) bool <span class="cov0" title="0">{
        q.RLock()
        defer q.RUnlock()

        return q.canSendStfu()
}</span>

// canSendStfu returns true if we can send an Stfu.
func (q *QuiescerLive) canSendStfu() bool <span class="cov8" title="1">{
        return !q.sent
}</span>

// CanRecvStfu returns true if we can receive an Stfu.
func (q *QuiescerLive) CanRecvStfu() bool <span class="cov0" title="0">{
        q.RLock()
        defer q.RUnlock()

        return q.canRecvStfu()
}</span>

// canRecvStfu returns true if we can receive an Stfu.
func (q *QuiescerLive) canRecvStfu() bool <span class="cov8" title="1">{
        return !q.received
}</span>

// SendOwedStfu sends Stfu if it owes one. It returns an error if the state
// machine is in an invalid state.
func (q *QuiescerLive) SendOwedStfu() error <span class="cov8" title="1">{
        q.Lock()
        defer q.Unlock()

        return q.sendOwedStfu()
}</span>

// sendOwedStfu sends Stfu if it owes one. It returns an error if the state
// machine is in an invalid state.
func (q *QuiescerLive) sendOwedStfu() error <span class="cov8" title="1">{
        if !q.oweStfu() || !q.canSendStfu() </span><span class="cov8" title="1">{
                return nil
        }</span>

        <span class="cov8" title="1">err := q.makeStfu().Sink(q.cfg.sendMsg)

        if err == nil </span><span class="cov8" title="1">{
                q.sent = true

                // Since we just sent an Stfu, we may have a newly quiesced
                // state. If so, we will try to resolve any outstanding
                // StfuReqs.
                q.tryResolveStfuReq()

                if q.isQuiescent() </span><span class="cov8" title="1">{
                        q.startTimeout()
                }</span>
        }

        <span class="cov8" title="1">return err</span>
}

// TryResolveStfuReq attempts to resolve the active quiescence request if the
// state machine has reached a quiescent state.
func (q *QuiescerLive) TryResolveStfuReq() <span class="cov0" title="0">{
        q.Lock()
        defer q.Unlock()

        q.tryResolveStfuReq()
}</span>

// tryResolveStfuReq attempts to resolve the active quiescence request if the
// state machine has reached a quiescent state.
func (q *QuiescerLive) tryResolveStfuReq() <span class="cov8" title="1">{
        q.activeQuiescenceReq.WhenSome(
                func(req StfuReq) </span><span class="cov8" title="1">{
                        if q.isQuiescent() </span><span class="cov8" title="1">{
                                req.Resolve(q.quiescenceInitiator())
                                q.activeQuiescenceReq = fn.None[StfuReq]()
                        }</span>
                },
        )
}

// InitStfu instructs the quiescer that we intend to begin a quiescence
// negotiation where we are the initiator. We don't yet send stfu yet because
// we need to wait for the link to give us a valid opportunity to do so.
func (q *QuiescerLive) InitStfu(req StfuReq) <span class="cov8" title="1">{
        q.Lock()
        defer q.Unlock()

        q.initStfu(req)
}</span>

// initStfu instructs the quiescer that we intend to begin a quiescence
// negotiation where we are the initiator. We don't yet send stfu yet because
// we need to wait for the link to give us a valid opportunity to do so.
func (q *QuiescerLive) initStfu(req StfuReq) <span class="cov8" title="1">{
        if q.localInit </span><span class="cov8" title="1">{
                req.Resolve(fn.Errf[lntypes.ChannelParty](
                        "quiescence already requested",
                ))

                return
        }</span>

        <span class="cov8" title="1">q.localInit = true
        q.activeQuiescenceReq = fn.Some(req)</span>
}

// OnResume accepts a no return closure that will run when the quiescer is
// resumed.
func (q *QuiescerLive) OnResume(hook func()) <span class="cov8" title="1">{
        q.Lock()
        defer q.Unlock()

        q.onResume(hook)
}</span>

// onResume accepts a no return closure that will run when the quiescer is
// resumed.
func (q *QuiescerLive) onResume(hook func()) <span class="cov8" title="1">{
        q.resumeQueue = append(q.resumeQueue, hook)
}</span>

// Resume runs all of the deferred actions that have accumulated while the
// channel has been quiescent and then resets the quiescer state to its initial
// state.
func (q *QuiescerLive) Resume() <span class="cov8" title="1">{
        q.Lock()
        defer q.Unlock()

        q.resume()
}</span>

// resume runs all of the deferred actions that have accumulated while the
// channel has been quiescent and then resets the quiescer state to its initial
// state.
func (q *QuiescerLive) resume() <span class="cov8" title="1">{
        q.log.Debug("quiescence terminated, resuming htlc traffic")

        // since we are resuming we want to cancel the quiescence timeout
        // action.
        q.cancelTimeout()

        for _, hook := range q.resumeQueue </span><span class="cov8" title="1">{
                hook()
        }</span>
        <span class="cov8" title="1">q.localInit = false
        q.remoteInit = false
        q.sent = false
        q.received = false
        q.resumeQueue = nil</span>
}

// startTimeout starts the timeout function that fires if the quiescer remains
// in a quiesced state for too long. If this function is called multiple times
// only the last one will have an effect.
func (q *QuiescerLive) startTimeout() <span class="cov8" title="1">{
        if q.cfg.onTimeout == nil </span><span class="cov8" title="1">{
                return
        }</span>

        <span class="cov8" title="1">old := q.timeoutTimer

        q.timeoutTimer = time.AfterFunc(q.cfg.timeoutDuration, q.cfg.onTimeout)

        if old != nil </span><span class="cov0" title="0">{
                old.Stop()
        }</span>
}

// cancelTimeout cancels the timeout function that would otherwise fire if the
// quiescer remains in a quiesced state too long. If this function is called
// before startTimeout or after another call to cancelTimeout, the effect will
// be a noop.
func (q *QuiescerLive) cancelTimeout() <span class="cov8" title="1">{
        if q.timeoutTimer != nil </span><span class="cov8" title="1">{
                q.timeoutTimer.Stop()
                q.timeoutTimer = nil
        }</span>
}

type quiescerNoop struct{}

var _ Quiescer = (*quiescerNoop)(nil)

func (q *quiescerNoop) InitStfu(req StfuReq) <span class="cov0" title="0">{
        req.Resolve(fn.Errf[lntypes.ChannelParty]("quiescence not supported"))
}</span>
func (q *quiescerNoop) RecvStfu(_ lnwire.Stfu) error <span class="cov0" title="0">{ return nil }</span>
func (q *quiescerNoop) CanRecvUpdates() bool         <span class="cov0" title="0">{ return true }</span>
func (q *quiescerNoop) CanSendUpdates() bool         <span class="cov0" title="0">{ return true }</span>
func (q *quiescerNoop) SendOwedStfu() error          <span class="cov0" title="0">{ return nil }</span>
func (q *quiescerNoop) IsQuiescent() bool            <span class="cov0" title="0">{ return false }</span>
func (q *quiescerNoop) OnResume(hook func())         <span class="cov0" title="0">{ hook() }</span>
func (q *quiescerNoop) Resume()                      {<span class="cov0" title="0">}</span>
func (q *quiescerNoop) QuiescenceInitiator() fn.Result[lntypes.ChannelParty] <span class="cov0" title="0">{
        return fn.Err[lntypes.ChannelParty](ErrNoQuiescenceInitiator)
}</span>
</pre>
		
		<pre class="file" id="file16" style="display: none">package htlcswitch

import (
        "bytes"
        "io"

        "github.com/go-errors/errors"
        "github.com/lightningnetwork/lnd/channeldb"
        "github.com/lightningnetwork/lnd/contractcourt"
        "github.com/lightningnetwork/lnd/kvdb"
        "github.com/lightningnetwork/lnd/lnwire"
)

var (
        // resBucketKey is used for the root level bucket that stores the
        // CircuitKey -&gt; ResolutionMsg mapping.
        resBucketKey = []byte("resolution-store-bucket-key")

        // errResMsgNotFound is used to let callers know that the resolution
        // message was not found for the given CircuitKey. This is used in the
        // checkResolutionMsg function.
        errResMsgNotFound = errors.New("resolution message not found")
)

// resolutionStore contains ResolutionMsgs received from the contractcourt. The
// Switch deletes these from the store when the underlying circuit has been
// removed via DeleteCircuits. If the circuit hasn't been deleted, the Switch
// will dispatch the ResolutionMsg to a link if this was a multi-hop HTLC or to
// itself if the Switch initiated the payment.
type resolutionStore struct {
        backend kvdb.Backend
}

func newResolutionStore(db kvdb.Backend) *resolutionStore <span class="cov8" title="1">{
        return &amp;resolutionStore{
                backend: db,
        }
}</span>

// addResolutionMsg persists a ResolutionMsg to the resolutionStore.
func (r *resolutionStore) addResolutionMsg(
        resMsg *contractcourt.ResolutionMsg) error <span class="cov8" title="1">{

        // The outKey will be the database key.
        outKey := &amp;CircuitKey{
                ChanID: resMsg.SourceChan,
                HtlcID: resMsg.HtlcIndex,
        }

        var resBuf bytes.Buffer
        if err := serializeResolutionMsg(&amp;resBuf, resMsg); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">err := kvdb.Update(r.backend, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                resBucket, err := tx.CreateTopLevelBucket(resBucketKey)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov8" title="1">return resBucket.Put(outKey.Bytes(), resBuf.Bytes())</span>
        }, func() {<span class="cov8" title="1">}</span>)
        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// checkResolutionMsg returns nil if the resolution message is found in the
// store. It returns an error if no resolution message was found for the
// passed outKey or if a database error occurred.
func (r *resolutionStore) checkResolutionMsg(outKey *CircuitKey) error <span class="cov8" title="1">{
        err := kvdb.View(r.backend, func(tx kvdb.RTx) error </span><span class="cov8" title="1">{
                resBucket := tx.ReadBucket(resBucketKey)
                if resBucket == nil </span><span class="cov0" title="0">{
                        // Return an error if the bucket doesn't exist.
                        return errResMsgNotFound
                }</span>

                <span class="cov8" title="1">msg := resBucket.Get(outKey.Bytes())
                if msg == nil </span><span class="cov8" title="1">{
                        // Return the not found error since no message exists
                        // for this CircuitKey.
                        return errResMsgNotFound
                }</span>

                // Return nil to indicate that the message was found.
                <span class="cov8" title="1">return nil</span>
        }, func() {<span class="cov8" title="1">}</span>)
        <span class="cov8" title="1">if err != nil </span><span class="cov8" title="1">{
                return err
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// fetchAllResolutionMsg returns a slice of all stored ResolutionMsgs. This is
// used by the Switch on start-up.
func (r *resolutionStore) fetchAllResolutionMsg() (
        []*contractcourt.ResolutionMsg, error) <span class="cov8" title="1">{

        var msgs []*contractcourt.ResolutionMsg

        err := kvdb.View(r.backend, func(tx kvdb.RTx) error </span><span class="cov8" title="1">{
                resBucket := tx.ReadBucket(resBucketKey)
                if resBucket == nil </span><span class="cov8" title="1">{
                        return nil
                }</span>

                <span class="cov8" title="1">return resBucket.ForEach(func(k, v []byte) error </span><span class="cov8" title="1">{
                        kr := bytes.NewReader(k)
                        outKey := &amp;CircuitKey{}
                        if err := outKey.Decode(kr); err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>

                        <span class="cov8" title="1">vr := bytes.NewReader(v)
                        resMsg, err := deserializeResolutionMsg(vr)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>

                        // Set the CircuitKey values on the ResolutionMsg.
                        <span class="cov8" title="1">resMsg.SourceChan = outKey.ChanID
                        resMsg.HtlcIndex = outKey.HtlcID

                        msgs = append(msgs, resMsg)
                        return nil</span>
                })
        }, func() <span class="cov8" title="1">{
                msgs = nil
        }</span>)
        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return msgs, nil</span>
}

// deleteResolutionMsg removes a ResolutionMsg with the passed-in CircuitKey.
func (r *resolutionStore) deleteResolutionMsg(outKey *CircuitKey) error <span class="cov8" title="1">{
        err := kvdb.Update(r.backend, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                resBucket, err := tx.CreateTopLevelBucket(resBucketKey)
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov8" title="1">return resBucket.Delete(outKey.Bytes())</span>
        }, func() {<span class="cov8" title="1">}</span>)
        <span class="cov8" title="1">return err</span>
}

// serializeResolutionMsg writes part of a ResolutionMsg to the passed
// io.Writer.
func serializeResolutionMsg(w io.Writer,
        resMsg *contractcourt.ResolutionMsg) error <span class="cov8" title="1">{

        isFail := resMsg.Failure != nil

        if err := channeldb.WriteElement(w, isFail); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // If this is a failure message, then we're done serializing.
        <span class="cov8" title="1">if isFail </span><span class="cov8" title="1">{
                return nil
        }</span>

        // Else this is a settle message, and we need to write the preimage.
        <span class="cov8" title="1">return channeldb.WriteElement(w, *resMsg.PreImage)</span>
}

// deserializeResolutionMsg reads part of a ResolutionMsg from the passed
// io.Reader.
func deserializeResolutionMsg(r io.Reader) (*contractcourt.ResolutionMsg,
        error) <span class="cov8" title="1">{

        resMsg := &amp;contractcourt.ResolutionMsg{}
        var isFail bool

        if err := channeldb.ReadElements(r, &amp;isFail); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // If a failure resolution msg was stored, set the Failure field.
        <span class="cov8" title="1">if isFail </span><span class="cov8" title="1">{
                failureMsg := &amp;lnwire.FailPermanentChannelFailure{}
                resMsg.Failure = failureMsg
                return resMsg, nil
        }</span>

        <span class="cov8" title="1">var preimage [32]byte
        resMsg.PreImage = &amp;preimage

        // Else this is a settle resolution msg and we will read the preimage.
        if err := channeldb.ReadElement(r, resMsg.PreImage); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return resMsg, nil</span>
}
</pre>
		
		<pre class="file" id="file17" style="display: none">package htlcswitch

import (
        "sync"

        "github.com/go-errors/errors"
        "github.com/lightningnetwork/lnd/channeldb"
        "github.com/lightningnetwork/lnd/kvdb"
)

// defaultSequenceBatchSize specifies the window of sequence numbers that are
// allocated for each write to disk made by the sequencer.
const defaultSequenceBatchSize = 1000

// Sequencer emits sequence numbers for locally initiated HTLCs. These are
// only used internally for tracking pending payments, however they must be
// unique in order to avoid circuit key collision in the circuit map.
type Sequencer interface {
        // NextID returns a unique sequence number for each invocation.
        NextID() (uint64, error)
}

var (
        // nextPaymentIDKey identifies the bucket that will keep track of the
        // persistent sequence numbers for payments.
        nextPaymentIDKey = []byte("next-payment-id-key")

        // ErrSequencerCorrupted signals that the persistence engine was not
        // initialized, or has been corrupted since startup.
        ErrSequencerCorrupted = errors.New(
                "sequencer database has been corrupted")
)

// persistentSequencer is a concrete implementation of IDGenerator, that uses
// channeldb to allocate sequence numbers.
type persistentSequencer struct {
        db *channeldb.DB

        mu sync.Mutex

        nextID    uint64
        horizonID uint64
}

// NewPersistentSequencer initializes a new sequencer using a channeldb backend.
func NewPersistentSequencer(db *channeldb.DB) (Sequencer, error) <span class="cov0" title="0">{
        g := &amp;persistentSequencer{
                db: db,
        }

        // Ensure the database bucket is created before any updates are
        // performed.
        if err := g.initDB(); err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov0" title="0">return g, nil</span>
}

// NextID returns a unique sequence number for every invocation, persisting the
// assignment to avoid reuse.
func (s *persistentSequencer) NextID() (uint64, error) <span class="cov0" title="0">{

        // nextID will be the unique sequence number returned if no errors are
        // encountered.
        var nextID uint64

        // If our sequence batch has not been exhausted, we can allocate the
        // next identifier in the range.
        s.mu.Lock()
        defer s.mu.Unlock()

        if s.nextID &lt; s.horizonID </span><span class="cov0" title="0">{
                nextID = s.nextID
                s.nextID++

                return nextID, nil
        }</span>

        // Otherwise, our sequence batch has been exhausted. We use the last
        // known sequence number on disk to mark the beginning of the next
        // sequence batch, and allocate defaultSequenceBatchSize (1000) at a
        // time.
        //
        // NOTE: This also will happen on the first invocation after startup,
        // i.e. when nextID and horizonID are both 0. The next sequence batch to be
        // allocated will start from the last known tip on disk, which is fine
        // as we only require uniqueness of the allocated numbers.
        <span class="cov0" title="0">var nextHorizonID uint64
        if err := kvdb.Update(s.db, func(tx kvdb.RwTx) error </span><span class="cov0" title="0">{
                nextIDBkt := tx.ReadWriteBucket(nextPaymentIDKey)
                if nextIDBkt == nil </span><span class="cov0" title="0">{
                        return ErrSequencerCorrupted
                }</span>

                <span class="cov0" title="0">nextID = nextIDBkt.Sequence()
                nextHorizonID = nextID + defaultSequenceBatchSize

                // Cannot fail when used in Update.
                nextIDBkt.SetSequence(nextHorizonID)

                return nil</span>
        }, func() <span class="cov0" title="0">{
                nextHorizonID = 0
        }</span>); err != nil <span class="cov0" title="0">{
                return 0, err
        }</span>

        // Never assign index zero, to avoid collisions with the EmptyKeystone.
        <span class="cov0" title="0">if nextID == 0 </span><span class="cov0" title="0">{
                nextID++
        }</span>

        // If our batch sequence allocation succeed, update our in-memory values
        // so we can continue to allocate sequence numbers without hitting disk.
        // The nextID is incremented by one in memory so the in can be used
        // issued directly on the next invocation.
        <span class="cov0" title="0">s.nextID = nextID + 1
        s.horizonID = nextHorizonID

        return nextID, nil</span>
}

// initDB populates the bucket used to generate payment sequence numbers.
func (s *persistentSequencer) initDB() error <span class="cov0" title="0">{
        return kvdb.Update(s.db, func(tx kvdb.RwTx) error </span><span class="cov0" title="0">{
                _, err := tx.CreateTopLevelBucket(nextPaymentIDKey)
                return err
        }</span>, func() {<span class="cov0" title="0">}</span>)
}
</pre>
		
		<pre class="file" id="file18" style="display: none">package htlcswitch

import (
        "bytes"
        "context"
        "errors"
        "fmt"
        "math/rand"
        "sync"
        "sync/atomic"
        "time"

        "github.com/btcsuite/btcd/btcec/v2/ecdsa"
        "github.com/btcsuite/btcd/btcutil"
        "github.com/btcsuite/btcd/wire"
        "github.com/davecgh/go-spew/spew"
        "github.com/lightningnetwork/lnd/chainntnfs"
        "github.com/lightningnetwork/lnd/channeldb"
        "github.com/lightningnetwork/lnd/clock"
        "github.com/lightningnetwork/lnd/contractcourt"
        "github.com/lightningnetwork/lnd/fn/v2"
        "github.com/lightningnetwork/lnd/graph/db/models"
        "github.com/lightningnetwork/lnd/htlcswitch/hop"
        "github.com/lightningnetwork/lnd/kvdb"
        "github.com/lightningnetwork/lnd/lntypes"
        "github.com/lightningnetwork/lnd/lnutils"
        "github.com/lightningnetwork/lnd/lnwallet"
        "github.com/lightningnetwork/lnd/lnwallet/chainfee"
        "github.com/lightningnetwork/lnd/lnwire"
        "github.com/lightningnetwork/lnd/ticker"
)

const (
        // DefaultFwdEventInterval is the duration between attempts to flush
        // pending forwarding events to disk.
        DefaultFwdEventInterval = 15 * time.Second

        // DefaultLogInterval is the duration between attempts to log statistics
        // about forwarding events.
        DefaultLogInterval = 10 * time.Second

        // DefaultAckInterval is the duration between attempts to ack any settle
        // fails in a forwarding package.
        DefaultAckInterval = 15 * time.Second

        // DefaultMailboxDeliveryTimeout is the duration after which Adds will
        // be cancelled if they could not get added to an outgoing commitment.
        DefaultMailboxDeliveryTimeout = time.Minute
)

var (
        // ErrChannelLinkNotFound is used when channel link hasn't been found.
        ErrChannelLinkNotFound = errors.New("channel link not found")

        // ErrDuplicateAdd signals that the ADD htlc was already forwarded
        // through the switch and is locked into another commitment txn.
        ErrDuplicateAdd = errors.New("duplicate add HTLC detected")

        // ErrUnknownErrorDecryptor signals that we were unable to locate the
        // error decryptor for this payment. This is likely due to restarting
        // the daemon.
        ErrUnknownErrorDecryptor = errors.New("unknown error decryptor")

        // ErrSwitchExiting signaled when the switch has received a shutdown
        // request.
        ErrSwitchExiting = errors.New("htlcswitch shutting down")

        // ErrNoLinksFound is an error returned when we attempt to retrieve the
        // active links in the switch for a specific destination.
        ErrNoLinksFound = errors.New("no channel links found")

        // ErrUnreadableFailureMessage is returned when the failure message
        // cannot be decrypted.
        ErrUnreadableFailureMessage = errors.New("unreadable failure message")

        // ErrLocalAddFailed signals that the ADD htlc for a local payment
        // failed to be processed.
        ErrLocalAddFailed = errors.New("local add HTLC failed")

        // errFeeExposureExceeded is only surfaced to callers of SendHTLC and
        // signals that sending the HTLC would exceed the outgoing link's fee
        // exposure threshold.
        errFeeExposureExceeded = errors.New("fee exposure exceeded")

        // DefaultMaxFeeExposure is the default threshold after which we'll
        // fail payments if they increase our fee exposure. This is currently
        // set to 500m msats.
        DefaultMaxFeeExposure = lnwire.MilliSatoshi(500_000_000)
)

// plexPacket encapsulates switch packet and adds error channel to receive
// error from request handler.
type plexPacket struct {
        pkt *htlcPacket
        err chan error
}

// ChanClose represents a request which close a particular channel specified by
// its id.
type ChanClose struct {
        // CloseType is a variable which signals the type of channel closure the
        // peer should execute.
        CloseType contractcourt.ChannelCloseType

        // ChanPoint represent the id of the channel which should be closed.
        ChanPoint *wire.OutPoint

        // TargetFeePerKw is the ideal fee that was specified by the caller.
        // This value is only utilized if the closure type is CloseRegular.
        // This will be the starting offered fee when the fee negotiation
        // process for the cooperative closure transaction kicks off.
        TargetFeePerKw chainfee.SatPerKWeight

        // MaxFee is the highest fee the caller is willing to pay.
        //
        // NOTE: This field is only respected if the caller is the initiator of
        // the channel.
        MaxFee chainfee.SatPerKWeight

        // DeliveryScript is an optional delivery script to pay funds out to.
        DeliveryScript lnwire.DeliveryAddress

        // Updates is used by request creator to receive the notifications about
        // execution of the close channel request.
        Updates chan interface{}

        // Err is used by request creator to receive request execution error.
        Err chan error

        // Ctx is a context linked to the lifetime of the caller.
        Ctx context.Context //nolint:containedctx
}

// Config defines the configuration for the service. ALL elements within the
// configuration MUST be non-nil for the service to carry out its duties.
type Config struct {
        // FwdingLog is an interface that will be used by the switch to log
        // forwarding events. A forwarding event happens each time a payment
        // circuit is successfully completed. So when we forward an HTLC, and a
        // settle is eventually received.
        FwdingLog ForwardingLog

        // LocalChannelClose kicks-off the workflow to execute a cooperative or
        // forced unilateral closure of the channel initiated by a local
        // subsystem.
        LocalChannelClose func(pubKey []byte, request *ChanClose)

        // DB is the database backend that will be used to back the switch's
        // persistent circuit map.
        DB kvdb.Backend

        // FetchAllOpenChannels is a function that fetches all currently open
        // channels from the channel database.
        FetchAllOpenChannels func() ([]*channeldb.OpenChannel, error)

        // FetchAllChannels is a function that fetches all pending open, open,
        // and waiting close channels from the database.
        FetchAllChannels func() ([]*channeldb.OpenChannel, error)

        // FetchClosedChannels is a function that fetches all closed channels
        // from the channel database.
        FetchClosedChannels func(
                pendingOnly bool) ([]*channeldb.ChannelCloseSummary, error)

        // SwitchPackager provides access to the forwarding packages of all
        // active channels. This gives the switch the ability to read arbitrary
        // forwarding packages, and ack settles and fails contained within them.
        SwitchPackager channeldb.FwdOperator

        // ExtractErrorEncrypter is an interface allowing switch to reextract
        // error encrypters stored in the circuit map on restarts, since they
        // are not stored directly within the database.
        ExtractErrorEncrypter hop.ErrorEncrypterExtracter

        // FetchLastChannelUpdate retrieves the latest routing policy for a
        // target channel. This channel will typically be the outgoing channel
        // specified when we receive an incoming HTLC.  This will be used to
        // provide payment senders our latest policy when sending encrypted
        // error messages.
        FetchLastChannelUpdate func(lnwire.ShortChannelID) (
                *lnwire.ChannelUpdate1, error)

        // Notifier is an instance of a chain notifier that we'll use to signal
        // the switch when a new block has arrived.
        Notifier chainntnfs.ChainNotifier

        // HtlcNotifier is an instance of a htlcNotifier which we will pipe htlc
        // events through.
        HtlcNotifier htlcNotifier

        // FwdEventTicker is a signal that instructs the htlcswitch to flush any
        // pending forwarding events.
        FwdEventTicker ticker.Ticker

        // LogEventTicker is a signal instructing the htlcswitch to log
        // aggregate stats about it's forwarding during the last interval.
        LogEventTicker ticker.Ticker

        // AckEventTicker is a signal instructing the htlcswitch to ack any settle
        // fails in forwarding packages.
        AckEventTicker ticker.Ticker

        // AllowCircularRoute is true if the user has configured their node to
        // allow forwards that arrive and depart our node over the same channel.
        AllowCircularRoute bool

        // RejectHTLC is a flag that instructs the htlcswitch to reject any
        // HTLCs that are not from the source hop.
        RejectHTLC bool

        // Clock is a time source for the switch.
        Clock clock.Clock

        // MailboxDeliveryTimeout is the interval after which Adds will be
        // cancelled if they have not been yet been delivered to a link. The
        // computed deadline will expiry this long after the Adds are added to
        // a mailbox via AddPacket.
        MailboxDeliveryTimeout time.Duration

        // MaxFeeExposure is the threshold in milli-satoshis after which we'll
        // fail incoming or outgoing payments for a particular channel.
        MaxFeeExposure lnwire.MilliSatoshi

        // SignAliasUpdate is used when sending FailureMessages backwards for
        // option_scid_alias channels. This avoids a potential privacy leak by
        // replacing the public, confirmed SCID with the alias in the
        // ChannelUpdate.
        SignAliasUpdate func(u *lnwire.ChannelUpdate1) (*ecdsa.Signature,
                error)

        // IsAlias returns whether or not a given SCID is an alias.
        IsAlias func(scid lnwire.ShortChannelID) bool
}

// Switch is the central messaging bus for all incoming/outgoing HTLCs.
// Connected peers with active channels are treated as named interfaces which
// refer to active channels as links. A link is the switch's message
// communication point with the goroutine that manages an active channel. New
// links are registered each time a channel is created, and unregistered once
// the channel is closed. The switch manages the hand-off process for multi-hop
// HTLCs, forwarding HTLCs initiated from within the daemon, and finally
// notifies users local-systems concerning their outstanding payment requests.
type Switch struct {
        started  int32 // To be used atomically.
        shutdown int32 // To be used atomically.

        // bestHeight is the best known height of the main chain. The links will
        // be used this information to govern decisions based on HTLC timeouts.
        // This will be retrieved by the registered links atomically.
        bestHeight uint32

        wg   sync.WaitGroup
        quit chan struct{}

        // cfg is a copy of the configuration struct that the htlc switch
        // service was initialized with.
        cfg *Config

        // networkResults stores the results of payments initiated by the user.
        // The store is used to later look up the payments and notify the
        // user of the result when they are complete. Each payment attempt
        // should be given a unique integer ID when it is created, otherwise
        // results might be overwritten.
        networkResults *networkResultStore

        // circuits is storage for payment circuits which are used to
        // forward the settle/fail htlc updates back to the add htlc initiator.
        circuits CircuitMap

        // mailOrchestrator manages the lifecycle of mailboxes used throughout
        // the switch, and facilitates delayed delivery of packets to links that
        // later come online.
        mailOrchestrator *mailOrchestrator

        // indexMtx is a read/write mutex that protects the set of indexes
        // below.
        indexMtx sync.RWMutex

        // pendingLinkIndex holds links that have not had their final, live
        // short_chan_id assigned.
        pendingLinkIndex map[lnwire.ChannelID]ChannelLink

        // links is a map of channel id and channel link which manages
        // this channel.
        linkIndex map[lnwire.ChannelID]ChannelLink

        // forwardingIndex is an index which is consulted by the switch when it
        // needs to locate the next hop to forward an incoming/outgoing HTLC
        // update to/from.
        //
        // TODO(roasbeef): eventually add a NetworkHop mapping before the
        // ChannelLink
        forwardingIndex map[lnwire.ShortChannelID]ChannelLink

        // interfaceIndex maps the compressed public key of a peer to all the
        // channels that the switch maintains with that peer.
        interfaceIndex map[[33]byte]map[lnwire.ChannelID]ChannelLink

        // linkStopIndex stores the currently stopping ChannelLinks,
        // represented by their ChannelID. The key is the link's ChannelID and
        // the value is a chan that is closed when the link has fully stopped.
        // This map is only added to if RemoveLink is called and is not added
        // to when the Switch is shutting down and calls Stop() on each link.
        //
        // MUST be used with the indexMtx.
        linkStopIndex map[lnwire.ChannelID]chan struct{}

        // htlcPlex is the channel which all connected links use to coordinate
        // the setup/teardown of Sphinx (onion routing) payment circuits.
        // Active links forward any add/settle messages over this channel each
        // state transition, sending new adds/settles which are fully locked
        // in.
        htlcPlex chan *plexPacket

        // chanCloseRequests is used to transfer the channel close request to
        // the channel close handler.
        chanCloseRequests chan *ChanClose

        // resolutionMsgs is the channel that all external contract resolution
        // messages will be sent over.
        resolutionMsgs chan *resolutionMsg

        // pendingFwdingEvents is the set of forwarding events which have been
        // collected during the current interval, but hasn't yet been written
        // to the forwarding log.
        fwdEventMtx         sync.Mutex
        pendingFwdingEvents []channeldb.ForwardingEvent

        // blockEpochStream is an active block epoch event stream backed by an
        // active ChainNotifier instance. This will be used to retrieve the
        // latest height of the chain.
        blockEpochStream *chainntnfs.BlockEpochEvent

        // pendingSettleFails is the set of settle/fail entries that we need to
        // ack in the forwarding package of the outgoing link. This was added to
        // make pipelining settles more efficient.
        pendingSettleFails []channeldb.SettleFailRef

        // resMsgStore is used to store the set of ResolutionMsg that come from
        // contractcourt. This is used so the Switch can properly forward them,
        // even on restarts.
        resMsgStore *resolutionStore

        // aliasToReal is a map used for option-scid-alias feature-bit links.
        // The alias SCID is the key and the real, confirmed SCID is the value.
        // If the channel is unconfirmed, there will not be a mapping for it.
        // Since channels can have multiple aliases, this map is essentially a
        // N-&gt;1 mapping for a channel. This MUST be accessed with the indexMtx.
        aliasToReal map[lnwire.ShortChannelID]lnwire.ShortChannelID

        // baseIndex is a map used for option-scid-alias feature-bit links.
        // The value is the SCID of the link's ShortChannelID. This value may
        // be an alias for zero-conf channels or a confirmed SCID for
        // non-zero-conf channels with the option-scid-alias feature-bit. The
        // key includes the value itself and also any other aliases. This MUST
        // be accessed with the indexMtx.
        baseIndex map[lnwire.ShortChannelID]lnwire.ShortChannelID
}

// New creates the new instance of htlc switch.
func New(cfg Config, currentHeight uint32) (*Switch, error) <span class="cov8" title="1">{
        resStore := newResolutionStore(cfg.DB)

        circuitMap, err := NewCircuitMap(&amp;CircuitMapConfig{
                DB:                    cfg.DB,
                FetchAllOpenChannels:  cfg.FetchAllOpenChannels,
                FetchClosedChannels:   cfg.FetchClosedChannels,
                ExtractErrorEncrypter: cfg.ExtractErrorEncrypter,
                CheckResolutionMsg:    resStore.checkResolutionMsg,
        })
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">s := &amp;Switch{
                bestHeight:        currentHeight,
                cfg:               &amp;cfg,
                circuits:          circuitMap,
                linkIndex:         make(map[lnwire.ChannelID]ChannelLink),
                forwardingIndex:   make(map[lnwire.ShortChannelID]ChannelLink),
                interfaceIndex:    make(map[[33]byte]map[lnwire.ChannelID]ChannelLink),
                pendingLinkIndex:  make(map[lnwire.ChannelID]ChannelLink),
                linkStopIndex:     make(map[lnwire.ChannelID]chan struct{}),
                networkResults:    newNetworkResultStore(cfg.DB),
                htlcPlex:          make(chan *plexPacket),
                chanCloseRequests: make(chan *ChanClose),
                resolutionMsgs:    make(chan *resolutionMsg),
                resMsgStore:       resStore,
                quit:              make(chan struct{}),
        }

        s.aliasToReal = make(map[lnwire.ShortChannelID]lnwire.ShortChannelID)
        s.baseIndex = make(map[lnwire.ShortChannelID]lnwire.ShortChannelID)

        s.mailOrchestrator = newMailOrchestrator(&amp;mailOrchConfig{
                forwardPackets:    s.ForwardPackets,
                clock:             s.cfg.Clock,
                expiry:            s.cfg.MailboxDeliveryTimeout,
                failMailboxUpdate: s.failMailboxUpdate,
        })

        return s, nil</span>
}

// resolutionMsg is a struct that wraps an existing ResolutionMsg with a done
// channel. We'll use this channel to synchronize delivery of the message with
// the caller.
type resolutionMsg struct {
        contractcourt.ResolutionMsg

        errChan chan error
}

// ProcessContractResolution is called by active contract resolvers once a
// contract they are watching over has been fully resolved. The message carries
// an external signal that *would* have been sent if the outgoing channel
// didn't need to go to the chain in order to fulfill a contract. We'll process
// this message just as if it came from an active outgoing channel.
func (s *Switch) ProcessContractResolution(msg contractcourt.ResolutionMsg) error <span class="cov8" title="1">{
        errChan := make(chan error, 1)

        select </span>{
        case s.resolutionMsgs &lt;- &amp;resolutionMsg{
                ResolutionMsg: msg,
                errChan:       errChan,
        }:<span class="cov8" title="1"></span>
        case &lt;-s.quit:<span class="cov0" title="0">
                return ErrSwitchExiting</span>
        }

        <span class="cov8" title="1">select </span>{
        case err := &lt;-errChan:<span class="cov8" title="1">
                return err</span>
        case &lt;-s.quit:<span class="cov0" title="0">
                return ErrSwitchExiting</span>
        }
}

// HasAttemptResult reads the network result store to fetch the specified
// attempt. Returns true if the attempt result exists.
func (s *Switch) HasAttemptResult(attemptID uint64) (bool, error) <span class="cov0" title="0">{
        _, err := s.networkResults.getResult(attemptID)
        if err == nil </span><span class="cov0" title="0">{
                return true, nil
        }</span>

        <span class="cov0" title="0">if !errors.Is(err, ErrPaymentIDNotFound) </span><span class="cov0" title="0">{
                return false, err
        }</span>

        <span class="cov0" title="0">return false, nil</span>
}

// GetAttemptResult returns the result of the HTLC attempt with the given
// attemptID. The paymentHash should be set to the payment's overall hash, or
// in case of AMP payments the payment's unique identifier.
//
// The method returns a channel where the HTLC attempt result will be sent when
// available, or an error is encountered during forwarding. When a result is
// received on the channel, the HTLC is guaranteed to no longer be in flight.
// The switch shutting down is signaled by closing the channel. If the
// attemptID is unknown, ErrPaymentIDNotFound will be returned.
func (s *Switch) GetAttemptResult(attemptID uint64, paymentHash lntypes.Hash,
        deobfuscator ErrorDecrypter) (&lt;-chan *PaymentResult, error) <span class="cov8" title="1">{

        var (
                nChan &lt;-chan *networkResult
                err   error
                inKey = CircuitKey{
                        ChanID: hop.Source,
                        HtlcID: attemptID,
                }
        )

        // If the HTLC is not found in the circuit map, check whether a result
        // is already available.
        // Assumption: no one will add this attempt ID other than the caller.
        if s.circuits.LookupCircuit(inKey) == nil </span><span class="cov8" title="1">{
                res, err := s.networkResults.getResult(attemptID)
                if err != nil </span><span class="cov8" title="1">{
                        return nil, err
                }</span>
                <span class="cov8" title="1">c := make(chan *networkResult, 1)
                c &lt;- res
                nChan = c</span>
        } else<span class="cov8" title="1"> {
                // The HTLC was committed to the circuits, subscribe for a
                // result.
                nChan, err = s.networkResults.subscribeResult(attemptID)
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>
        }

        <span class="cov8" title="1">resultChan := make(chan *PaymentResult, 1)

        // Since the attempt was known, we can start a goroutine that can
        // extract the result when it is available, and pass it on to the
        // caller.
        s.wg.Add(1)
        go func() </span><span class="cov8" title="1">{
                defer s.wg.Done()

                var n *networkResult
                select </span>{
                case n = &lt;-nChan:<span class="cov8" title="1"></span>
                case &lt;-s.quit:<span class="cov8" title="1">
                        // We close the result channel to signal a shutdown. We
                        // don't send any result in this case since the HTLC is
                        // still in flight.
                        close(resultChan)
                        return</span>
                }

                <span class="cov8" title="1">log.Debugf("Received network result %T for attemptID=%v", n.msg,
                        attemptID)

                // Extract the result and pass it to the result channel.
                result, err := s.extractResult(
                        deobfuscator, n, attemptID, paymentHash,
                )
                if err != nil </span><span class="cov0" title="0">{
                        e := fmt.Errorf("unable to extract result: %w", err)
                        log.Error(e)
                        resultChan &lt;- &amp;PaymentResult{
                                Error: e,
                        }
                        return
                }</span>
                <span class="cov8" title="1">resultChan &lt;- result</span>
        }()

        <span class="cov8" title="1">return resultChan, nil</span>
}

// CleanStore calls the underlying result store, telling it is safe to delete
// all entries except the ones in the keepPids map. This should be called
// preiodically to let the switch clean up payment results that we have
// handled.
func (s *Switch) CleanStore(keepPids map[uint64]struct{}) error <span class="cov0" title="0">{
        return s.networkResults.cleanStore(keepPids)
}</span>

// SendHTLC is used by other subsystems which aren't belong to htlc switch
// package in order to send the htlc update. The attemptID used MUST be unique
// for this HTLC, and MUST be used only once, otherwise the switch might reject
// it.
func (s *Switch) SendHTLC(firstHop lnwire.ShortChannelID, attemptID uint64,
        htlc *lnwire.UpdateAddHTLC) error <span class="cov8" title="1">{

        // Generate and send new update packet, if error will be received on
        // this stage it means that packet haven't left boundaries of our
        // system and something wrong happened.
        packet := &amp;htlcPacket{
                incomingChanID: hop.Source,
                incomingHTLCID: attemptID,
                outgoingChanID: firstHop,
                htlc:           htlc,
                amount:         htlc.Amount,
        }

        // Attempt to fetch the target link before creating a circuit so that
        // we don't leave dangling circuits. The getLocalLink method does not
        // require the circuit variable to be set on the *htlcPacket.
        link, linkErr := s.getLocalLink(packet, htlc)
        if linkErr != nil </span><span class="cov8" title="1">{
                // Notify the htlc notifier of a link failure on our outgoing
                // link. Incoming timelock/amount values are not set because
                // they are not present for local sends.
                s.cfg.HtlcNotifier.NotifyLinkFailEvent(
                        newHtlcKey(packet),
                        HtlcInfo{
                                OutgoingTimeLock: htlc.Expiry,
                                OutgoingAmt:      htlc.Amount,
                        },
                        HtlcEventTypeSend,
                        linkErr,
                        false,
                )

                return linkErr
        }</span>

        // Evaluate whether this HTLC would bypass our fee exposure. If it
        // does, don't send it out and instead return an error.
        <span class="cov8" title="1">if s.dustExceedsFeeThreshold(link, htlc.Amount, false) </span><span class="cov8" title="1">{
                // Notify the htlc notifier of a link failure on our outgoing
                // link. We use the FailTemporaryChannelFailure in place of a
                // more descriptive error message.
                linkErr := NewLinkError(
                        &amp;lnwire.FailTemporaryChannelFailure{},
                )
                s.cfg.HtlcNotifier.NotifyLinkFailEvent(
                        newHtlcKey(packet),
                        HtlcInfo{
                                OutgoingTimeLock: htlc.Expiry,
                                OutgoingAmt:      htlc.Amount,
                        },
                        HtlcEventTypeSend,
                        linkErr,
                        false,
                )

                return errFeeExposureExceeded
        }</span>

        <span class="cov8" title="1">circuit := newPaymentCircuit(&amp;htlc.PaymentHash, packet)
        actions, err := s.circuits.CommitCircuits(circuit)
        if err != nil </span><span class="cov0" title="0">{
                log.Errorf("unable to commit circuit in switch: %v", err)
                return err
        }</span>

        // Drop duplicate packet if it has already been seen.
        <span class="cov8" title="1">switch </span>{
        case len(actions.Drops) == 1:<span class="cov8" title="1">
                return ErrDuplicateAdd</span>

        case len(actions.Fails) == 1:<span class="cov0" title="0">
                return ErrLocalAddFailed</span>
        }

        // Give the packet to the link's mailbox so that HTLC's are properly
        // canceled back if the mailbox timeout elapses.
        <span class="cov8" title="1">packet.circuit = circuit

        return link.handleSwitchPacket(packet)</span>
}

// UpdateForwardingPolicies sends a message to the switch to update the
// forwarding policies for the set of target channels, keyed in chanPolicies.
//
// NOTE: This function is synchronous and will block until either the
// forwarding policies for all links have been updated, or the switch shuts
// down.
func (s *Switch) UpdateForwardingPolicies(
        chanPolicies map[wire.OutPoint]models.ForwardingPolicy) <span class="cov0" title="0">{

        log.Tracef("Updating link policies: %v", lnutils.SpewLogClosure(
                chanPolicies))

        s.indexMtx.RLock()

        // Update each link in chanPolicies.
        for targetLink, policy := range chanPolicies </span><span class="cov0" title="0">{
                cid := lnwire.NewChanIDFromOutPoint(targetLink)

                link, ok := s.linkIndex[cid]
                if !ok </span><span class="cov0" title="0">{
                        log.Debugf("Unable to find ChannelPoint(%v) to update "+
                                "link policy", targetLink)
                        continue</span>
                }

                <span class="cov0" title="0">link.UpdateForwardingPolicy(policy)</span>
        }

        <span class="cov0" title="0">s.indexMtx.RUnlock()</span>
}

// IsForwardedHTLC checks for a given channel and htlc index if it is related
// to an opened circuit that represents a forwarded payment.
func (s *Switch) IsForwardedHTLC(chanID lnwire.ShortChannelID,
        htlcIndex uint64) bool <span class="cov8" title="1">{

        circuit := s.circuits.LookupOpenCircuit(models.CircuitKey{
                ChanID: chanID,
                HtlcID: htlcIndex,
        })
        return circuit != nil &amp;&amp; circuit.Incoming.ChanID != hop.Source
}</span>

// ForwardPackets adds a list of packets to the switch for processing. Fails
// and settles are added on a first past, simultaneously constructing circuits
// for any adds. After persisting the circuits, another pass of the adds is
// given to forward them through the router. The sending link's quit channel is
// used to prevent deadlocks when the switch stops a link in the midst of
// forwarding.
func (s *Switch) ForwardPackets(linkQuit &lt;-chan struct{},
        packets ...*htlcPacket) error <span class="cov8" title="1">{

        var (
                // fwdChan is a buffered channel used to receive err msgs from
                // the htlcPlex when forwarding this batch.
                fwdChan = make(chan error, len(packets))

                // numSent keeps a running count of how many packets are
                // forwarded to the switch, which determines how many responses
                // we will wait for on the fwdChan..
                numSent int
        )

        // No packets, nothing to do.
        if len(packets) == 0 </span><span class="cov8" title="1">{
                return nil
        }</span>

        // Setup a barrier to prevent the background tasks from processing
        // responses until this function returns to the user.
        <span class="cov8" title="1">var wg sync.WaitGroup
        wg.Add(1)
        defer wg.Done()

        // Before spawning the following goroutine to proxy our error responses,
        // check to see if we have already been issued a shutdown request. If
        // so, we exit early to avoid incrementing the switch's waitgroup while
        // it is already in the process of shutting down.
        select </span>{
        case &lt;-linkQuit:<span class="cov8" title="1">
                return nil</span>
        case &lt;-s.quit:<span class="cov8" title="1">
                return nil</span>
        default:<span class="cov8" title="1">
                // Spawn a goroutine to log the errors returned from failed packets.
                s.wg.Add(1)
                go s.logFwdErrs(&amp;numSent, &amp;wg, fwdChan)</span>
        }

        // Make a first pass over the packets, forwarding any settles or fails.
        // As adds are found, we create a circuit and append it to our set of
        // circuits to be written to disk.
        <span class="cov8" title="1">var circuits []*PaymentCircuit
        var addBatch []*htlcPacket
        for _, packet := range packets </span><span class="cov8" title="1">{
                switch htlc := packet.htlc.(type) </span>{
                case *lnwire.UpdateAddHTLC:<span class="cov8" title="1">
                        circuit := newPaymentCircuit(&amp;htlc.PaymentHash, packet)
                        packet.circuit = circuit
                        circuits = append(circuits, circuit)
                        addBatch = append(addBatch, packet)</span>
                default:<span class="cov8" title="1">
                        err := s.routeAsync(packet, fwdChan, linkQuit)
                        if err != nil </span><span class="cov8" title="1">{
                                return fmt.Errorf("failed to forward packet %w",
                                        err)
                        }</span>
                        <span class="cov8" title="1">numSent++</span>
                }
        }

        // If this batch did not contain any circuits to commit, we can return
        // early.
        <span class="cov8" title="1">if len(circuits) == 0 </span><span class="cov8" title="1">{
                return nil
        }</span>

        // Write any circuits that we found to disk.
        <span class="cov8" title="1">actions, err := s.circuits.CommitCircuits(circuits...)
        if err != nil </span><span class="cov0" title="0">{
                log.Errorf("unable to commit circuits in switch: %v", err)
        }</span>

        // Split the htlc packets by comparing an in-order seek to the head of
        // the added, dropped, or failed circuits.
        //
        // NOTE: This assumes each list is guaranteed to be a subsequence of the
        // circuits, and that the union of the sets results in the original set
        // of circuits.
        <span class="cov8" title="1">var addedPackets, failedPackets []*htlcPacket
        for _, packet := range addBatch </span><span class="cov8" title="1">{
                switch </span>{
                case len(actions.Adds) &gt; 0 &amp;&amp; packet.circuit == actions.Adds[0]:<span class="cov8" title="1">
                        addedPackets = append(addedPackets, packet)
                        actions.Adds = actions.Adds[1:]</span>

                case len(actions.Drops) &gt; 0 &amp;&amp; packet.circuit == actions.Drops[0]:<span class="cov8" title="1">
                        actions.Drops = actions.Drops[1:]</span>

                case len(actions.Fails) &gt; 0 &amp;&amp; packet.circuit == actions.Fails[0]:<span class="cov8" title="1">
                        failedPackets = append(failedPackets, packet)
                        actions.Fails = actions.Fails[1:]</span>
                }
        }

        // Now, forward any packets for circuits that were successfully added to
        // the switch's circuit map.
        <span class="cov8" title="1">for _, packet := range addedPackets </span><span class="cov8" title="1">{
                err := s.routeAsync(packet, fwdChan, linkQuit)
                if err != nil </span><span class="cov8" title="1">{
                        return fmt.Errorf("failed to forward packet %w", err)
                }</span>
                <span class="cov8" title="1">numSent++</span>
        }

        // Lastly, for any packets that failed, this implies that they were
        // left in a half added state, which can happen when recovering from
        // failures.
        <span class="cov8" title="1">if len(failedPackets) &gt; 0 </span><span class="cov8" title="1">{
                var failure lnwire.FailureMessage
                incomingID := failedPackets[0].incomingChanID

                // If the incoming channel is an option_scid_alias channel,
                // then we'll need to replace the SCID in the ChannelUpdate.
                update := s.failAliasUpdate(incomingID, true)
                if update == nil </span><span class="cov8" title="1">{
                        // Fallback to the original non-option behavior.
                        update, err := s.cfg.FetchLastChannelUpdate(
                                incomingID,
                        )
                        if err != nil </span><span class="cov0" title="0">{
                                failure = &amp;lnwire.FailTemporaryNodeFailure{}
                        }</span> else<span class="cov8" title="1"> {
                                failure = lnwire.NewTemporaryChannelFailure(
                                        update,
                                )
                        }</span>
                } else<span class="cov8" title="1"> {
                        // This is an option_scid_alias channel.
                        failure = lnwire.NewTemporaryChannelFailure(update)
                }</span>

                <span class="cov8" title="1">linkError := NewDetailedLinkError(
                        failure, OutgoingFailureIncompleteForward,
                )

                for _, packet := range failedPackets </span><span class="cov8" title="1">{
                        // We don't handle the error here since this method
                        // always returns an error.
                        _ = s.failAddPacket(packet, linkError)
                }</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// logFwdErrs logs any errors received on `fwdChan`.
func (s *Switch) logFwdErrs(num *int, wg *sync.WaitGroup, fwdChan chan error) <span class="cov8" title="1">{
        defer s.wg.Done()

        // Wait here until the outer function has finished persisting
        // and routing the packets. This guarantees we don't read from num until
        // the value is accurate.
        wg.Wait()

        numSent := *num
        for i := 0; i &lt; numSent; i++ </span><span class="cov8" title="1">{
                select </span>{
                case err := &lt;-fwdChan:<span class="cov8" title="1">
                        if err != nil </span><span class="cov8" title="1">{
                                log.Errorf("Unhandled error while reforwarding htlc "+
                                        "settle/fail over htlcswitch: %v", err)
                        }</span>
                case &lt;-s.quit:<span class="cov8" title="1">
                        log.Errorf("unable to forward htlc packet " +
                                "htlc switch was stopped")
                        return</span>
                }
        }
}

// routeAsync sends a packet through the htlc switch, using the provided err
// chan to propagate errors back to the caller. The link's quit channel is
// provided so that the send can be canceled if either the link or the switch
// receive a shutdown requuest. This method does not wait for a response from
// the htlcForwarder before returning.
func (s *Switch) routeAsync(packet *htlcPacket, errChan chan error,
        linkQuit &lt;-chan struct{}) error <span class="cov8" title="1">{

        command := &amp;plexPacket{
                pkt: packet,
                err: errChan,
        }

        select </span>{
        case s.htlcPlex &lt;- command:<span class="cov8" title="1">
                return nil</span>
        case &lt;-linkQuit:<span class="cov8" title="1">
                return ErrLinkShuttingDown</span>
        case &lt;-s.quit:<span class="cov0" title="0">
                return errors.New("htlc switch was stopped")</span>
        }
}

// getLocalLink handles the addition of a htlc for a send that originates from
// our node. It returns the link that the htlc should be forwarded outwards on,
// and a link error if the htlc cannot be forwarded.
func (s *Switch) getLocalLink(pkt *htlcPacket, htlc *lnwire.UpdateAddHTLC) (
        ChannelLink, *LinkError) <span class="cov8" title="1">{

        // Try to find links by node destination.
        s.indexMtx.RLock()
        link, err := s.getLinkByShortID(pkt.outgoingChanID)
        defer s.indexMtx.RUnlock()
        if err != nil </span><span class="cov8" title="1">{
                // If the link was not found for the outgoingChanID, an outside
                // subsystem may be using the confirmed SCID of a zero-conf
                // channel. In this case, we'll consult the Switch maps to see
                // if an alias exists and use the alias to lookup the link.
                // This extra step is a consequence of not updating the Switch
                // forwardingIndex when a zero-conf channel is confirmed. We
                // don't need to change the outgoingChanID since the link will
                // do that upon receiving the packet.
                baseScid, ok := s.baseIndex[pkt.outgoingChanID]
                if !ok </span><span class="cov0" title="0">{
                        log.Errorf("Link %v not found", pkt.outgoingChanID)
                        return nil, NewLinkError(&amp;lnwire.FailUnknownNextPeer{})
                }</span>

                // The base SCID was found, so we'll use that to fetch the
                // link.
                <span class="cov8" title="1">link, err = s.getLinkByShortID(baseScid)
                if err != nil </span><span class="cov0" title="0">{
                        log.Errorf("Link %v not found", baseScid)
                        return nil, NewLinkError(&amp;lnwire.FailUnknownNextPeer{})
                }</span>
        }

        <span class="cov8" title="1">if !link.EligibleToForward() </span><span class="cov8" title="1">{
                log.Errorf("Link %v is not available to forward",
                        pkt.outgoingChanID)

                // The update does not need to be populated as the error
                // will be returned back to the router.
                return nil, NewDetailedLinkError(
                        lnwire.NewTemporaryChannelFailure(nil),
                        OutgoingFailureLinkNotEligible,
                )
        }</span>

        // Ensure that the htlc satisfies the outgoing channel policy.
        <span class="cov8" title="1">currentHeight := atomic.LoadUint32(&amp;s.bestHeight)
        htlcErr := link.CheckHtlcTransit(
                htlc.PaymentHash, htlc.Amount, htlc.Expiry, currentHeight,
                htlc.CustomRecords,
        )
        if htlcErr != nil </span><span class="cov8" title="1">{
                log.Errorf("Link %v policy for local forward not "+
                        "satisfied", pkt.outgoingChanID)
                return nil, htlcErr
        }</span>
        <span class="cov8" title="1">return link, nil</span>
}

// handleLocalResponse processes a Settle or Fail responding to a
// locally-initiated payment. This is handled asynchronously to avoid blocking
// the main event loop within the switch, as these operations can require
// multiple db transactions. The guarantees of the circuit map are stringent
// enough such that we are able to tolerate reordering of these operations
// without side effects. The primary operations handled are:
//  1. Save the payment result to the pending payment store.
//  2. Notify subscribers about the payment result.
//  3. Ack settle/fail references, to avoid resending this response internally
//  4. Teardown the closing circuit in the circuit map
//
// NOTE: This method MUST be spawned as a goroutine.
func (s *Switch) handleLocalResponse(pkt *htlcPacket) <span class="cov8" title="1">{
        defer s.wg.Done()

        attemptID := pkt.incomingHTLCID

        // The error reason will be unencypted in case this a local
        // failure or a converted error.
        unencrypted := pkt.localFailure || pkt.convertedError
        n := &amp;networkResult{
                msg:          pkt.htlc,
                unencrypted:  unencrypted,
                isResolution: pkt.isResolution,
        }

        // Store the result to the db. This will also notify subscribers about
        // the result.
        if err := s.networkResults.storeResult(attemptID, n); err != nil </span><span class="cov0" title="0">{
                log.Errorf("Unable to store attempt result for pid=%v: %v",
                        attemptID, err)
                return
        }</span>

        // First, we'll clean up any fwdpkg references, circuit entries, and
        // mark in our db that the payment for this payment hash has either
        // succeeded or failed.
        //
        // If this response is contained in a forwarding package, we'll start by
        // acking the settle/fail so that we don't continue to retransmit the
        // HTLC internally.
        <span class="cov8" title="1">if pkt.destRef != nil </span><span class="cov8" title="1">{
                if err := s.ackSettleFail(*pkt.destRef); err != nil </span><span class="cov0" title="0">{
                        log.Warnf("Unable to ack settle/fail reference: %s: %v",
                                *pkt.destRef, err)
                        return
                }</span>
        }

        // Next, we'll remove the circuit since we are about to complete an
        // fulfill/fail of this HTLC. Since we've already removed the
        // settle/fail fwdpkg reference, the response from the peer cannot be
        // replayed internally if this step fails. If this happens, this logic
        // will be executed when a provided resolution message comes through.
        // This can only happen if the circuit is still open, which is why this
        // ordering is chosen.
        <span class="cov8" title="1">if err := s.teardownCircuit(pkt); err != nil </span><span class="cov0" title="0">{
                log.Errorf("Unable to teardown circuit %s: %v",
                        pkt.inKey(), err)
                return
        }</span>

        // Finally, notify on the htlc failure or success that has been handled.
        <span class="cov8" title="1">key := newHtlcKey(pkt)
        eventType := getEventType(pkt)

        switch htlc := pkt.htlc.(type) </span>{
        case *lnwire.UpdateFulfillHTLC:<span class="cov8" title="1">
                s.cfg.HtlcNotifier.NotifySettleEvent(key, htlc.PaymentPreimage,
                        eventType)</span>

        case *lnwire.UpdateFailHTLC:<span class="cov8" title="1">
                s.cfg.HtlcNotifier.NotifyForwardingFailEvent(key, eventType)</span>
        }
}

// extractResult uses the given deobfuscator to extract the payment result from
// the given network message.
func (s *Switch) extractResult(deobfuscator ErrorDecrypter, n *networkResult,
        attemptID uint64, paymentHash lntypes.Hash) (*PaymentResult, error) <span class="cov8" title="1">{

        switch htlc := n.msg.(type) </span>{

        // We've received a settle update which means we can finalize the user
        // payment and return successful response.
        case *lnwire.UpdateFulfillHTLC:<span class="cov8" title="1">
                return &amp;PaymentResult{
                        Preimage: htlc.PaymentPreimage,
                }, nil</span>

        // We've received a fail update which means we can finalize the
        // user payment and return fail response.
        case *lnwire.UpdateFailHTLC:<span class="cov8" title="1">
                // TODO(yy): construct deobfuscator here to avoid creating it
                // in paymentLifecycle even for settled HTLCs.
                paymentErr := s.parseFailedPayment(
                        deobfuscator, attemptID, paymentHash, n.unencrypted,
                        n.isResolution, htlc,
                )

                return &amp;PaymentResult{
                        Error: paymentErr,
                }, nil</span>

        default:<span class="cov0" title="0">
                return nil, fmt.Errorf("received unknown response type: %T",
                        htlc)</span>
        }
}

// parseFailedPayment determines the appropriate failure message to return to
// a user initiated payment. The three cases handled are:
//  1. An unencrypted failure, which should already plaintext.
//  2. A resolution from the chain arbitrator, which possibly has no failure
//     reason attached.
//  3. A failure from the remote party, which will need to be decrypted using
//     the payment deobfuscator.
func (s *Switch) parseFailedPayment(deobfuscator ErrorDecrypter,
        attemptID uint64, paymentHash lntypes.Hash, unencrypted,
        isResolution bool, htlc *lnwire.UpdateFailHTLC) error <span class="cov8" title="1">{

        switch </span>{

        // The payment never cleared the link, so we don't need to
        // decrypt the error, simply decode it them report back to the
        // user.
        case unencrypted:<span class="cov8" title="1">
                r := bytes.NewReader(htlc.Reason)
                failureMsg, err := lnwire.DecodeFailure(r, 0)
                if err != nil </span><span class="cov0" title="0">{
                        // If we could not decode the failure reason, return a link
                        // error indicating that we failed to decode the onion.
                        linkError := NewDetailedLinkError(
                                // As this didn't even clear the link, we don't
                                // need to apply an update here since it goes
                                // directly to the router.
                                lnwire.NewTemporaryChannelFailure(nil),
                                OutgoingFailureDecodeError,
                        )

                        log.Errorf("%v: (hash=%v, pid=%d): %v",
                                linkError.FailureDetail.FailureString(),
                                paymentHash, attemptID, err)

                        return linkError
                }</span>

                // If we successfully decoded the failure reason, return it.
                <span class="cov8" title="1">return NewLinkError(failureMsg)</span>

        // A payment had to be timed out on chain before it got past
        // the first hop. In this case, we'll report a permanent
        // channel failure as this means us, or the remote party had to
        // go on chain.
        case isResolution &amp;&amp; htlc.Reason == nil:<span class="cov0" title="0">
                linkError := NewDetailedLinkError(
                        &amp;lnwire.FailPermanentChannelFailure{},
                        OutgoingFailureOnChainTimeout,
                )

                log.Infof("%v: hash=%v, pid=%d",
                        linkError.FailureDetail.FailureString(),
                        paymentHash, attemptID)

                return linkError</span>

        // A regular multi-hop payment error that we'll need to
        // decrypt.
        default:<span class="cov8" title="1">
                // We'll attempt to fully decrypt the onion encrypted
                // error. If we're unable to then we'll bail early.
                failure, err := deobfuscator.DecryptError(htlc.Reason)
                if err != nil </span><span class="cov8" title="1">{
                        log.Errorf("unable to de-obfuscate onion failure "+
                                "(hash=%v, pid=%d): %v",
                                paymentHash, attemptID, err)

                        return ErrUnreadableFailureMessage
                }</span>

                <span class="cov8" title="1">return failure</span>
        }
}

// handlePacketForward is used in cases when we need forward the htlc update
// from one channel link to another and be able to propagate the settle/fail
// updates back. This behaviour is achieved by creation of payment circuits.
func (s *Switch) handlePacketForward(packet *htlcPacket) error <span class="cov8" title="1">{
        switch htlc := packet.htlc.(type) </span>{
        // Channel link forwarded us a new htlc, therefore we initiate the
        // payment circuit within our internal state so we can properly forward
        // the ultimate settle message back latter.
        case *lnwire.UpdateAddHTLC:<span class="cov8" title="1">
                return s.handlePacketAdd(packet, htlc)</span>

        case *lnwire.UpdateFulfillHTLC:<span class="cov8" title="1">
                return s.handlePacketSettle(packet)</span>

        // Channel link forwarded us an update_fail_htlc message.
        //
        // NOTE: when the channel link receives an update_fail_malformed_htlc
        // from upstream, it will convert the message into update_fail_htlc and
        // forward it. Thus there's no need to catch `UpdateFailMalformedHTLC`
        // here.
        case *lnwire.UpdateFailHTLC:<span class="cov8" title="1">
                return s.handlePacketFail(packet, htlc)</span>

        default:<span class="cov0" title="0">
                return fmt.Errorf("wrong update type: %T", htlc)</span>
        }
}

// checkCircularForward checks whether a forward is circular (arrives and
// departs on the same link) and returns a link error if the switch is
// configured to disallow this behaviour.
func (s *Switch) checkCircularForward(incoming, outgoing lnwire.ShortChannelID,
        allowCircular bool, paymentHash lntypes.Hash) *LinkError <span class="cov8" title="1">{

        // If they are equal, we can skip the alias mapping checks.
        if incoming == outgoing </span><span class="cov8" title="1">{
                // The switch may be configured to allow circular routes, so
                // just log and return nil.
                if allowCircular </span><span class="cov8" title="1">{
                        log.Debugf("allowing circular route over link: %v "+
                                "(payment hash: %x)", incoming, paymentHash)
                        return nil
                }</span>

                // Otherwise, we'll return a temporary channel failure.
                <span class="cov8" title="1">return NewDetailedLinkError(
                        lnwire.NewTemporaryChannelFailure(nil),
                        OutgoingFailureCircularRoute,
                )</span>
        }

        // We'll fetch the "base" SCID from the baseIndex for the incoming and
        // outgoing SCIDs. If either one does not have a base SCID, then the
        // two channels are not equal since one will be a channel that does not
        // need a mapping and SCID equality was checked above. If the "base"
        // SCIDs are equal, then this is a circular route. Otherwise, it isn't.
        <span class="cov8" title="1">s.indexMtx.RLock()
        incomingBaseScid, ok := s.baseIndex[incoming]
        if !ok </span><span class="cov8" title="1">{
                // This channel does not use baseIndex, bail out.
                s.indexMtx.RUnlock()
                return nil
        }</span>

        <span class="cov8" title="1">outgoingBaseScid, ok := s.baseIndex[outgoing]
        if !ok </span><span class="cov8" title="1">{
                // This channel does not use baseIndex, bail out.
                s.indexMtx.RUnlock()
                return nil
        }</span>
        <span class="cov8" title="1">s.indexMtx.RUnlock()

        // Check base SCID equality.
        if incomingBaseScid != outgoingBaseScid </span><span class="cov0" title="0">{
                // The base SCIDs are not equal so these are not the same
                // channel.
                return nil
        }</span>

        // If the incoming and outgoing link are equal, the htlc is part of a
        // circular route which may be used to lock up our liquidity. If the
        // switch is configured to allow circular routes, log that we are
        // allowing the route then return nil.
        <span class="cov8" title="1">if allowCircular </span><span class="cov8" title="1">{
                log.Debugf("allowing circular route over link: %v "+
                        "(payment hash: %x)", incoming, paymentHash)
                return nil
        }</span>

        // If our node disallows circular routes, return a temporary channel
        // failure. There is nothing wrong with the policy used by the remote
        // node, so we do not include a channel update.
        <span class="cov8" title="1">return NewDetailedLinkError(
                lnwire.NewTemporaryChannelFailure(nil),
                OutgoingFailureCircularRoute,
        )</span>
}

// failAddPacket encrypts a fail packet back to an add packet's source.
// The ciphertext will be derived from the failure message proivded by context.
// This method returns the failErr if all other steps complete successfully.
func (s *Switch) failAddPacket(packet *htlcPacket, failure *LinkError) error <span class="cov8" title="1">{
        // Encrypt the failure so that the sender will be able to read the error
        // message. Since we failed this packet, we use EncryptFirstHop to
        // obfuscate the failure for their eyes only.
        reason, err := packet.obfuscator.EncryptFirstHop(failure.WireMessage())
        if err != nil </span><span class="cov0" title="0">{
                err := fmt.Errorf("unable to obfuscate "+
                        "error: %v", err)
                log.Error(err)
                return err
        }</span>

        <span class="cov8" title="1">log.Error(failure.Error())

        // Create a failure packet for this htlc. The full set of
        // information about the htlc failure is included so that they can
        // be included in link failure notifications.
        failPkt := &amp;htlcPacket{
                sourceRef:       packet.sourceRef,
                incomingChanID:  packet.incomingChanID,
                incomingHTLCID:  packet.incomingHTLCID,
                outgoingChanID:  packet.outgoingChanID,
                outgoingHTLCID:  packet.outgoingHTLCID,
                incomingAmount:  packet.incomingAmount,
                amount:          packet.amount,
                incomingTimeout: packet.incomingTimeout,
                outgoingTimeout: packet.outgoingTimeout,
                circuit:         packet.circuit,
                obfuscator:      packet.obfuscator,
                linkFailure:     failure,
                htlc: &amp;lnwire.UpdateFailHTLC{
                        Reason: reason,
                },
        }

        // Route a fail packet back to the source link.
        err = s.mailOrchestrator.Deliver(failPkt.incomingChanID, failPkt)
        if err != nil </span><span class="cov0" title="0">{
                err = fmt.Errorf("source chanid=%v unable to "+
                        "handle switch packet: %v",
                        packet.incomingChanID, err)
                log.Error(err)
                return err
        }</span>

        <span class="cov8" title="1">return failure</span>
}

// closeCircuit accepts a settle or fail htlc and the associated htlc packet and
// attempts to determine the source that forwarded this htlc. This method will
// set the incoming chan and htlc ID of the given packet if the source was
// found, and will properly [re]encrypt any failure messages.
func (s *Switch) closeCircuit(pkt *htlcPacket) (*PaymentCircuit, error) <span class="cov8" title="1">{
        // If the packet has its source, that means it was failed locally by
        // the outgoing link. We fail it here to make sure only one response
        // makes it through the switch.
        if pkt.hasSource </span><span class="cov8" title="1">{
                circuit, err := s.circuits.FailCircuit(pkt.inKey())
                switch err </span>{

                // Circuit successfully closed.
                case nil:<span class="cov8" title="1">
                        return circuit, nil</span>

                // Circuit was previously closed, but has not been deleted.
                // We'll just drop this response until the circuit has been
                // fully removed.
                case ErrCircuitClosing:<span class="cov0" title="0">
                        return nil, err</span>

                // Failed to close circuit because it does not exist. This is
                // likely because the circuit was already successfully closed.
                // Since this packet failed locally, there is no forwarding
                // package entry to acknowledge.
                case ErrUnknownCircuit:<span class="cov0" title="0">
                        return nil, err</span>

                // Unexpected error.
                default:<span class="cov0" title="0">
                        return nil, err</span>
                }
        }

        // Otherwise, this is packet was received from the remote party.  Use
        // circuit map to find the incoming link to receive the settle/fail.
        <span class="cov8" title="1">circuit, err := s.circuits.CloseCircuit(pkt.outKey())
        switch err </span>{

        // Open circuit successfully closed.
        case nil:<span class="cov8" title="1">
                pkt.incomingChanID = circuit.Incoming.ChanID
                pkt.incomingHTLCID = circuit.Incoming.HtlcID
                pkt.circuit = circuit
                pkt.sourceRef = &amp;circuit.AddRef

                pktType := "SETTLE"
                if _, ok := pkt.htlc.(*lnwire.UpdateFailHTLC); ok </span><span class="cov8" title="1">{
                        pktType = "FAIL"
                }</span>

                <span class="cov8" title="1">log.Debugf("Closed completed %s circuit for %x: "+
                        "(%s, %d) &lt;-&gt; (%s, %d)", pktType, pkt.circuit.PaymentHash,
                        pkt.incomingChanID, pkt.incomingHTLCID,
                        pkt.outgoingChanID, pkt.outgoingHTLCID)

                return circuit, nil</span>

        // Circuit was previously closed, but has not been deleted. We'll just
        // drop this response until the circuit has been removed.
        case ErrCircuitClosing:<span class="cov0" title="0">
                return nil, err</span>

        // Failed to close circuit because it does not exist. This is likely
        // because the circuit was already successfully closed.
        case ErrUnknownCircuit:<span class="cov8" title="1">
                if pkt.destRef != nil </span><span class="cov8" title="1">{
                        // Add this SettleFailRef to the set of pending settle/fail entries
                        // awaiting acknowledgement.
                        s.pendingSettleFails = append(s.pendingSettleFails, *pkt.destRef)
                }</span>

                // If this is a settle, we will not log an error message as settles
                // are expected to hit the ErrUnknownCircuit case. The only way fails
                // can hit this case if the link restarts after having just sent a fail
                // to the switch.
                <span class="cov8" title="1">_, isSettle := pkt.htlc.(*lnwire.UpdateFulfillHTLC)
                if !isSettle </span><span class="cov0" title="0">{
                        err := fmt.Errorf("unable to find target channel "+
                                "for HTLC fail: channel ID = %s, "+
                                "HTLC ID = %d", pkt.outgoingChanID,
                                pkt.outgoingHTLCID)
                        log.Error(err)

                        return nil, err
                }</span>

                <span class="cov8" title="1">return nil, nil</span>

        // Unexpected error.
        default:<span class="cov0" title="0">
                return nil, err</span>
        }
}

// ackSettleFail is used by the switch to ACK any settle/fail entries in the
// forwarding package of the outgoing link for a payment circuit. We do this if
// we're the originator of the payment, so the link stops attempting to
// re-broadcast.
func (s *Switch) ackSettleFail(settleFailRefs ...channeldb.SettleFailRef) error <span class="cov8" title="1">{
        return kvdb.Batch(s.cfg.DB, func(tx kvdb.RwTx) error </span><span class="cov8" title="1">{
                return s.cfg.SwitchPackager.AckSettleFails(tx, settleFailRefs...)
        }</span>)
}

// teardownCircuit removes a pending or open circuit from the switch's circuit
// map and prints useful logging statements regarding the outcome.
func (s *Switch) teardownCircuit(pkt *htlcPacket) error <span class="cov8" title="1">{
        var pktType string
        switch htlc := pkt.htlc.(type) </span>{
        case *lnwire.UpdateFulfillHTLC:<span class="cov8" title="1">
                pktType = "SETTLE"</span>
        case *lnwire.UpdateFailHTLC:<span class="cov8" title="1">
                pktType = "FAIL"</span>
        default:<span class="cov0" title="0">
                return fmt.Errorf("cannot tear down packet of type: %T", htlc)</span>
        }

        <span class="cov8" title="1">var paymentHash lntypes.Hash

        // Perform a defensive check to make sure we don't try to access a nil
        // circuit.
        circuit := pkt.circuit
        if circuit != nil </span><span class="cov8" title="1">{
                copy(paymentHash[:], circuit.PaymentHash[:])
        }</span>

        <span class="cov8" title="1">log.Debugf("Tearing down circuit with %s pkt, removing circuit=%v "+
                "with keystone=%v", pktType, pkt.inKey(), pkt.outKey())

        err := s.circuits.DeleteCircuits(pkt.inKey())
        if err != nil </span><span class="cov0" title="0">{
                log.Warnf("Failed to tear down circuit (%s, %d) &lt;-&gt; (%s, %d) "+
                        "with payment_hash=%v using %s pkt", pkt.incomingChanID,
                        pkt.incomingHTLCID, pkt.outgoingChanID,
                        pkt.outgoingHTLCID, pkt.circuit.PaymentHash, pktType)

                return err
        }</span>

        <span class="cov8" title="1">log.Debugf("Closed %s circuit for %v: (%s, %d) &lt;-&gt; (%s, %d)", pktType,
                paymentHash, pkt.incomingChanID, pkt.incomingHTLCID,
                pkt.outgoingChanID, pkt.outgoingHTLCID)

        return nil</span>
}

// CloseLink creates and sends the close channel command to the target link
// directing the specified closure type. If the closure type is CloseRegular,
// targetFeePerKw parameter should be the ideal fee-per-kw that will be used as
// a starting point for close negotiation. The deliveryScript parameter is an
// optional parameter which sets a user specified script to close out to.
func (s *Switch) CloseLink(ctx context.Context, chanPoint *wire.OutPoint,
        closeType contractcourt.ChannelCloseType,
        targetFeePerKw, maxFee chainfee.SatPerKWeight,
        deliveryScript lnwire.DeliveryAddress) (chan interface{}, chan error) <span class="cov0" title="0">{

        // TODO(roasbeef) abstract out the close updates.
        updateChan := make(chan interface{}, 2)
        errChan := make(chan error, 1)

        command := &amp;ChanClose{
                CloseType:      closeType,
                ChanPoint:      chanPoint,
                Updates:        updateChan,
                TargetFeePerKw: targetFeePerKw,
                DeliveryScript: deliveryScript,
                Err:            errChan,
                MaxFee:         maxFee,
                Ctx:            ctx,
        }

        select </span>{
        case s.chanCloseRequests &lt;- command:<span class="cov0" title="0">
                return updateChan, errChan</span>

        case &lt;-s.quit:<span class="cov0" title="0">
                errChan &lt;- ErrSwitchExiting
                close(updateChan)
                return updateChan, errChan</span>
        }
}

// htlcForwarder is responsible for optimally forwarding (and possibly
// fragmenting) incoming/outgoing HTLCs amongst all active interfaces and their
// links. The duties of the forwarder are similar to that of a network switch,
// in that it facilitates multi-hop payments by acting as a central messaging
// bus. The switch communicates will active links to create, manage, and tear
// down active onion routed payments. Each active channel is modeled as
// networked device with metadata such as the available payment bandwidth, and
// total link capacity.
//
// NOTE: This MUST be run as a goroutine.
func (s *Switch) htlcForwarder() <span class="cov8" title="1">{
        defer s.wg.Done()

        defer func() </span><span class="cov8" title="1">{
                s.blockEpochStream.Cancel()

                // Remove all links once we've been signalled for shutdown.
                var linksToStop []ChannelLink
                s.indexMtx.Lock()
                for _, link := range s.linkIndex </span><span class="cov8" title="1">{
                        activeLink := s.removeLink(link.ChanID())
                        if activeLink == nil </span><span class="cov0" title="0">{
                                log.Errorf("unable to remove ChannelLink(%v) "+
                                        "on stop", link.ChanID())
                                continue</span>
                        }
                        <span class="cov8" title="1">linksToStop = append(linksToStop, activeLink)</span>
                }
                <span class="cov8" title="1">for _, link := range s.pendingLinkIndex </span><span class="cov8" title="1">{
                        pendingLink := s.removeLink(link.ChanID())
                        if pendingLink == nil </span><span class="cov0" title="0">{
                                log.Errorf("unable to remove ChannelLink(%v) "+
                                        "on stop", link.ChanID())
                                continue</span>
                        }
                        <span class="cov8" title="1">linksToStop = append(linksToStop, pendingLink)</span>
                }
                <span class="cov8" title="1">s.indexMtx.Unlock()

                // Now that all pending and live links have been removed from
                // the forwarding indexes, stop each one before shutting down.
                // We'll shut them down in parallel to make exiting as fast as
                // possible.
                var wg sync.WaitGroup
                for _, link := range linksToStop </span><span class="cov8" title="1">{
                        wg.Add(1)
                        go func(l ChannelLink) </span><span class="cov8" title="1">{
                                defer wg.Done()

                                l.Stop()
                        }</span>(link)
                }
                <span class="cov8" title="1">wg.Wait()

                // Before we exit fully, we'll attempt to flush out any
                // forwarding events that may still be lingering since the last
                // batch flush.
                if err := s.FlushForwardingEvents(); err != nil </span><span class="cov0" title="0">{
                        log.Errorf("unable to flush forwarding events: %v", err)
                }</span>
        }()

        // TODO(roasbeef): cleared vs settled distinction
        <span class="cov8" title="1">var (
                totalNumUpdates uint64
                totalSatSent    btcutil.Amount
                totalSatRecv    btcutil.Amount
        )
        s.cfg.LogEventTicker.Resume()
        defer s.cfg.LogEventTicker.Stop()

        // Every 15 seconds, we'll flush out the forwarding events that
        // occurred during that period.
        s.cfg.FwdEventTicker.Resume()
        defer s.cfg.FwdEventTicker.Stop()

        defer s.cfg.AckEventTicker.Stop()

out:
        for </span><span class="cov8" title="1">{

                // If the set of pending settle/fail entries is non-zero,
                // reinstate the ack ticker so we can batch ack them.
                if len(s.pendingSettleFails) &gt; 0 </span><span class="cov8" title="1">{
                        s.cfg.AckEventTicker.Resume()
                }</span>

                <span class="cov8" title="1">select </span>{
                case blockEpoch, ok := &lt;-s.blockEpochStream.Epochs:<span class="cov0" title="0">
                        if !ok </span><span class="cov0" title="0">{
                                break out</span>
                        }

                        <span class="cov0" title="0">atomic.StoreUint32(&amp;s.bestHeight, uint32(blockEpoch.Height))</span>

                // A local close request has arrived, we'll forward this to the
                // relevant link (if it exists) so the channel can be
                // cooperatively closed (if possible).
                case req := &lt;-s.chanCloseRequests:<span class="cov0" title="0">
                        chanID := lnwire.NewChanIDFromOutPoint(*req.ChanPoint)

                        s.indexMtx.RLock()
                        link, ok := s.linkIndex[chanID]
                        if !ok </span><span class="cov0" title="0">{
                                s.indexMtx.RUnlock()

                                req.Err &lt;- fmt.Errorf("no peer for channel with "+
                                        "chan_id=%x", chanID[:])
                                continue</span>
                        }
                        <span class="cov0" title="0">s.indexMtx.RUnlock()

                        peerPub := link.PeerPubKey()
                        log.Debugf("Requesting local channel close: peer=%x, "+
                                "chan_id=%x", link.PeerPubKey(), chanID[:])

                        go s.cfg.LocalChannelClose(peerPub[:], req)</span>

                case resolutionMsg := &lt;-s.resolutionMsgs:<span class="cov8" title="1">
                        // We'll persist the resolution message to the Switch's
                        // resolution store.
                        resMsg := resolutionMsg.ResolutionMsg
                        err := s.resMsgStore.addResolutionMsg(&amp;resMsg)
                        if err != nil </span><span class="cov0" title="0">{
                                // This will only fail if there is a database
                                // error or a serialization error. Sending the
                                // error prevents the contractcourt from being
                                // in a state where it believes the send was
                                // successful, when it wasn't.
                                log.Errorf("unable to add resolution msg: %v",
                                        err)
                                resolutionMsg.errChan &lt;- err
                                continue</span>
                        }

                        // At this point, the resolution message has been
                        // persisted. It is safe to signal success by sending
                        // a nil error since the Switch will re-deliver the
                        // resolution message on restart.
                        <span class="cov8" title="1">resolutionMsg.errChan &lt;- nil

                        // Create a htlc packet for this resolution. We do
                        // not have some of the information that we'll need
                        // for blinded error handling here , so we'll rely on
                        // our forwarding logic to fill it in later.
                        pkt := &amp;htlcPacket{
                                outgoingChanID: resolutionMsg.SourceChan,
                                outgoingHTLCID: resolutionMsg.HtlcIndex,
                                isResolution:   true,
                        }

                        // Resolution messages will either be cancelling
                        // backwards an existing HTLC, or settling a previously
                        // outgoing HTLC. Based on this, we'll map the message
                        // to the proper htlcPacket.
                        if resolutionMsg.Failure != nil </span><span class="cov0" title="0">{
                                pkt.htlc = &amp;lnwire.UpdateFailHTLC{}
                        }</span> else<span class="cov8" title="1"> {
                                pkt.htlc = &amp;lnwire.UpdateFulfillHTLC{
                                        PaymentPreimage: *resolutionMsg.PreImage,
                                }
                        }</span>

                        <span class="cov8" title="1">log.Debugf("Received outside contract resolution, "+
                                "mapping to: %v", spew.Sdump(pkt))

                        // We don't check the error, as the only failure we can
                        // encounter is due to the circuit already being
                        // closed. This is fine, as processing this message is
                        // meant to be idempotent.
                        err = s.handlePacketForward(pkt)
                        if err != nil </span><span class="cov0" title="0">{
                                log.Errorf("Unable to forward resolution msg: %v", err)
                        }</span>

                // A new packet has arrived for forwarding, we'll interpret the
                // packet concretely, then either forward it along, or
                // interpret a return packet to a locally initialized one.
                case cmd := &lt;-s.htlcPlex:<span class="cov8" title="1">
                        cmd.err &lt;- s.handlePacketForward(cmd.pkt)</span>

                // When this time ticks, then it indicates that we should
                // collect all the forwarding events since the last internal,
                // and write them out to our log.
                case &lt;-s.cfg.FwdEventTicker.Ticks():<span class="cov8" title="1">
                        s.wg.Add(1)
                        go func() </span><span class="cov8" title="1">{
                                defer s.wg.Done()

                                if err := s.FlushForwardingEvents(); err != nil </span><span class="cov0" title="0">{
                                        log.Errorf("Unable to flush "+
                                                "forwarding events: %v", err)
                                }</span>
                        }()

                // The log ticker has fired, so we'll calculate some forwarding
                // stats for the last 10 seconds to display within the logs to
                // users.
                case &lt;-s.cfg.LogEventTicker.Ticks():<span class="cov8" title="1">
                        // First, we'll collate the current running tally of
                        // our forwarding stats.
                        prevSatSent := totalSatSent
                        prevSatRecv := totalSatRecv
                        prevNumUpdates := totalNumUpdates

                        var (
                                newNumUpdates uint64
                                newSatSent    btcutil.Amount
                                newSatRecv    btcutil.Amount
                        )

                        // Next, we'll run through all the registered links and
                        // compute their up-to-date forwarding stats.
                        s.indexMtx.RLock()
                        for _, link := range s.linkIndex </span><span class="cov8" title="1">{
                                // TODO(roasbeef): when links first registered
                                // stats printed.
                                updates, sent, recv := link.Stats()
                                newNumUpdates += updates
                                newSatSent += sent.ToSatoshis()
                                newSatRecv += recv.ToSatoshis()
                        }</span>
                        <span class="cov8" title="1">s.indexMtx.RUnlock()

                        var (
                                diffNumUpdates uint64
                                diffSatSent    btcutil.Amount
                                diffSatRecv    btcutil.Amount
                        )

                        // If this is the first time we're computing these
                        // stats, then the diff is just the new value. We do
                        // this in order to avoid integer underflow issues.
                        if prevNumUpdates == 0 </span><span class="cov8" title="1">{
                                diffNumUpdates = newNumUpdates
                                diffSatSent = newSatSent
                                diffSatRecv = newSatRecv
                        }</span> else<span class="cov0" title="0"> {
                                diffNumUpdates = newNumUpdates - prevNumUpdates
                                diffSatSent = newSatSent - prevSatSent
                                diffSatRecv = newSatRecv - prevSatRecv
                        }</span>

                        // If the diff of num updates is zero, then we haven't
                        // forwarded anything in the last 10 seconds, so we can
                        // skip this update.
                        <span class="cov8" title="1">if diffNumUpdates == 0 </span><span class="cov8" title="1">{
                                continue</span>
                        }

                        // If the diff of num updates is negative, then some
                        // links may have been unregistered from the switch, so
                        // we'll update our stats to only include our registered
                        // links.
                        <span class="cov8" title="1">if int64(diffNumUpdates) &lt; 0 </span><span class="cov0" title="0">{
                                totalNumUpdates = newNumUpdates
                                totalSatSent = newSatSent
                                totalSatRecv = newSatRecv
                                continue</span>
                        }

                        // Otherwise, we'll log this diff, then accumulate the
                        // new stats into the running total.
                        <span class="cov8" title="1">log.Debugf("Sent %d satoshis and received %d satoshis "+
                                "in the last 10 seconds (%f tx/sec)",
                                diffSatSent, diffSatRecv,
                                float64(diffNumUpdates)/10)

                        totalNumUpdates += diffNumUpdates
                        totalSatSent += diffSatSent
                        totalSatRecv += diffSatRecv</span>

                // The ack ticker has fired so if we have any settle/fail entries
                // for a forwarding package to ack, we will do so here in a batch
                // db call.
                case &lt;-s.cfg.AckEventTicker.Ticks():<span class="cov0" title="0">
                        // If the current set is empty, pause the ticker.
                        if len(s.pendingSettleFails) == 0 </span><span class="cov0" title="0">{
                                s.cfg.AckEventTicker.Pause()
                                continue</span>
                        }

                        // Batch ack the settle/fail entries.
                        <span class="cov0" title="0">if err := s.ackSettleFail(s.pendingSettleFails...); err != nil </span><span class="cov0" title="0">{
                                log.Errorf("Unable to ack batch of settle/fails: %v", err)
                                continue</span>
                        }

                        <span class="cov0" title="0">log.Tracef("Acked %d settle fails: %v",
                                len(s.pendingSettleFails),
                                lnutils.SpewLogClosure(s.pendingSettleFails))

                        // Reset the pendingSettleFails buffer while keeping acquired
                        // memory.
                        s.pendingSettleFails = s.pendingSettleFails[:0]</span>

                case &lt;-s.quit:<span class="cov8" title="1">
                        return</span>
                }
        }
}

// Start starts all helper goroutines required for the operation of the switch.
func (s *Switch) Start() error <span class="cov8" title="1">{
        if !atomic.CompareAndSwapInt32(&amp;s.started, 0, 1) </span><span class="cov0" title="0">{
                log.Warn("Htlc Switch already started")
                return errors.New("htlc switch already started")
        }</span>

        <span class="cov8" title="1">log.Infof("HTLC Switch starting")

        blockEpochStream, err := s.cfg.Notifier.RegisterBlockEpochNtfn(nil)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">s.blockEpochStream = blockEpochStream

        s.wg.Add(1)
        go s.htlcForwarder()

        if err := s.reforwardResponses(); err != nil </span><span class="cov0" title="0">{
                s.Stop()
                log.Errorf("unable to reforward responses: %v", err)
                return err
        }</span>

        <span class="cov8" title="1">if err := s.reforwardResolutions(); err != nil </span><span class="cov0" title="0">{
                // We are already stopping so we can ignore the error.
                _ = s.Stop()
                log.Errorf("unable to reforward resolutions: %v", err)
                return err
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// reforwardResolutions fetches the set of resolution messages stored on-disk
// and reforwards them if their circuits are still open. If the circuits have
// been deleted, then we will delete the resolution message from the database.
func (s *Switch) reforwardResolutions() error <span class="cov8" title="1">{
        // Fetch all stored resolution messages, deleting the ones that are
        // resolved.
        resMsgs, err := s.resMsgStore.fetchAllResolutionMsg()
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">switchPackets := make([]*htlcPacket, 0, len(resMsgs))
        for _, resMsg := range resMsgs </span><span class="cov8" title="1">{
                // If the open circuit no longer exists, then we can remove the
                // message from the store.
                outKey := CircuitKey{
                        ChanID: resMsg.SourceChan,
                        HtlcID: resMsg.HtlcIndex,
                }

                if s.circuits.LookupOpenCircuit(outKey) == nil </span><span class="cov8" title="1">{
                        // The open circuit doesn't exist.
                        err := s.resMsgStore.deleteResolutionMsg(&amp;outKey)
                        if err != nil </span><span class="cov0" title="0">{
                                return err
                        }</span>

                        <span class="cov8" title="1">continue</span>
                }

                // The circuit is still open, so we can assume that the link or
                // switch (if we are the source) hasn't cleaned it up yet.
                // We rely on our forwarding logic to fill in details that
                // are not currently available to us.
                <span class="cov0" title="0">resPkt := &amp;htlcPacket{
                        outgoingChanID: resMsg.SourceChan,
                        outgoingHTLCID: resMsg.HtlcIndex,
                        isResolution:   true,
                }

                if resMsg.Failure != nil </span><span class="cov0" title="0">{
                        resPkt.htlc = &amp;lnwire.UpdateFailHTLC{}
                }</span> else<span class="cov0" title="0"> {
                        resPkt.htlc = &amp;lnwire.UpdateFulfillHTLC{
                                PaymentPreimage: *resMsg.PreImage,
                        }
                }</span>

                <span class="cov0" title="0">switchPackets = append(switchPackets, resPkt)</span>
        }

        // We'll now dispatch the set of resolution messages to the proper
        // destination. An error is only encountered here if the switch is
        // shutting down.
        <span class="cov8" title="1">if err := s.ForwardPackets(nil, switchPackets...); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// reforwardResponses for every known, non-pending channel, loads all associated
// forwarding packages and reforwards any Settle or Fail HTLCs found. This is
// used to resurrect the switch's mailboxes after a restart. This also runs for
// waiting close channels since there may be settles or fails that need to be
// reforwarded before they completely close.
func (s *Switch) reforwardResponses() error <span class="cov8" title="1">{
        openChannels, err := s.cfg.FetchAllChannels()
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">for _, openChannel := range openChannels </span><span class="cov8" title="1">{
                shortChanID := openChannel.ShortChanID()

                // Locally-initiated payments never need reforwarding.
                if shortChanID == hop.Source </span><span class="cov0" title="0">{
                        continue</span>
                }

                // If the channel is pending, it should have no forwarding
                // packages, and nothing to reforward.
                <span class="cov8" title="1">if openChannel.IsPending </span><span class="cov0" title="0">{
                        continue</span>
                }

                // Channels in open or waiting-close may still have responses in
                // their forwarding packages. We will continue to reattempt
                // forwarding on startup until the channel is fully-closed.
                //
                // Load this channel's forwarding packages, and deliver them to
                // the switch.
                <span class="cov8" title="1">fwdPkgs, err := s.loadChannelFwdPkgs(shortChanID)
                if err != nil </span><span class="cov0" title="0">{
                        log.Errorf("unable to load forwarding "+
                                "packages for %v: %v", shortChanID, err)
                        return err
                }</span>

                <span class="cov8" title="1">s.reforwardSettleFails(fwdPkgs)</span>
        }

        <span class="cov8" title="1">return nil</span>
}

// loadChannelFwdPkgs loads all forwarding packages owned by the `source` short
// channel identifier.
func (s *Switch) loadChannelFwdPkgs(source lnwire.ShortChannelID) ([]*channeldb.FwdPkg, error) <span class="cov8" title="1">{

        var fwdPkgs []*channeldb.FwdPkg
        if err := kvdb.View(s.cfg.DB, func(tx kvdb.RTx) error </span><span class="cov8" title="1">{
                var err error
                fwdPkgs, err = s.cfg.SwitchPackager.LoadChannelFwdPkgs(
                        tx, source,
                )
                return err
        }</span>, func() <span class="cov8" title="1">{
                fwdPkgs = nil
        }</span>); err != nil <span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return fwdPkgs, nil</span>
}

// reforwardSettleFails parses the Settle and Fail HTLCs from the list of
// forwarding packages, and reforwards those that have not been acknowledged.
// This is intended to occur on startup, in order to recover the switch's
// mailboxes, and to ensure that responses can be propagated in case the
// outgoing link never comes back online.
//
// NOTE: This should mimic the behavior processRemoteSettleFails.
func (s *Switch) reforwardSettleFails(fwdPkgs []*channeldb.FwdPkg) <span class="cov8" title="1">{
        for _, fwdPkg := range fwdPkgs </span><span class="cov8" title="1">{
                switchPackets := make([]*htlcPacket, 0, len(fwdPkg.SettleFails))
                for i, update := range fwdPkg.SettleFails </span><span class="cov0" title="0">{
                        // Skip any settles or fails that have already been
                        // acknowledged by the incoming link that originated the
                        // forwarded Add.
                        if fwdPkg.SettleFailFilter.Contains(uint16(i)) </span><span class="cov0" title="0">{
                                continue</span>
                        }

                        <span class="cov0" title="0">switch msg := update.UpdateMsg.(type) </span>{
                        // A settle for an HTLC we previously forwarded HTLC has
                        // been received. So we'll forward the HTLC to the
                        // switch which will handle propagating the settle to
                        // the prior hop.
                        case *lnwire.UpdateFulfillHTLC:<span class="cov0" title="0">
                                destRef := fwdPkg.DestRef(uint16(i))
                                settlePacket := &amp;htlcPacket{
                                        outgoingChanID: fwdPkg.Source,
                                        outgoingHTLCID: msg.ID,
                                        destRef:        &amp;destRef,
                                        htlc:           msg,
                                }

                                // Add the packet to the batch to be forwarded, and
                                // notify the overflow queue that a spare spot has been
                                // freed up within the commitment state.
                                switchPackets = append(switchPackets, settlePacket)</span>

                        // A failureCode message for a previously forwarded HTLC has been
                        // received. As a result a new slot will be freed up in our
                        // commitment state, so we'll forward this to the switch so the
                        // backwards undo can continue.
                        case *lnwire.UpdateFailHTLC:<span class="cov0" title="0">
                                // Fetch the reason the HTLC was canceled so
                                // we can continue to propagate it. This
                                // failure originated from another node, so
                                // the linkFailure field is not set on this
                                // packet. We rely on the link to fill in
                                // additional circuit information for us.
                                failPacket := &amp;htlcPacket{
                                        outgoingChanID: fwdPkg.Source,
                                        outgoingHTLCID: msg.ID,
                                        destRef: &amp;channeldb.SettleFailRef{
                                                Source: fwdPkg.Source,
                                                Height: fwdPkg.Height,
                                                Index:  uint16(i),
                                        },
                                        htlc: msg,
                                }

                                // Add the packet to the batch to be forwarded, and
                                // notify the overflow queue that a spare spot has been
                                // freed up within the commitment state.
                                switchPackets = append(switchPackets, failPacket)</span>
                        }
                }

                // Since this send isn't tied to a specific link, we pass a nil
                // link quit channel, meaning the send will fail only if the
                // switch receives a shutdown request.
                <span class="cov8" title="1">if err := s.ForwardPackets(nil, switchPackets...); err != nil </span><span class="cov0" title="0">{
                        log.Errorf("Unhandled error while reforwarding packets "+
                                "settle/fail over htlcswitch: %v", err)
                }</span>
        }
}

// Stop gracefully stops all active helper goroutines, then waits until they've
// exited.
func (s *Switch) Stop() error <span class="cov8" title="1">{
        if !atomic.CompareAndSwapInt32(&amp;s.shutdown, 0, 1) </span><span class="cov8" title="1">{
                log.Warn("Htlc Switch already stopped")
                return errors.New("htlc switch already shutdown")
        }</span>

        <span class="cov8" title="1">log.Info("HTLC Switch shutting down...")
        defer log.Debug("HTLC Switch shutdown complete")

        close(s.quit)

        s.wg.Wait()

        // Wait until all active goroutines have finished exiting before
        // stopping the mailboxes, otherwise the mailbox map could still be
        // accessed and modified.
        s.mailOrchestrator.Stop()

        return nil</span>
}

// CreateAndAddLink will create a link and then add it to the internal maps
// when given a ChannelLinkConfig and LightningChannel.
func (s *Switch) CreateAndAddLink(linkCfg ChannelLinkConfig,
        lnChan *lnwallet.LightningChannel) error <span class="cov0" title="0">{

        link := NewChannelLink(linkCfg, lnChan)
        return s.AddLink(link)
}</span>

// AddLink is used to initiate the handling of the add link command. The
// request will be propagated and handled in the main goroutine.
func (s *Switch) AddLink(link ChannelLink) error <span class="cov8" title="1">{
        s.indexMtx.Lock()
        defer s.indexMtx.Unlock()

        chanID := link.ChanID()

        // First, ensure that this link is not already active in the switch.
        _, err := s.getLink(chanID)
        if err == nil </span><span class="cov8" title="1">{
                return fmt.Errorf("unable to add ChannelLink(%v), already "+
                        "active", chanID)
        }</span>

        // Get and attach the mailbox for this link, which buffers packets in
        // case there packets that we tried to deliver while this link was
        // offline.
        <span class="cov8" title="1">shortChanID := link.ShortChanID()
        mailbox := s.mailOrchestrator.GetOrCreateMailBox(chanID, shortChanID)
        link.AttachMailBox(mailbox)

        // Attach the Switch's failAliasUpdate function to the link.
        link.attachFailAliasUpdate(s.failAliasUpdate)

        if err := link.Start(); err != nil </span><span class="cov0" title="0">{
                log.Errorf("AddLink failed to start link with chanID=%v: %v",
                        chanID, err)
                s.removeLink(chanID)
                return err
        }</span>

        <span class="cov8" title="1">if shortChanID == hop.Source </span><span class="cov8" title="1">{
                log.Infof("Adding pending link chan_id=%v, short_chan_id=%v",
                        chanID, shortChanID)

                s.pendingLinkIndex[chanID] = link
        }</span> else<span class="cov8" title="1"> {
                log.Infof("Adding live link chan_id=%v, short_chan_id=%v",
                        chanID, shortChanID)

                s.addLiveLink(link)
                s.mailOrchestrator.BindLiveShortChanID(
                        mailbox, chanID, shortChanID,
                )
        }</span>

        <span class="cov8" title="1">return nil</span>
}

// addLiveLink adds a link to all associated forwarding index, this makes it a
// candidate for forwarding HTLCs.
func (s *Switch) addLiveLink(link ChannelLink) <span class="cov8" title="1">{
        linkScid := link.ShortChanID()

        // We'll add the link to the linkIndex which lets us quickly
        // look up a channel when we need to close or register it, and
        // the forwarding index which'll be used when forwarding HTLC's
        // in the multi-hop setting.
        s.linkIndex[link.ChanID()] = link
        s.forwardingIndex[linkScid] = link

        // Next we'll add the link to the interface index so we can
        // quickly look up all the channels for a particular node.
        peerPub := link.PeerPubKey()
        if _, ok := s.interfaceIndex[peerPub]; !ok </span><span class="cov8" title="1">{
                s.interfaceIndex[peerPub] = make(map[lnwire.ChannelID]ChannelLink)
        }</span>
        <span class="cov8" title="1">s.interfaceIndex[peerPub][link.ChanID()] = link

        s.updateLinkAliases(link)</span>
}

// UpdateLinkAliases is the externally exposed wrapper for updating link
// aliases. It acquires the indexMtx and calls the internal method.
func (s *Switch) UpdateLinkAliases(link ChannelLink) <span class="cov0" title="0">{
        s.indexMtx.Lock()
        defer s.indexMtx.Unlock()

        s.updateLinkAliases(link)
}</span>

// updateLinkAliases updates the aliases for a given link. This will cause the
// htlcswitch to consult the alias manager on the up to date values of its
// alias maps.
//
// NOTE: this MUST be called with the indexMtx held.
func (s *Switch) updateLinkAliases(link ChannelLink) <span class="cov8" title="1">{
        linkScid := link.ShortChanID()

        aliases := link.getAliases()
        if link.isZeroConf() </span><span class="cov8" title="1">{
                if link.zeroConfConfirmed() </span><span class="cov8" title="1">{
                        // Since the zero-conf channel has confirmed, we can
                        // populate the aliasToReal mapping.
                        confirmedScid := link.confirmedScid()

                        for _, alias := range aliases </span><span class="cov8" title="1">{
                                s.aliasToReal[alias] = confirmedScid
                        }</span>

                        // Add the confirmed SCID as a key in the baseIndex.
                        <span class="cov8" title="1">s.baseIndex[confirmedScid] = linkScid</span>
                }

                // Now we populate the baseIndex which will be used to fetch
                // the link given any of the channel's alias SCIDs or the real
                // SCID. The link's SCID is an alias, so we don't need to
                // special-case it like the option-scid-alias feature-bit case
                // further down.
                <span class="cov8" title="1">for _, alias := range aliases </span><span class="cov8" title="1">{
                        s.baseIndex[alias] = linkScid
                }</span>
        } else<span class="cov8" title="1"> if link.negotiatedAliasFeature() </span><span class="cov8" title="1">{
                // First, we flush any alias mappings for this link's scid
                // before we populate the map again, in order to get rid of old
                // values that no longer exist.
                for alias, real := range s.aliasToReal </span><span class="cov8" title="1">{
                        if real == linkScid </span><span class="cov0" title="0">{
                                delete(s.aliasToReal, alias)
                        }</span>
                }

                <span class="cov8" title="1">for alias, real := range s.baseIndex </span><span class="cov8" title="1">{
                        if real == linkScid </span><span class="cov0" title="0">{
                                delete(s.baseIndex, alias)
                        }</span>
                }

                // The link's SCID is the confirmed SCID for non-zero-conf
                // option-scid-alias feature bit channels.
                <span class="cov8" title="1">for _, alias := range aliases </span><span class="cov8" title="1">{
                        s.aliasToReal[alias] = linkScid
                        s.baseIndex[alias] = linkScid
                }</span>

                // Since the link's SCID is confirmed, it was not included in
                // the baseIndex above as a key. Add it now.
                <span class="cov8" title="1">s.baseIndex[linkScid] = linkScid</span>
        }
}

// GetLink is used to initiate the handling of the get link command. The
// request will be propagated/handled to/in the main goroutine.
func (s *Switch) GetLink(chanID lnwire.ChannelID) (ChannelUpdateHandler,
        error) <span class="cov8" title="1">{

        s.indexMtx.RLock()
        defer s.indexMtx.RUnlock()

        return s.getLink(chanID)
}</span>

// getLink returns the link stored in either the pending index or the live
// lindex.
func (s *Switch) getLink(chanID lnwire.ChannelID) (ChannelLink, error) <span class="cov8" title="1">{
        link, ok := s.linkIndex[chanID]
        if !ok </span><span class="cov8" title="1">{
                link, ok = s.pendingLinkIndex[chanID]
                if !ok </span><span class="cov8" title="1">{
                        return nil, ErrChannelLinkNotFound
                }</span>
        }

        <span class="cov8" title="1">return link, nil</span>
}

// GetLinkByShortID attempts to return the link which possesses the target short
// channel ID.
func (s *Switch) GetLinkByShortID(chanID lnwire.ShortChannelID) (ChannelLink,
        error) <span class="cov0" title="0">{

        s.indexMtx.RLock()
        defer s.indexMtx.RUnlock()

        link, err := s.getLinkByShortID(chanID)
        if err != nil </span><span class="cov0" title="0">{
                // If we failed to find the link under the passed-in SCID, we
                // consult the Switch's baseIndex map to see if the confirmed
                // SCID was used for a zero-conf channel.
                aliasID, ok := s.baseIndex[chanID]
                if !ok </span><span class="cov0" title="0">{
                        return nil, err
                }</span>

                // An alias was found, use it to lookup if a link exists.
                <span class="cov0" title="0">return s.getLinkByShortID(aliasID)</span>
        }

        <span class="cov0" title="0">return link, nil</span>
}

// getLinkByShortID attempts to return the link which possesses the target
// short channel ID.
//
// NOTE: This MUST be called with the indexMtx held.
func (s *Switch) getLinkByShortID(chanID lnwire.ShortChannelID) (ChannelLink, error) <span class="cov8" title="1">{
        link, ok := s.forwardingIndex[chanID]
        if !ok </span><span class="cov8" title="1">{
                return nil, ErrChannelLinkNotFound
        }</span>

        <span class="cov8" title="1">return link, nil</span>
}

// getLinkByMapping attempts to fetch the link via the htlcPacket's
// outgoingChanID, possibly using a mapping. If it finds the link via mapping,
// the outgoingChanID will be changed so that an error can be properly
// attributed when looping over linkErrs in handlePacketForward.
//
// * If the outgoingChanID is an alias, we'll fetch the link regardless if it's
// public or not.
//
// * If the outgoingChanID is a confirmed SCID, we'll need to do more checks.
//   - If there is no entry found in baseIndex, fetch the link. This channel
//     did not have the option-scid-alias feature negotiated (which includes
//     zero-conf and option-scid-alias channel-types).
//   - If there is an entry found, fetch the link from forwardingIndex and
//     fail if this is a private link.
//
// NOTE: This MUST be called with the indexMtx read lock held.
func (s *Switch) getLinkByMapping(pkt *htlcPacket) (ChannelLink, error) <span class="cov8" title="1">{
        // Determine if this ShortChannelID is an alias or a confirmed SCID.
        chanID := pkt.outgoingChanID
        aliasID := s.cfg.IsAlias(chanID)

        log.Debugf("Querying outgoing link using chanID=%v, aliasID=%v", chanID,
                aliasID)

        // Set the originalOutgoingChanID so the proper channel_update can be
        // sent back if the option-scid-alias feature bit was negotiated.
        pkt.originalOutgoingChanID = chanID

        if aliasID </span><span class="cov8" title="1">{
                // Since outgoingChanID is an alias, we'll fetch the link via
                // baseIndex.
                baseScid, ok := s.baseIndex[chanID]
                if !ok </span><span class="cov0" title="0">{
                        // No mapping exists, bail.
                        return nil, ErrChannelLinkNotFound
                }</span>

                // A mapping exists, so use baseScid to find the link in the
                // forwardingIndex.
                <span class="cov8" title="1">link, ok := s.forwardingIndex[baseScid]
                if !ok </span><span class="cov0" title="0">{
                        // Link not found, bail.
                        return nil, ErrChannelLinkNotFound
                }</span>

                // Change the packet's outgoingChanID field so that errors are
                // properly attributed.
                <span class="cov8" title="1">pkt.outgoingChanID = baseScid

                // Return the link without checking if it's private or not.
                return link, nil</span>
        }

        // The outgoingChanID is a confirmed SCID. Attempt to fetch the base
        // SCID from baseIndex.
        <span class="cov8" title="1">baseScid, ok := s.baseIndex[chanID]
        if !ok </span><span class="cov8" title="1">{
                // outgoingChanID is not a key in base index meaning this
                // channel did not have the option-scid-alias feature bit
                // negotiated. We'll fetch the link and return it.
                link, ok := s.forwardingIndex[chanID]
                if !ok </span><span class="cov8" title="1">{
                        // The link wasn't found, bail out.
                        return nil, ErrChannelLinkNotFound
                }</span>

                <span class="cov8" title="1">return link, nil</span>
        }

        // Fetch the link whose internal SCID is baseScid.
        <span class="cov8" title="1">link, ok := s.forwardingIndex[baseScid]
        if !ok </span><span class="cov0" title="0">{
                // Link wasn't found, bail out.
                return nil, ErrChannelLinkNotFound
        }</span>

        // If the link is unadvertised, we fail since the real SCID was used to
        // forward over it and this is a channel where the option-scid-alias
        // feature bit was negotiated.
        <span class="cov8" title="1">if link.IsUnadvertised() </span><span class="cov8" title="1">{
                log.Debugf("Link is unadvertised, chanID=%v, baseScid=%v",
                        chanID, baseScid)

                return nil, ErrChannelLinkNotFound
        }</span>

        // The link is public so the confirmed SCID can be used to forward over
        // it. We'll also replace pkt's outgoingChanID field so errors can
        // properly be attributed in the calling function.
        <span class="cov8" title="1">pkt.outgoingChanID = baseScid
        return link, nil</span>
}

// HasActiveLink returns true if the given channel ID has a link in the link
// index AND the link is eligible to forward.
func (s *Switch) HasActiveLink(chanID lnwire.ChannelID) bool <span class="cov8" title="1">{
        s.indexMtx.RLock()
        defer s.indexMtx.RUnlock()

        if link, ok := s.linkIndex[chanID]; ok </span><span class="cov8" title="1">{
                return link.EligibleToForward()
        }</span>

        <span class="cov0" title="0">return false</span>
}

// RemoveLink purges the switch of any link associated with chanID. If a pending
// or active link is not found, this method does nothing. Otherwise, the method
// returns after the link has been completely shutdown.
func (s *Switch) RemoveLink(chanID lnwire.ChannelID) <span class="cov8" title="1">{
        s.indexMtx.Lock()
        link, err := s.getLink(chanID)
        if err != nil </span><span class="cov0" title="0">{
                // If err is non-nil, this means that link is also nil. The
                // link variable cannot be nil without err being non-nil.
                s.indexMtx.Unlock()
                log.Tracef("Unable to remove link for ChannelID(%v): %v",
                        chanID, err)
                return
        }</span>

        // Check if the link is already stopping and grab the stop chan if it
        // is.
        <span class="cov8" title="1">stopChan, ok := s.linkStopIndex[chanID]
        if !ok </span><span class="cov8" title="1">{
                // If the link is non-nil, it is not currently stopping, so
                // we'll add a stop chan to the linkStopIndex.
                stopChan = make(chan struct{})
                s.linkStopIndex[chanID] = stopChan
        }</span>
        <span class="cov8" title="1">s.indexMtx.Unlock()

        if ok </span><span class="cov0" title="0">{
                // If the stop chan exists, we will wait for it to be closed.
                // Once it is closed, we will exit.
                select </span>{
                case &lt;-stopChan:<span class="cov0" title="0">
                        return</span>
                case &lt;-s.quit:<span class="cov0" title="0">
                        return</span>
                }
        }

        // Stop the link before removing it from the maps.
        <span class="cov8" title="1">link.Stop()

        s.indexMtx.Lock()
        _ = s.removeLink(chanID)

        // Close stopChan and remove this link from the linkStopIndex.
        // Deleting from the index and removing from the link must be done
        // in the same block while the mutex is held.
        close(stopChan)
        delete(s.linkStopIndex, chanID)
        s.indexMtx.Unlock()</span>
}

// removeLink is used to remove and stop the channel link.
//
// NOTE: This MUST be called with the indexMtx held.
func (s *Switch) removeLink(chanID lnwire.ChannelID) ChannelLink <span class="cov8" title="1">{
        log.Infof("Removing channel link with ChannelID(%v)", chanID)

        link, err := s.getLink(chanID)
        if err != nil </span><span class="cov0" title="0">{
                return nil
        }</span>

        // Remove the channel from live link indexes.
        <span class="cov8" title="1">delete(s.pendingLinkIndex, link.ChanID())
        delete(s.linkIndex, link.ChanID())
        delete(s.forwardingIndex, link.ShortChanID())

        // If the link has been added to the peer index, then we'll move to
        // delete the entry within the index.
        peerPub := link.PeerPubKey()
        if peerIndex, ok := s.interfaceIndex[peerPub]; ok </span><span class="cov8" title="1">{
                delete(peerIndex, link.ChanID())

                // If after deletion, there are no longer any links, then we'll
                // remove the interface map all together.
                if len(peerIndex) == 0 </span><span class="cov8" title="1">{
                        delete(s.interfaceIndex, peerPub)
                }</span>
        }

        <span class="cov8" title="1">return link</span>
}

// UpdateShortChanID locates the link with the passed-in chanID and updates the
// underlying channel state. This is only used in zero-conf channels to allow
// the confirmed SCID to be updated.
func (s *Switch) UpdateShortChanID(chanID lnwire.ChannelID) error <span class="cov8" title="1">{
        s.indexMtx.Lock()
        defer s.indexMtx.Unlock()

        // Locate the target link in the link index. If no such link exists,
        // then we will ignore the request.
        link, ok := s.linkIndex[chanID]
        if !ok </span><span class="cov0" title="0">{
                return fmt.Errorf("link %v not found", chanID)
        }</span>

        // Try to update the link's underlying channel state, returning early
        // if this update failed.
        <span class="cov8" title="1">_, err := link.UpdateShortChanID()
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // Since the zero-conf channel is confirmed, we should populate the
        // aliasToReal map and update the baseIndex.
        <span class="cov8" title="1">aliases := link.getAliases()

        confirmedScid := link.confirmedScid()

        for _, alias := range aliases </span><span class="cov8" title="1">{
                s.aliasToReal[alias] = confirmedScid
        }</span>

        <span class="cov8" title="1">s.baseIndex[confirmedScid] = link.ShortChanID()

        return nil</span>
}

// GetLinksByInterface fetches all the links connected to a particular node
// identified by the serialized compressed form of its public key.
func (s *Switch) GetLinksByInterface(hop [33]byte) ([]ChannelUpdateHandler,
        error) <span class="cov0" title="0">{

        s.indexMtx.RLock()
        defer s.indexMtx.RUnlock()

        var handlers []ChannelUpdateHandler

        links, err := s.getLinks(hop)
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        // Range over the returned []ChannelLink to convert them into
        // []ChannelUpdateHandler.
        <span class="cov0" title="0">for _, link := range links </span><span class="cov0" title="0">{
                handlers = append(handlers, link)
        }</span>

        <span class="cov0" title="0">return handlers, nil</span>
}

// getLinks is function which returns the channel links of the peer by hop
// destination id.
//
// NOTE: This MUST be called with the indexMtx held.
func (s *Switch) getLinks(destination [33]byte) ([]ChannelLink, error) <span class="cov8" title="1">{
        links, ok := s.interfaceIndex[destination]
        if !ok </span><span class="cov0" title="0">{
                return nil, ErrNoLinksFound
        }</span>

        <span class="cov8" title="1">channelLinks := make([]ChannelLink, 0, len(links))
        for _, link := range links </span><span class="cov8" title="1">{
                channelLinks = append(channelLinks, link)
        }</span>

        <span class="cov8" title="1">return channelLinks, nil</span>
}

// CircuitModifier returns a reference to subset of the interfaces provided by
// the circuit map, to allow links to open and close circuits.
func (s *Switch) CircuitModifier() CircuitModifier <span class="cov8" title="1">{
        return s.circuits
}</span>

// CircuitLookup returns a reference to subset of the interfaces provided by the
// circuit map, to allow looking up circuits.
func (s *Switch) CircuitLookup() CircuitLookup <span class="cov0" title="0">{
        return s.circuits
}</span>

// commitCircuits persistently adds a circuit to the switch's circuit map.
func (s *Switch) commitCircuits(circuits ...*PaymentCircuit) (
        *CircuitFwdActions, error) <span class="cov8" title="1">{

        return s.circuits.CommitCircuits(circuits...)
}</span>

// FlushForwardingEvents flushes out the set of pending forwarding events to
// the persistent log. This will be used by the switch to periodically flush
// out the set of forwarding events to disk. External callers can also use this
// method to ensure all data is flushed to dis before querying the log.
func (s *Switch) FlushForwardingEvents() error <span class="cov8" title="1">{
        // First, we'll obtain a copy of the current set of pending forwarding
        // events.
        s.fwdEventMtx.Lock()

        // If we won't have any forwarding events, then we can exit early.
        if len(s.pendingFwdingEvents) == 0 </span><span class="cov8" title="1">{
                s.fwdEventMtx.Unlock()
                return nil
        }</span>

        <span class="cov8" title="1">events := make([]channeldb.ForwardingEvent, len(s.pendingFwdingEvents))
        copy(events[:], s.pendingFwdingEvents[:])

        // With the copy obtained, we can now clear out the header pointer of
        // the current slice. This way, we can re-use the underlying storage
        // allocated for the slice.
        s.pendingFwdingEvents = s.pendingFwdingEvents[:0]
        s.fwdEventMtx.Unlock()

        // Finally, we'll write out the copied events to the persistent
        // forwarding log.
        return s.cfg.FwdingLog.AddForwardingEvents(events)</span>
}

// BestHeight returns the best height known to the switch.
func (s *Switch) BestHeight() uint32 <span class="cov8" title="1">{
        return atomic.LoadUint32(&amp;s.bestHeight)
}</span>

// dustExceedsFeeThreshold takes in a ChannelLink, HTLC amount, and a boolean
// to determine whether the default fee threshold has been exceeded. This
// heuristic takes into account the trimmed-to-dust mechanism. The sum of the
// commitment's dust with the mailbox's dust with the amount is checked against
// the fee exposure threshold. If incoming is true, then the amount is not
// included in the sum as it was already included in the commitment's dust. A
// boolean is returned telling the caller whether the HTLC should be failed
// back.
func (s *Switch) dustExceedsFeeThreshold(link ChannelLink,
        amount lnwire.MilliSatoshi, incoming bool) bool <span class="cov8" title="1">{

        // Retrieve the link's current commitment feerate and dustClosure.
        feeRate := link.getFeeRate()
        isDust := link.getDustClosure()

        // Evaluate if the HTLC is dust on either sides' commitment.
        isLocalDust := isDust(
                feeRate, incoming, lntypes.Local, amount.ToSatoshis(),
        )
        isRemoteDust := isDust(
                feeRate, incoming, lntypes.Remote, amount.ToSatoshis(),
        )

        if !(isLocalDust || isRemoteDust) </span><span class="cov8" title="1">{
                // If the HTLC is not dust on either commitment, it's fine to
                // forward.
                return false
        }</span>

        // Fetch the dust sums currently in the mailbox for this link.
        <span class="cov8" title="1">cid := link.ChanID()
        sid := link.ShortChanID()
        mailbox := s.mailOrchestrator.GetOrCreateMailBox(cid, sid)
        localMailDust, remoteMailDust := mailbox.DustPackets()

        // If the htlc is dust on the local commitment, we'll obtain the dust
        // sum for it.
        if isLocalDust </span><span class="cov8" title="1">{
                localSum := link.getDustSum(
                        lntypes.Local, fn.None[chainfee.SatPerKWeight](),
                )
                localSum += localMailDust

                // Optionally include the HTLC amount only for outgoing
                // HTLCs.
                if !incoming </span><span class="cov8" title="1">{
                        localSum += amount
                }</span>

                // Finally check against the defined fee threshold.
                <span class="cov8" title="1">if localSum &gt; s.cfg.MaxFeeExposure </span><span class="cov8" title="1">{
                        return true
                }</span>
        }

        // Also check if the htlc is dust on the remote commitment, if we've
        // reached this point.
        <span class="cov8" title="1">if isRemoteDust </span><span class="cov8" title="1">{
                remoteSum := link.getDustSum(
                        lntypes.Remote, fn.None[chainfee.SatPerKWeight](),
                )
                remoteSum += remoteMailDust

                // Optionally include the HTLC amount only for outgoing
                // HTLCs.
                if !incoming </span><span class="cov8" title="1">{
                        remoteSum += amount
                }</span>

                // Finally check against the defined fee threshold.
                <span class="cov8" title="1">if remoteSum &gt; s.cfg.MaxFeeExposure </span><span class="cov0" title="0">{
                        return true
                }</span>
        }

        // If we reached this point, this HTLC is fine to forward.
        <span class="cov8" title="1">return false</span>
}

// failMailboxUpdate is passed to the mailbox orchestrator which in turn passes
// it to individual mailboxes. It allows the mailboxes to construct a
// FailureMessage when failing back HTLC's due to expiry and may include an
// alias in the ShortChannelID field. The outgoingScid is the SCID originally
// used in the onion. The mailboxScid is the SCID that the mailbox and link
// use. The mailboxScid is only used in the non-alias case, so it is always
// the confirmed SCID.
func (s *Switch) failMailboxUpdate(outgoingScid,
        mailboxScid lnwire.ShortChannelID) lnwire.FailureMessage <span class="cov8" title="1">{

        // Try to use the failAliasUpdate function in case this is a channel
        // that uses aliases. If it returns nil, we'll fallback to the original
        // pre-alias behavior.
        update := s.failAliasUpdate(outgoingScid, false)
        if update == nil </span><span class="cov8" title="1">{
                // Execute the fallback behavior.
                var err error
                update, err = s.cfg.FetchLastChannelUpdate(mailboxScid)
                if err != nil </span><span class="cov0" title="0">{
                        return &amp;lnwire.FailTemporaryNodeFailure{}
                }</span>
        }

        <span class="cov8" title="1">return lnwire.NewTemporaryChannelFailure(update)</span>
}

// failAliasUpdate prepares a ChannelUpdate for a failed incoming or outgoing
// HTLC on a channel where the option-scid-alias feature bit was negotiated. If
// the associated channel is not one of these, this function will return nil
// and the caller is expected to handle this properly. In this case, a return
// to the original non-alias behavior is expected.
func (s *Switch) failAliasUpdate(scid lnwire.ShortChannelID,
        incoming bool) *lnwire.ChannelUpdate1 <span class="cov8" title="1">{

        // This function does not defer the unlocking because of the database
        // lookups for ChannelUpdate.
        s.indexMtx.RLock()

        if s.cfg.IsAlias(scid) </span><span class="cov8" title="1">{
                // The alias SCID was used. In the incoming case this means
                // the channel is zero-conf as the link sets the scid. In the
                // outgoing case, the sender set the scid to use and may be
                // either the alias or the confirmed one, if it exists.
                realScid, ok := s.aliasToReal[scid]
                if !ok </span><span class="cov0" title="0">{
                        // The real, confirmed SCID does not exist yet. Find
                        // the "base" SCID that the link uses via the
                        // baseIndex. If we can't find it, return nil. This
                        // means the channel is zero-conf.
                        baseScid, ok := s.baseIndex[scid]
                        s.indexMtx.RUnlock()
                        if !ok </span><span class="cov0" title="0">{
                                return nil
                        }</span>

                        <span class="cov0" title="0">update, err := s.cfg.FetchLastChannelUpdate(baseScid)
                        if err != nil </span><span class="cov0" title="0">{
                                return nil
                        }</span>

                        // Replace the baseScid with the passed-in alias.
                        <span class="cov0" title="0">update.ShortChannelID = scid
                        sig, err := s.cfg.SignAliasUpdate(update)
                        if err != nil </span><span class="cov0" title="0">{
                                return nil
                        }</span>

                        <span class="cov0" title="0">update.Signature, err = lnwire.NewSigFromSignature(sig)
                        if err != nil </span><span class="cov0" title="0">{
                                return nil
                        }</span>

                        <span class="cov0" title="0">return update</span>
                }

                <span class="cov8" title="1">s.indexMtx.RUnlock()

                // Fetch the SCID via the confirmed SCID and replace it with
                // the alias.
                update, err := s.cfg.FetchLastChannelUpdate(realScid)
                if err != nil </span><span class="cov0" title="0">{
                        return nil
                }</span>

                // In the incoming case, we want to ensure that we don't leak
                // the UTXO in case the channel is private. In the outgoing
                // case, since the alias was used, we do the same thing.
                <span class="cov8" title="1">update.ShortChannelID = scid
                sig, err := s.cfg.SignAliasUpdate(update)
                if err != nil </span><span class="cov0" title="0">{
                        return nil
                }</span>

                <span class="cov8" title="1">update.Signature, err = lnwire.NewSigFromSignature(sig)
                if err != nil </span><span class="cov0" title="0">{
                        return nil
                }</span>

                <span class="cov8" title="1">return update</span>
        }

        // If the confirmed SCID is not in baseIndex, this is not an
        // option-scid-alias or zero-conf channel.
        <span class="cov8" title="1">baseScid, ok := s.baseIndex[scid]
        if !ok </span><span class="cov8" title="1">{
                s.indexMtx.RUnlock()
                return nil
        }</span>

        // Fetch the link so we can get an alias to use in the ShortChannelID
        // of the ChannelUpdate.
        <span class="cov8" title="1">link, ok := s.forwardingIndex[baseScid]
        s.indexMtx.RUnlock()
        if !ok </span><span class="cov0" title="0">{
                // This should never happen, but if it does for some reason,
                // fallback to the old behavior.
                return nil
        }</span>

        <span class="cov8" title="1">aliases := link.getAliases()
        if len(aliases) == 0 </span><span class="cov0" title="0">{
                // This should never happen, but if it does, fallback.
                return nil
        }</span>

        // Fetch the ChannelUpdate via the real, confirmed SCID.
        <span class="cov8" title="1">update, err := s.cfg.FetchLastChannelUpdate(scid)
        if err != nil </span><span class="cov0" title="0">{
                return nil
        }</span>

        // The incoming case will replace the ShortChannelID in the retrieved
        // ChannelUpdate with the alias to ensure no privacy leak occurs. This
        // would happen if a private non-zero-conf option-scid-alias
        // feature-bit channel leaked its UTXO here rather than supplying an
        // alias. In the outgoing case, the confirmed SCID was actually used
        // for forwarding in the onion, so no replacement is necessary as the
        // sender knows the scid.
        <span class="cov8" title="1">if incoming </span><span class="cov8" title="1">{
                // We will replace and sign the update with the first alias.
                // Since this happens on the incoming side, it's not actually
                // possible to know what the sender used in the onion.
                update.ShortChannelID = aliases[0]
                sig, err := s.cfg.SignAliasUpdate(update)
                if err != nil </span><span class="cov0" title="0">{
                        return nil
                }</span>

                <span class="cov8" title="1">update.Signature, err = lnwire.NewSigFromSignature(sig)
                if err != nil </span><span class="cov0" title="0">{
                        return nil
                }</span>
        }

        <span class="cov8" title="1">return update</span>
}

// AddAliasForLink instructs the Switch to update its in-memory maps to reflect
// that a link has a new alias.
func (s *Switch) AddAliasForLink(chanID lnwire.ChannelID,
        alias lnwire.ShortChannelID) error <span class="cov0" title="0">{

        // Fetch the link so that we can update the underlying channel's set of
        // aliases.
        s.indexMtx.RLock()
        link, err := s.getLink(chanID)
        s.indexMtx.RUnlock()
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // If the link is a channel where the option-scid-alias feature bit was
        // not negotiated, we'll return an error.
        <span class="cov0" title="0">if !link.negotiatedAliasFeature() </span><span class="cov0" title="0">{
                return fmt.Errorf("attempted to update non-alias channel")
        }</span>

        <span class="cov0" title="0">linkScid := link.ShortChanID()

        // We'll update the maps so the Switch includes this alias in its
        // forwarding decisions.
        if link.isZeroConf() </span><span class="cov0" title="0">{
                if link.zeroConfConfirmed() </span><span class="cov0" title="0">{
                        // If the channel has confirmed on-chain, we'll
                        // add this alias to the aliasToReal map.
                        confirmedScid := link.confirmedScid()

                        s.aliasToReal[alias] = confirmedScid
                }</span>

                // Add this alias to the baseIndex mapping.
                <span class="cov0" title="0">s.baseIndex[alias] = linkScid</span>
        } else<span class="cov0" title="0"> if link.negotiatedAliasFeature() </span><span class="cov0" title="0">{
                // The channel is confirmed, so we'll populate the aliasToReal
                // and baseIndex maps.
                s.aliasToReal[alias] = linkScid
                s.baseIndex[alias] = linkScid
        }</span>

        <span class="cov0" title="0">return nil</span>
}

// handlePacketAdd handles forwarding an Add packet.
func (s *Switch) handlePacketAdd(packet *htlcPacket,
        htlc *lnwire.UpdateAddHTLC) error <span class="cov8" title="1">{

        // Check if the node is set to reject all onward HTLCs and also make
        // sure that HTLC is not from the source node.
        if s.cfg.RejectHTLC </span><span class="cov8" title="1">{
                failure := NewDetailedLinkError(
                        &amp;lnwire.FailChannelDisabled{},
                        OutgoingFailureForwardsDisabled,
                )

                return s.failAddPacket(packet, failure)
        }</span>

        // Before we attempt to find a non-strict forwarding path for this
        // htlc, check whether the htlc is being routed over the same incoming
        // and outgoing channel. If our node does not allow forwards of this
        // nature, we fail the htlc early. This check is in place to disallow
        // inefficiently routed htlcs from locking up our balance. With
        // channels where the option-scid-alias feature was negotiated, we also
        // have to be sure that the IDs aren't the same since one or both could
        // be an alias.
        <span class="cov8" title="1">linkErr := s.checkCircularForward(
                packet.incomingChanID, packet.outgoingChanID,
                s.cfg.AllowCircularRoute, htlc.PaymentHash,
        )
        if linkErr != nil </span><span class="cov8" title="1">{
                return s.failAddPacket(packet, linkErr)
        }</span>

        <span class="cov8" title="1">s.indexMtx.RLock()
        targetLink, err := s.getLinkByMapping(packet)
        if err != nil </span><span class="cov8" title="1">{
                s.indexMtx.RUnlock()

                log.Debugf("unable to find link with "+
                        "destination %v", packet.outgoingChanID)

                // If packet was forwarded from another channel link than we
                // should notify this link that some error occurred.
                linkError := NewLinkError(
                        &amp;lnwire.FailUnknownNextPeer{},
                )

                return s.failAddPacket(packet, linkError)
        }</span>
        <span class="cov8" title="1">targetPeerKey := targetLink.PeerPubKey()
        interfaceLinks, _ := s.getLinks(targetPeerKey)
        s.indexMtx.RUnlock()

        // We'll keep track of any HTLC failures during the link selection
        // process. This way we can return the error for precise link that the
        // sender selected, while optimistically trying all links to utilize
        // our available bandwidth.
        linkErrs := make(map[lnwire.ShortChannelID]*LinkError)

        // Find all destination channel links with appropriate bandwidth.
        var destinations []ChannelLink
        for _, link := range interfaceLinks </span><span class="cov8" title="1">{
                var failure *LinkError

                // We'll skip any links that aren't yet eligible for
                // forwarding.
                if !link.EligibleToForward() </span><span class="cov8" title="1">{
                        failure = NewDetailedLinkError(
                                &amp;lnwire.FailUnknownNextPeer{},
                                OutgoingFailureLinkNotEligible,
                        )
                }</span> else<span class="cov8" title="1"> {
                        // We'll ensure that the HTLC satisfies the current
                        // forwarding conditions of this target link.
                        currentHeight := atomic.LoadUint32(&amp;s.bestHeight)
                        failure = link.CheckHtlcForward(
                                htlc.PaymentHash, packet.incomingAmount,
                                packet.amount, packet.incomingTimeout,
                                packet.outgoingTimeout, packet.inboundFee,
                                currentHeight, packet.originalOutgoingChanID,
                                htlc.CustomRecords,
                        )
                }</span>

                // If this link can forward the htlc, add it to the set of
                // destinations.
                <span class="cov8" title="1">if failure == nil </span><span class="cov8" title="1">{
                        destinations = append(destinations, link)
                        continue</span>
                }

                <span class="cov8" title="1">linkErrs[link.ShortChanID()] = failure</span>
        }

        // If we had a forwarding failure due to the HTLC not satisfying the
        // current policy, then we'll send back an error, but ensure we send
        // back the error sourced at the *target* link.
        <span class="cov8" title="1">if len(destinations) == 0 </span><span class="cov8" title="1">{
                // At this point, some or all of the links rejected the HTLC so
                // we couldn't forward it. So we'll try to look up the error
                // that came from the source.
                linkErr, ok := linkErrs[packet.outgoingChanID]
                if !ok </span><span class="cov0" title="0">{
                        // If we can't find the error of the source, then we'll
                        // return an unknown next peer, though this should
                        // never happen.
                        linkErr = NewLinkError(
                                &amp;lnwire.FailUnknownNextPeer{},
                        )
                        log.Warnf("unable to find err source for "+
                                "outgoing_link=%v, errors=%v",
                                packet.outgoingChanID,
                                lnutils.SpewLogClosure(linkErrs))
                }</span>

                <span class="cov8" title="1">log.Tracef("incoming HTLC(%x) violated "+
                        "target outgoing link (id=%v) policy: %v",
                        htlc.PaymentHash[:], packet.outgoingChanID,
                        linkErr)

                return s.failAddPacket(packet, linkErr)</span>
        }

        // Choose a random link out of the set of links that can forward this
        // htlc. The reason for randomization is to evenly distribute the htlc
        // load without making assumptions about what the best channel is.
        //nolint:gosec
        <span class="cov8" title="1">destination := destinations[rand.Intn(len(destinations))]

        // Retrieve the incoming link by its ShortChannelID. Note that the
        // incomingChanID is never set to hop.Source here.
        s.indexMtx.RLock()
        incomingLink, err := s.getLinkByShortID(packet.incomingChanID)
        s.indexMtx.RUnlock()
        if err != nil </span><span class="cov0" title="0">{
                // If we couldn't find the incoming link, we can't evaluate the
                // incoming's exposure to dust, so we just fail the HTLC back.
                linkErr := NewLinkError(
                        &amp;lnwire.FailTemporaryChannelFailure{},
                )

                return s.failAddPacket(packet, linkErr)
        }</span>

        // Evaluate whether this HTLC would increase our fee exposure over the
        // threshold on the incoming link. If it does, fail it backwards.
        <span class="cov8" title="1">if s.dustExceedsFeeThreshold(
                incomingLink, packet.incomingAmount, true,
        ) </span><span class="cov0" title="0">{
                // The incoming dust exceeds the threshold, so we fail the add
                // back.
                linkErr := NewLinkError(
                        &amp;lnwire.FailTemporaryChannelFailure{},
                )

                return s.failAddPacket(packet, linkErr)
        }</span>

        // Also evaluate whether this HTLC would increase our fee exposure over
        // the threshold on the destination link. If it does, fail it back.
        <span class="cov8" title="1">if s.dustExceedsFeeThreshold(
                destination, packet.amount, false,
        ) </span><span class="cov8" title="1">{
                // The outgoing dust exceeds the threshold, so we fail the add
                // back.
                linkErr := NewLinkError(
                        &amp;lnwire.FailTemporaryChannelFailure{},
                )

                return s.failAddPacket(packet, linkErr)
        }</span>

        // Send the packet to the destination channel link which manages the
        // channel.
        <span class="cov8" title="1">packet.outgoingChanID = destination.ShortChanID()

        return destination.handleSwitchPacket(packet)</span>
}

// handlePacketSettle handles forwarding a settle packet.
func (s *Switch) handlePacketSettle(packet *htlcPacket) error <span class="cov8" title="1">{
        // If the source of this packet has not been set, use the circuit map
        // to lookup the origin.
        circuit, err := s.closeCircuit(packet)

        // If the circuit is in the process of closing, we will return a nil as
        // there's another packet handling undergoing.
        if errors.Is(err, ErrCircuitClosing) </span><span class="cov0" title="0">{
                log.Debugf("Circuit is closing for packet=%v", packet)
                return nil
        }</span>

        // Exit early if there's another error.
        <span class="cov8" title="1">if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // closeCircuit returns a nil circuit when a settle packet returns an
        // ErrUnknownCircuit error upon the inner call to CloseCircuit.
        //
        // NOTE: We can only get a nil circuit when it has already been deleted
        // and when `UpdateFulfillHTLC` is received. After which `RevokeAndAck`
        // is received, which invokes `processRemoteSettleFails` in its link.
        <span class="cov8" title="1">if circuit == nil </span><span class="cov8" title="1">{
                log.Debugf("Circuit already closed for packet=%v", packet)
                return nil
        }</span>

        <span class="cov8" title="1">localHTLC := packet.incomingChanID == hop.Source

        // If this is a locally initiated HTLC, we need to handle the packet by
        // storing the network result.
        //
        // A blank IncomingChanID in a circuit indicates that it is a pending
        // user-initiated payment.
        //
        // NOTE: `closeCircuit` modifies the state of `packet`.
        if localHTLC </span><span class="cov8" title="1">{
                // TODO(yy): remove the goroutine and send back the error here.
                s.wg.Add(1)
                go s.handleLocalResponse(packet)

                // If this is a locally initiated HTLC, there's no need to
                // forward it so we exit.
                return nil
        }</span>

        // If this is an HTLC settle, and it wasn't from a locally initiated
        // HTLC, then we'll log a forwarding event so we can flush it to disk
        // later.
        <span class="cov8" title="1">if circuit.Outgoing != nil </span><span class="cov8" title="1">{
                log.Infof("Forwarded HTLC(%x) of %v (fee: %v) "+
                        "from IncomingChanID(%v) to OutgoingChanID(%v)",
                        circuit.PaymentHash[:], circuit.OutgoingAmount,
                        circuit.IncomingAmount-circuit.OutgoingAmount,
                        circuit.Incoming.ChanID, circuit.Outgoing.ChanID)

                s.fwdEventMtx.Lock()
                s.pendingFwdingEvents = append(
                        s.pendingFwdingEvents,
                        channeldb.ForwardingEvent{
                                Timestamp:      time.Now(),
                                IncomingChanID: circuit.Incoming.ChanID,
                                OutgoingChanID: circuit.Outgoing.ChanID,
                                AmtIn:          circuit.IncomingAmount,
                                AmtOut:         circuit.OutgoingAmount,
                        },
                )
                s.fwdEventMtx.Unlock()
        }</span>

        // Deliver this packet.
        <span class="cov8" title="1">return s.mailOrchestrator.Deliver(packet.incomingChanID, packet)</span>
}

// handlePacketFail handles forwarding a fail packet.
func (s *Switch) handlePacketFail(packet *htlcPacket,
        htlc *lnwire.UpdateFailHTLC) error <span class="cov8" title="1">{

        // If the source of this packet has not been set, use the circuit map
        // to lookup the origin.
        circuit, err := s.closeCircuit(packet)
        if err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        // If this is a locally initiated HTLC, we need to handle the packet by
        // storing the network result.
        //
        // A blank IncomingChanID in a circuit indicates that it is a pending
        // user-initiated payment.
        //
        // NOTE: `closeCircuit` modifies the state of `packet`.
        <span class="cov8" title="1">if packet.incomingChanID == hop.Source </span><span class="cov8" title="1">{
                // TODO(yy): remove the goroutine and send back the error here.
                s.wg.Add(1)
                go s.handleLocalResponse(packet)

                // If this is a locally initiated HTLC, there's no need to
                // forward it so we exit.
                return nil
        }</span>

        // Exit early if this hasSource is true. This flag is only set via
        // mailbox's `FailAdd`. This method has two callsites,
        // - the packet has timed out after `MailboxDeliveryTimeout`, defaults
        //   to 1 min.
        // - the HTLC fails the validation in `channel.AddHTLC`.
        // In either case, the `Reason` field is populated. Thus there's no
        // need to proceed and extract the failure reason below.
        <span class="cov8" title="1">if packet.hasSource </span><span class="cov8" title="1">{
                // Deliver this packet.
                return s.mailOrchestrator.Deliver(packet.incomingChanID, packet)
        }</span>

        // HTLC resolutions and messages restored from disk don't have the
        // obfuscator set from the original htlc add packet - set it here for
        // use in blinded errors.
        <span class="cov8" title="1">packet.obfuscator = circuit.ErrorEncrypter

        switch </span>{
        // No message to encrypt, locally sourced payment.
        case circuit.ErrorEncrypter == nil:<span class="cov0" title="0"></span>
                // TODO(yy) further check this case as we shouldn't end up here
                // as `isLocal` is already false.

        // If this is a resolution message, then we'll need to encrypt it as
        // it's actually internally sourced.
        case packet.isResolution:<span class="cov0" title="0">
                var err error
                // TODO(roasbeef): don't need to pass actually?
                failure := &amp;lnwire.FailPermanentChannelFailure{}
                htlc.Reason, err = circuit.ErrorEncrypter.EncryptFirstHop(
                        failure,
                )
                if err != nil </span><span class="cov0" title="0">{
                        err = fmt.Errorf("unable to obfuscate error: %w", err)
                        log.Error(err)
                }</span>

        // Alternatively, if the remote party sends us an
        // UpdateFailMalformedHTLC, then we'll need to convert this into a
        // proper well formatted onion error as there's no HMAC currently.
        case packet.convertedError:<span class="cov8" title="1">
                log.Infof("Converting malformed HTLC error for circuit for "+
                        "Circuit(%x: (%s, %d) &lt;-&gt; (%s, %d))",
                        packet.circuit.PaymentHash,
                        packet.incomingChanID, packet.incomingHTLCID,
                        packet.outgoingChanID, packet.outgoingHTLCID)

                htlc.Reason = circuit.ErrorEncrypter.EncryptMalformedError(
                        htlc.Reason,
                )</span>

        default:<span class="cov8" title="1">
                // Otherwise, it's a forwarded error, so we'll perform a
                // wrapper encryption as normal.
                htlc.Reason = circuit.ErrorEncrypter.IntermediateEncrypt(
                        htlc.Reason,
                )</span>
        }

        // Deliver this packet.
        <span class="cov8" title="1">return s.mailOrchestrator.Deliver(packet.incomingChanID, packet)</span>
}
</pre>
		
		<pre class="file" id="file19" style="display: none">package htlcswitch

import (
        "bytes"
        "context"
        crand "crypto/rand"
        "crypto/sha256"
        "encoding/binary"
        "encoding/hex"
        "fmt"
        "net"
        "os"
        "runtime"
        "runtime/pprof"
        "sync/atomic"
        "testing"
        "time"

        "github.com/btcsuite/btcd/btcec/v2"
        "github.com/btcsuite/btcd/btcec/v2/ecdsa"
        "github.com/btcsuite/btcd/btcutil"
        "github.com/btcsuite/btcd/chaincfg/chainhash"
        "github.com/btcsuite/btcd/wire"
        "github.com/go-errors/errors"
        sphinx "github.com/lightningnetwork/lightning-onion"
        "github.com/lightningnetwork/lnd/channeldb"
        "github.com/lightningnetwork/lnd/contractcourt"
        "github.com/lightningnetwork/lnd/graph/db/models"
        "github.com/lightningnetwork/lnd/htlcswitch/hop"
        "github.com/lightningnetwork/lnd/input"
        "github.com/lightningnetwork/lnd/invoices"
        "github.com/lightningnetwork/lnd/keychain"
        "github.com/lightningnetwork/lnd/kvdb"
        "github.com/lightningnetwork/lnd/lnpeer"
        "github.com/lightningnetwork/lnd/lntest/channels"
        "github.com/lightningnetwork/lnd/lntest/wait"
        "github.com/lightningnetwork/lnd/lntypes"
        "github.com/lightningnetwork/lnd/lnwallet"
        "github.com/lightningnetwork/lnd/lnwallet/chainfee"
        "github.com/lightningnetwork/lnd/lnwire"
        "github.com/lightningnetwork/lnd/shachain"
        "github.com/lightningnetwork/lnd/ticker"
        "github.com/stretchr/testify/require"
)

// maxInflightHtlcs specifies the max number of inflight HTLCs. This number is
// chosen to be smaller than the default 483 so the test can run faster.
const maxInflightHtlcs = 50

var (
        alicePrivKey = []byte("alice priv key")
        bobPrivKey   = []byte("bob priv key")
        carolPrivKey = []byte("carol priv key")

        testRBytes, _ = hex.DecodeString("8ce2bc69281ce27da07e6683571319d18e949ddfa2965fb6caa1bf0314f882d7")
        testSBytes, _ = hex.DecodeString("299105481d63e0f4bc2a88121167221b6700d72a0ead154c03be696a292d24ae")
        testRScalar   = new(btcec.ModNScalar)
        testSScalar   = new(btcec.ModNScalar)
        _             = testRScalar.SetByteSlice(testRBytes)
        _             = testSScalar.SetByteSlice(testSBytes)
        testSig       = ecdsa.NewSignature(testRScalar, testSScalar)

        wireSig, _ = lnwire.NewSigFromSignature(testSig)

        testBatchTimeout = 50 * time.Millisecond
)

var idSeqNum uint64

// genID generates a unique tuple to identify a test channel.
func genID() (lnwire.ChannelID, lnwire.ShortChannelID) <span class="cov8" title="1">{
        id := atomic.AddUint64(&amp;idSeqNum, 1)

        var scratch [8]byte

        binary.BigEndian.PutUint64(scratch[:], id)
        hash1, _ := chainhash.NewHash(bytes.Repeat(scratch[:], 4))

        chanPoint1 := wire.NewOutPoint(hash1, uint32(id))
        chanID1 := lnwire.NewChanIDFromOutPoint(*chanPoint1)
        aliceChanID := lnwire.NewShortChanIDFromInt(id)

        return chanID1, aliceChanID
}</span>

// genIDs generates ids for two test channels.
func genIDs() (lnwire.ChannelID, lnwire.ChannelID, lnwire.ShortChannelID,
        lnwire.ShortChannelID) <span class="cov8" title="1">{

        chanID1, aliceChanID := genID()
        chanID2, bobChanID := genID()

        return chanID1, chanID2, aliceChanID, bobChanID
}</span>

// mockGetChanUpdateMessage helper function which returns topology update of
// the channel
func mockGetChanUpdateMessage(_ lnwire.ShortChannelID) (*lnwire.ChannelUpdate1,
        error) <span class="cov8" title="1">{

        return &amp;lnwire.ChannelUpdate1{
                Signature: wireSig,
        }, nil
}</span>

// generateRandomBytes returns securely generated random bytes.
// It will return an error if the system's secure random
// number generator fails to function correctly, in which
// case the caller should not continue.
func generateRandomBytes(n int) ([]byte, error) <span class="cov8" title="1">{
        b := make([]byte, n)

        // TODO(roasbeef): should use counter in tests (atomic) rather than
        // this

        _, err := crand.Read(b)
        // Note that Err == nil only if we read len(b) bytes.
        if err != nil </span><span class="cov0" title="0">{
                return nil, err
        }</span>

        <span class="cov8" title="1">return b, nil</span>
}

type testLightningChannel struct {
        channel *lnwallet.LightningChannel
        restore func() (*lnwallet.LightningChannel, error)
}

// createTestChannel creates the channel and returns our and remote channels
// representations.
//
// TODO(roasbeef): need to factor out, similar func re-used in many parts of codebase
func createTestChannel(t *testing.T, alicePrivKey, bobPrivKey []byte,
        aliceAmount, bobAmount, aliceReserve, bobReserve btcutil.Amount,
        chanID lnwire.ShortChannelID) (*testLightningChannel,
        *testLightningChannel, error) <span class="cov8" title="1">{

        aliceKeyPriv, aliceKeyPub := btcec.PrivKeyFromBytes(alicePrivKey)
        bobKeyPriv, bobKeyPub := btcec.PrivKeyFromBytes(bobPrivKey)

        channelCapacity := aliceAmount + bobAmount
        csvTimeoutAlice := uint32(5)
        csvTimeoutBob := uint32(4)
        isAliceInitiator := true

        aliceBounds := channeldb.ChannelStateBounds{
                MaxPendingAmount: lnwire.NewMSatFromSatoshis(
                        channelCapacity),
                ChanReserve:      aliceReserve,
                MinHTLC:          0,
                MaxAcceptedHtlcs: maxInflightHtlcs,
        }
        aliceCommitParams := channeldb.CommitmentParams{
                DustLimit: btcutil.Amount(200),
                CsvDelay:  uint16(csvTimeoutAlice),
        }

        bobBounds := channeldb.ChannelStateBounds{
                MaxPendingAmount: lnwire.NewMSatFromSatoshis(
                        channelCapacity),
                ChanReserve:      bobReserve,
                MinHTLC:          0,
                MaxAcceptedHtlcs: maxInflightHtlcs,
        }
        bobCommitParams := channeldb.CommitmentParams{
                DustLimit: btcutil.Amount(800),
                CsvDelay:  uint16(csvTimeoutBob),
        }

        var hash [sha256.Size]byte
        randomSeed, err := generateRandomBytes(sha256.Size)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">copy(hash[:], randomSeed)

        prevOut := &amp;wire.OutPoint{
                Hash:  chainhash.Hash(hash),
                Index: 0,
        }
        fundingTxIn := wire.NewTxIn(prevOut, nil, nil)

        aliceCfg := channeldb.ChannelConfig{
                ChannelStateBounds: aliceBounds,
                CommitmentParams:   aliceCommitParams,
                MultiSigKey: keychain.KeyDescriptor{
                        PubKey: aliceKeyPub,
                },
                RevocationBasePoint: keychain.KeyDescriptor{
                        PubKey: aliceKeyPub,
                },
                PaymentBasePoint: keychain.KeyDescriptor{
                        PubKey: aliceKeyPub,
                },
                DelayBasePoint: keychain.KeyDescriptor{
                        PubKey: aliceKeyPub,
                },
                HtlcBasePoint: keychain.KeyDescriptor{
                        PubKey: aliceKeyPub,
                },
        }
        bobCfg := channeldb.ChannelConfig{
                ChannelStateBounds: bobBounds,
                CommitmentParams:   bobCommitParams,
                MultiSigKey: keychain.KeyDescriptor{
                        PubKey: bobKeyPub,
                },
                RevocationBasePoint: keychain.KeyDescriptor{
                        PubKey: bobKeyPub,
                },
                PaymentBasePoint: keychain.KeyDescriptor{
                        PubKey: bobKeyPub,
                },
                DelayBasePoint: keychain.KeyDescriptor{
                        PubKey: bobKeyPub,
                },
                HtlcBasePoint: keychain.KeyDescriptor{
                        PubKey: bobKeyPub,
                },
        }

        bobRoot, err := chainhash.NewHash(bobKeyPriv.Serialize())
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">bobPreimageProducer := shachain.NewRevocationProducer(*bobRoot)
        bobFirstRevoke, err := bobPreimageProducer.AtIndex(0)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">bobCommitPoint := input.ComputeCommitmentPoint(bobFirstRevoke[:])

        aliceRoot, err := chainhash.NewHash(aliceKeyPriv.Serialize())
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">alicePreimageProducer := shachain.NewRevocationProducer(*aliceRoot)
        aliceFirstRevoke, err := alicePreimageProducer.AtIndex(0)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">aliceCommitPoint := input.ComputeCommitmentPoint(aliceFirstRevoke[:])

        aliceCommitTx, bobCommitTx, err := lnwallet.CreateCommitmentTxns(
                aliceAmount, bobAmount, &amp;aliceCfg, &amp;bobCfg, aliceCommitPoint,
                bobCommitPoint, *fundingTxIn, channeldb.SingleFunderTweaklessBit,
                isAliceInitiator, 0,
        )
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>

        <span class="cov8" title="1">dbAlice := channeldb.OpenForTesting(t, t.TempDir())
        dbBob := channeldb.OpenForTesting(t, t.TempDir())

        estimator := chainfee.NewStaticEstimator(6000, 0)
        feePerKw, err := estimator.EstimateFeePerKW(1)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">commitFee := feePerKw.FeeForWeight(724)

        const broadcastHeight = 1
        bobAddr := &amp;net.TCPAddr{
                IP:   net.ParseIP("127.0.0.1"),
                Port: 18555,
        }

        aliceAddr := &amp;net.TCPAddr{
                IP:   net.ParseIP("127.0.0.1"),
                Port: 18556,
        }

        aliceCommit := channeldb.ChannelCommitment{
                CommitHeight:  0,
                LocalBalance:  lnwire.NewMSatFromSatoshis(aliceAmount - commitFee),
                RemoteBalance: lnwire.NewMSatFromSatoshis(bobAmount),
                CommitFee:     commitFee,
                FeePerKw:      btcutil.Amount(feePerKw),
                CommitTx:      aliceCommitTx,
                CommitSig:     bytes.Repeat([]byte{1}, 71),
        }
        bobCommit := channeldb.ChannelCommitment{
                CommitHeight:  0,
                LocalBalance:  lnwire.NewMSatFromSatoshis(bobAmount),
                RemoteBalance: lnwire.NewMSatFromSatoshis(aliceAmount - commitFee),
                CommitFee:     commitFee,
                FeePerKw:      btcutil.Amount(feePerKw),
                CommitTx:      bobCommitTx,
                CommitSig:     bytes.Repeat([]byte{1}, 71),
        }

        aliceChannelState := &amp;channeldb.OpenChannel{
                LocalChanCfg:            aliceCfg,
                RemoteChanCfg:           bobCfg,
                IdentityPub:             aliceKeyPub,
                FundingOutpoint:         *prevOut,
                ChanType:                channeldb.SingleFunderTweaklessBit,
                IsInitiator:             isAliceInitiator,
                Capacity:                channelCapacity,
                RemoteCurrentRevocation: bobCommitPoint,
                RevocationProducer:      alicePreimageProducer,
                RevocationStore:         shachain.NewRevocationStore(),
                LocalCommitment:         aliceCommit,
                RemoteCommitment:        aliceCommit,
                ShortChannelID:          chanID,
                Db:                      dbAlice.ChannelStateDB(),
                Packager:                channeldb.NewChannelPackager(chanID),
                FundingTxn:              channels.TestFundingTx,
        }

        bobChannelState := &amp;channeldb.OpenChannel{
                LocalChanCfg:            bobCfg,
                RemoteChanCfg:           aliceCfg,
                IdentityPub:             bobKeyPub,
                FundingOutpoint:         *prevOut,
                ChanType:                channeldb.SingleFunderTweaklessBit,
                IsInitiator:             !isAliceInitiator,
                Capacity:                channelCapacity,
                RemoteCurrentRevocation: aliceCommitPoint,
                RevocationProducer:      bobPreimageProducer,
                RevocationStore:         shachain.NewRevocationStore(),
                LocalCommitment:         bobCommit,
                RemoteCommitment:        bobCommit,
                ShortChannelID:          chanID,
                Db:                      dbBob.ChannelStateDB(),
                Packager:                channeldb.NewChannelPackager(chanID),
        }

        if err := aliceChannelState.SyncPending(bobAddr, broadcastHeight); err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>

        <span class="cov8" title="1">if err := bobChannelState.SyncPending(aliceAddr, broadcastHeight); err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>

        <span class="cov8" title="1">aliceSigner := input.NewMockSigner(
                []*btcec.PrivateKey{aliceKeyPriv}, nil,
        )
        bobSigner := input.NewMockSigner(
                []*btcec.PrivateKey{bobKeyPriv}, nil,
        )

        alicePool := lnwallet.NewSigPool(runtime.NumCPU(), aliceSigner)
        signerMock := lnwallet.NewDefaultAuxSignerMock(t)
        channelAlice, err := lnwallet.NewLightningChannel(
                aliceSigner, aliceChannelState, alicePool,
                lnwallet.WithLeafStore(&amp;lnwallet.MockAuxLeafStore{}),
                lnwallet.WithAuxSigner(signerMock),
        )
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">alicePool.Start()

        bobPool := lnwallet.NewSigPool(runtime.NumCPU(), bobSigner)
        channelBob, err := lnwallet.NewLightningChannel(
                bobSigner, bobChannelState, bobPool,
                lnwallet.WithLeafStore(&amp;lnwallet.MockAuxLeafStore{}),
                lnwallet.WithAuxSigner(signerMock),
        )
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">bobPool.Start()

        // Now that the channel are open, simulate the start of a session by
        // having Alice and Bob extend their revocation windows to each other.
        aliceNextRevoke, err := channelAlice.NextRevocationKey()
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">if err := channelBob.InitNextRevocation(aliceNextRevoke); err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>

        <span class="cov8" title="1">bobNextRevoke, err := channelBob.NextRevocationKey()
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>
        <span class="cov8" title="1">if err := channelAlice.InitNextRevocation(bobNextRevoke); err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>

        <span class="cov8" title="1">restoreAlice := func() (*lnwallet.LightningChannel, error) </span><span class="cov8" title="1">{
                aliceStoredChannels, err := dbAlice.ChannelStateDB().
                        FetchOpenChannels(aliceKeyPub)
                switch err </span>{
                case nil:<span class="cov8" title="1"></span>
                case kvdb.ErrDatabaseNotOpen:<span class="cov0" title="0">
                        dbAlice = channeldb.OpenForTesting(t, dbAlice.Path())

                        aliceStoredChannels, err = dbAlice.ChannelStateDB().
                                FetchOpenChannels(aliceKeyPub)
                        if err != nil </span><span class="cov0" title="0">{
                                return nil, errors.Errorf("unable to fetch alice "+
                                        "channel: %v", err)
                        }</span>
                default:<span class="cov0" title="0">
                        return nil, errors.Errorf("unable to fetch alice channel: "+
                                "%v", err)</span>
                }

                <span class="cov8" title="1">var aliceStoredChannel *channeldb.OpenChannel
                for _, channel := range aliceStoredChannels </span><span class="cov8" title="1">{
                        if channel.FundingOutpoint.String() == prevOut.String() </span><span class="cov8" title="1">{
                                aliceStoredChannel = channel
                                break</span>
                        }
                }

                <span class="cov8" title="1">if aliceStoredChannel == nil </span><span class="cov0" title="0">{
                        return nil, errors.New("unable to find stored alice channel")
                }</span>

                <span class="cov8" title="1">newAliceChannel, err := lnwallet.NewLightningChannel(
                        aliceSigner, aliceStoredChannel, alicePool,
                        lnwallet.WithLeafStore(&amp;lnwallet.MockAuxLeafStore{}),
                        lnwallet.WithAuxSigner(signerMock),
                )
                if err != nil </span><span class="cov0" title="0">{
                        return nil, errors.Errorf("unable to create new channel: %v",
                                err)
                }</span>

                <span class="cov8" title="1">return newAliceChannel, nil</span>
        }

        <span class="cov8" title="1">restoreBob := func() (*lnwallet.LightningChannel, error) </span><span class="cov8" title="1">{
                bobStoredChannels, err := dbBob.ChannelStateDB().
                        FetchOpenChannels(bobKeyPub)
                switch err </span>{
                case nil:<span class="cov8" title="1"></span>
                case kvdb.ErrDatabaseNotOpen:<span class="cov0" title="0">
                        dbBob = channeldb.OpenForTesting(t, dbBob.Path())
                        if err != nil </span><span class="cov0" title="0">{
                                return nil, errors.Errorf("unable to reopen bob "+
                                        "db: %v", err)
                        }</span>

                        <span class="cov0" title="0">bobStoredChannels, err = dbBob.ChannelStateDB().
                                FetchOpenChannels(bobKeyPub)
                        if err != nil </span><span class="cov0" title="0">{
                                return nil, errors.Errorf("unable to fetch bob "+
                                        "channel: %v", err)
                        }</span>
                default:<span class="cov0" title="0">
                        return nil, errors.Errorf("unable to fetch bob channel: "+
                                "%v", err)</span>
                }

                <span class="cov8" title="1">var bobStoredChannel *channeldb.OpenChannel
                for _, channel := range bobStoredChannels </span><span class="cov8" title="1">{
                        if channel.FundingOutpoint.String() == prevOut.String() </span><span class="cov8" title="1">{
                                bobStoredChannel = channel
                                break</span>
                        }
                }

                <span class="cov8" title="1">if bobStoredChannel == nil </span><span class="cov0" title="0">{
                        return nil, errors.New("unable to find stored bob channel")
                }</span>

                <span class="cov8" title="1">newBobChannel, err := lnwallet.NewLightningChannel(
                        bobSigner, bobStoredChannel, bobPool,
                        lnwallet.WithLeafStore(&amp;lnwallet.MockAuxLeafStore{}),
                        lnwallet.WithAuxSigner(signerMock),
                )
                if err != nil </span><span class="cov0" title="0">{
                        return nil, errors.Errorf("unable to create new channel: %v",
                                err)
                }</span>
                <span class="cov8" title="1">return newBobChannel, nil</span>
        }

        <span class="cov8" title="1">testLightningChannelAlice := &amp;testLightningChannel{
                channel: channelAlice,
                restore: restoreAlice,
        }

        testLightningChannelBob := &amp;testLightningChannel{
                channel: channelBob,
                restore: restoreBob,
        }

        return testLightningChannelAlice, testLightningChannelBob, nil</span>
}

// getChanID retrieves the channel point from an lnnwire message.
func getChanID(msg lnwire.Message) (lnwire.ChannelID, error) <span class="cov8" title="1">{
        var chanID lnwire.ChannelID
        switch msg := msg.(type) </span>{
        case *lnwire.UpdateAddHTLC:<span class="cov8" title="1">
                chanID = msg.ChanID</span>
        case *lnwire.UpdateFulfillHTLC:<span class="cov8" title="1">
                chanID = msg.ChanID</span>
        case *lnwire.UpdateFailHTLC:<span class="cov0" title="0">
                chanID = msg.ChanID</span>
        case *lnwire.RevokeAndAck:<span class="cov8" title="1">
                chanID = msg.ChanID</span>
        case *lnwire.CommitSig:<span class="cov8" title="1">
                chanID = msg.ChanID</span>
        case *lnwire.ChannelReestablish:<span class="cov8" title="1">
                chanID = msg.ChanID</span>
        case *lnwire.ChannelReady:<span class="cov8" title="1">
                chanID = msg.ChanID</span>
        case *lnwire.UpdateFee:<span class="cov8" title="1">
                chanID = msg.ChanID</span>
        default:<span class="cov0" title="0">
                return chanID, fmt.Errorf("unknown type: %T", msg)</span>
        }

        <span class="cov8" title="1">return chanID, nil</span>
}

// generateHoldPayment generates the htlc add request by given path blob and
// invoice which should be added by destination peer.
func generatePaymentWithPreimage(invoiceAmt, htlcAmt lnwire.MilliSatoshi,
        timelock uint32, blob [lnwire.OnionPacketSize]byte,
        preimage *lntypes.Preimage, rhash, payAddr [32]byte) (
        *invoices.Invoice, *lnwire.UpdateAddHTLC, uint64, error) <span class="cov8" title="1">{

        // Create the db invoice. Normally the payment requests needs to be set,
        // because it is decoded in InvoiceRegistry to obtain the cltv expiry.
        // But because the mock registry used in tests is mocking the decode
        // step and always returning the value of testInvoiceCltvExpiry, we
        // don't need to bother here with creating and signing a payment
        // request.

        invoice := &amp;invoices.Invoice{
                CreationDate: time.Now(),
                Terms: invoices.ContractTerm{
                        FinalCltvDelta:  testInvoiceCltvExpiry,
                        Value:           invoiceAmt,
                        PaymentPreimage: preimage,
                        PaymentAddr:     payAddr,
                        Features: lnwire.NewFeatureVector(
                                nil, lnwire.Features,
                        ),
                },
                HodlInvoice: preimage == nil,
        }

        htlc := &amp;lnwire.UpdateAddHTLC{
                PaymentHash: rhash,
                Amount:      htlcAmt,
                Expiry:      timelock,
                OnionBlob:   blob,
        }

        pid, err := generateRandomBytes(8)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, 0, err
        }</span>
        <span class="cov8" title="1">paymentID := binary.BigEndian.Uint64(pid)

        return invoice, htlc, paymentID, nil</span>
}

// generatePayment generates the htlc add request by given path blob and
// invoice which should be added by destination peer.
func generatePayment(invoiceAmt, htlcAmt lnwire.MilliSatoshi, timelock uint32,
        blob [lnwire.OnionPacketSize]byte) (*invoices.Invoice,
        *lnwire.UpdateAddHTLC, uint64, error) <span class="cov8" title="1">{

        var preimage lntypes.Preimage
        r, err := generateRandomBytes(sha256.Size)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, 0, err
        }</span>
        <span class="cov8" title="1">copy(preimage[:], r)

        rhash := sha256.Sum256(preimage[:])

        var payAddr [sha256.Size]byte
        r, err = generateRandomBytes(sha256.Size)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, 0, err
        }</span>
        <span class="cov8" title="1">copy(payAddr[:], r)

        return generatePaymentWithPreimage(
                invoiceAmt, htlcAmt, timelock, blob, &amp;preimage, rhash, payAddr,
        )</span>
}

// generateRoute generates the path blob by given array of peers.
func generateRoute(hops ...*hop.Payload) (
        [lnwire.OnionPacketSize]byte, error) <span class="cov8" title="1">{

        var blob [lnwire.OnionPacketSize]byte
        if len(hops) == 0 </span><span class="cov0" title="0">{
                return blob, errors.New("empty path")
        }</span>

        <span class="cov8" title="1">iterator := newMockHopIterator(hops...)

        w := bytes.NewBuffer(blob[0:0])
        if err := iterator.EncodeNextHop(w); err != nil </span><span class="cov0" title="0">{
                return blob, err
        }</span>

        <span class="cov8" title="1">return blob, nil</span>

}

// threeHopNetwork is used for managing the created cluster of 3 hops.
type threeHopNetwork struct {
        aliceServer       *mockServer
        aliceChannelLink  *channelLink
        aliceOnionDecoder *mockIteratorDecoder

        bobServer            *mockServer
        firstBobChannelLink  *channelLink
        secondBobChannelLink *channelLink
        bobOnionDecoder      *mockIteratorDecoder

        carolServer       *mockServer
        carolChannelLink  *channelLink
        carolOnionDecoder *mockIteratorDecoder

        hopNetwork
}

// generateHops creates the per hop payload, the total amount to be sent, and
// also the time lock value needed to route an HTLC with the target amount over
// the specified path.
func generateHops(payAmt lnwire.MilliSatoshi, startingHeight uint32,
        path ...*channelLink) (lnwire.MilliSatoshi, uint32, []*hop.Payload) <span class="cov8" title="1">{

        totalTimelock := startingHeight
        runningAmt := payAmt

        hops := make([]*hop.Payload, len(path))
        for i := len(path) - 1; i &gt;= 0; i-- </span><span class="cov8" title="1">{
                // If this is the last hop, then the next hop is the special
                // "exit node". Otherwise, we look to the "prior" hop.
                nextHop := hop.Exit
                if i != len(path)-1 </span><span class="cov8" title="1">{
                        nextHop = path[i+1].channel.ShortChanID()
                }</span>

                <span class="cov8" title="1">var timeLock uint32
                // If this is the last, hop, then the time lock will be their
                // specified delta policy plus our starting height.
                if i == len(path)-1 </span><span class="cov8" title="1">{
                        totalTimelock += testInvoiceCltvExpiry
                        timeLock = totalTimelock
                }</span> else<span class="cov8" title="1"> {
                        // Otherwise, the outgoing time lock should be the
                        // incoming timelock minus their specified delta.
                        delta := path[i+1].cfg.FwrdingPolicy.TimeLockDelta
                        totalTimelock += delta
                        timeLock = totalTimelock - delta
                }</span>

                // Finally, we'll need to calculate the amount to forward. For
                // the last hop, it's just the payment amount.
                <span class="cov8" title="1">amount := payAmt
                if i != len(path)-1 </span><span class="cov8" title="1">{
                        prevHop := hops[i+1]
                        prevAmount := prevHop.ForwardingInfo().AmountToForward

                        fee := ExpectedFee(path[i].cfg.FwrdingPolicy, prevAmount)
                        runningAmt += fee

                        // Otherwise, for a node to forward an HTLC, then
                        // following inequality most hold true:
                        //     * amt_in - fee &gt;= amt_to_forward
                        amount = runningAmt - fee
                }</span>

                <span class="cov8" title="1">var nextHopBytes [8]byte
                binary.BigEndian.PutUint64(nextHopBytes[:], nextHop.ToUint64())

                hops[i] = hop.NewLegacyPayload(&amp;sphinx.HopData{
                        Realm:         [1]byte{}, // hop.BitcoinNetwork
                        NextAddress:   nextHopBytes,
                        ForwardAmount: uint64(amount),
                        OutgoingCltv:  timeLock,
                })</span>
        }

        <span class="cov8" title="1">return runningAmt, totalTimelock, hops</span>
}

type paymentResponse struct {
        rhash lntypes.Hash
        err   chan error
}

func (r *paymentResponse) Wait(d time.Duration) (lntypes.Hash, error) <span class="cov8" title="1">{
        return r.rhash, waitForPaymentResult(r.err, d)
}</span>

// waitForPaymentResult waits for either an error to be received on c or a
// timeout.
func waitForPaymentResult(c chan error, d time.Duration) error <span class="cov8" title="1">{
        select </span>{
        case err := &lt;-c:<span class="cov8" title="1">
                close(c)
                return err</span>
        case &lt;-time.After(d):<span class="cov8" title="1">
                return errors.New("htlc was not settled in time")</span>
        }
}

// waitForPayFuncResult executes the given function and waits for a result with
// a timeout.
func waitForPayFuncResult(payFunc func() error, d time.Duration) error <span class="cov8" title="1">{
        errChan := make(chan error)
        go func() </span><span class="cov8" title="1">{
                errChan &lt;- payFunc()
        }</span>()

        <span class="cov8" title="1">return waitForPaymentResult(errChan, d)</span>
}

// makePayment takes the destination node and amount as input, sends the
// payment and returns the error channel to wait for error to be received and
// invoice in order to check its status after the payment finished.
//
// With this function you can send payments:
// * from Alice to Bob
// * from Alice to Carol through the Bob
// * from Alice to some another peer through the Bob
func makePayment(sendingPeer, receivingPeer lnpeer.Peer,
        firstHop lnwire.ShortChannelID, hops []*hop.Payload,
        invoiceAmt, htlcAmt lnwire.MilliSatoshi,
        timelock uint32) *paymentResponse <span class="cov8" title="1">{

        paymentErr := make(chan error, 1)
        var rhash lntypes.Hash

        invoice, payFunc, err := preparePayment(sendingPeer, receivingPeer,
                firstHop, hops, invoiceAmt, htlcAmt, timelock,
        )
        if err != nil </span><span class="cov0" title="0">{
                paymentErr &lt;- err
                return &amp;paymentResponse{
                        rhash: rhash,
                        err:   paymentErr,
                }
        }</span>

        <span class="cov8" title="1">rhash = invoice.Terms.PaymentPreimage.Hash()

        // Send payment and expose err channel.
        go func() </span><span class="cov8" title="1">{
                paymentErr &lt;- payFunc()
        }</span>()

        <span class="cov8" title="1">return &amp;paymentResponse{
                rhash: rhash,
                err:   paymentErr,
        }</span>
}

// preparePayment creates an invoice at the receivingPeer and returns a function
// that, when called, launches the payment from the sendingPeer.
func preparePayment(sendingPeer, receivingPeer lnpeer.Peer,
        firstHop lnwire.ShortChannelID, hops []*hop.Payload,
        invoiceAmt, htlcAmt lnwire.MilliSatoshi,
        timelock uint32) (*invoices.Invoice, func() error, error) <span class="cov8" title="1">{

        sender := sendingPeer.(*mockServer)
        receiver := receivingPeer.(*mockServer)

        // Generate route convert it to blob, and return next destination for
        // htlc add request.
        blob, err := generateRoute(hops...)
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>

        // Generate payment: invoice and htlc.
        <span class="cov8" title="1">invoice, htlc, pid, err := generatePayment(
                invoiceAmt, htlcAmt, timelock, blob,
        )
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>

        // Check who is last in the route and add invoice to server registry.
        <span class="cov8" title="1">hash := invoice.Terms.PaymentPreimage.Hash()
        if err := receiver.registry.AddInvoice(
                context.Background(), *invoice, hash,
        ); err != nil </span><span class="cov0" title="0">{
                return nil, nil, err
        }</span>

        // Send payment and expose err channel.
        <span class="cov8" title="1">return invoice, func() error </span><span class="cov8" title="1">{
                err := sender.htlcSwitch.SendHTLC(
                        firstHop, pid, htlc,
                )
                if err != nil </span><span class="cov8" title="1">{
                        return err
                }</span>
                <span class="cov8" title="1">resultChan, err := sender.htlcSwitch.GetAttemptResult(
                        pid, hash, newMockDeobfuscator(),
                )
                if err != nil </span><span class="cov0" title="0">{
                        return err
                }</span>

                <span class="cov8" title="1">result, ok := &lt;-resultChan
                if !ok </span><span class="cov8" title="1">{
                        return fmt.Errorf("shutting down")
                }</span>

                <span class="cov8" title="1">if result.Error != nil </span><span class="cov8" title="1">{
                        return result.Error
                }</span>

                <span class="cov8" title="1">return nil</span>
        }, nil
}

// start starts the three hop network alice,bob,carol servers.
func (n *threeHopNetwork) start() error <span class="cov8" title="1">{
        if err := n.aliceServer.Start(); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if err := n.bobServer.Start(); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if err := n.carolServer.Start(); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>

        <span class="cov8" title="1">return waitLinksEligible(map[string]*channelLink{
                "alice":      n.aliceChannelLink,
                "bob first":  n.firstBobChannelLink,
                "bob second": n.secondBobChannelLink,
                "carol":      n.carolChannelLink,
        })</span>
}

// stop stops nodes and cleanup its databases.
func (n *threeHopNetwork) stop() <span class="cov8" title="1">{
        done := make(chan struct{})
        go func() </span><span class="cov8" title="1">{
                n.aliceServer.Stop()
                done &lt;- struct{}{}
        }</span>()

        <span class="cov8" title="1">go func() </span><span class="cov8" title="1">{
                n.bobServer.Stop()
                done &lt;- struct{}{}
        }</span>()

        <span class="cov8" title="1">go func() </span><span class="cov8" title="1">{
                n.carolServer.Stop()
                done &lt;- struct{}{}
        }</span>()

        <span class="cov8" title="1">for i := 0; i &lt; 3; i++ </span><span class="cov8" title="1">{
                &lt;-done
        }</span>
}

type clusterChannels struct {
        aliceToBob *lnwallet.LightningChannel
        bobToAlice *lnwallet.LightningChannel
        bobToCarol *lnwallet.LightningChannel
        carolToBob *lnwallet.LightningChannel
}

// createClusterChannels creates lightning channels which are needed for
// network cluster to be initialized.
func createClusterChannels(t *testing.T, aliceToBob, bobToCarol btcutil.Amount) (
        *clusterChannels, func() (*clusterChannels, error), error) <span class="cov8" title="1">{

        _, _, firstChanID, secondChanID := genIDs()

        // Create lightning channels between Alice&lt;-&gt;Bob and Bob&lt;-&gt;Carol
        aliceChannel, firstBobChannel, err := createTestChannel(t, alicePrivKey,
                bobPrivKey, aliceToBob, aliceToBob, 0, 0, firstChanID,
        )
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, errors.Errorf("unable to create "+
                        "alice&lt;-&gt;bob channel: %v", err)
        }</span>

        <span class="cov8" title="1">secondBobChannel, carolChannel, err := createTestChannel(t, bobPrivKey,
                carolPrivKey, bobToCarol, bobToCarol, 0, 0, secondChanID,
        )
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, errors.Errorf("unable to create "+
                        "bob&lt;-&gt;carol channel: %v", err)
        }</span>

        <span class="cov8" title="1">restoreFromDb := func() (*clusterChannels, error) </span><span class="cov8" title="1">{

                a2b, err := aliceChannel.restore()
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>

                <span class="cov8" title="1">b2a, err := firstBobChannel.restore()
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>

                <span class="cov8" title="1">b2c, err := secondBobChannel.restore()
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>

                <span class="cov8" title="1">c2b, err := carolChannel.restore()
                if err != nil </span><span class="cov0" title="0">{
                        return nil, err
                }</span>

                <span class="cov8" title="1">return &amp;clusterChannels{
                        aliceToBob: a2b,
                        bobToAlice: b2a,
                        bobToCarol: b2c,
                        carolToBob: c2b,
                }, nil</span>
        }

        <span class="cov8" title="1">return &amp;clusterChannels{
                aliceToBob: aliceChannel.channel,
                bobToAlice: firstBobChannel.channel,
                bobToCarol: secondBobChannel.channel,
                carolToBob: carolChannel.channel,
        }, restoreFromDb, nil</span>
}

// newThreeHopNetwork function creates the following topology and returns the
// control object to manage this cluster:
//
// alice                      bob                             carol
// server - &lt;-connection-&gt; - server - - &lt;-connection-&gt; - - - server
//
//        |                           |                               |
//
// alice htlc                     bob htlc                          carol htlc
// switch                      switch        \                    switch
//
//        |                         |       \                       |
//        |                         |        \                       |
//
// alice                   first bob     second bob           carol
// channel link                      channel link   channel link      channel link
//
// This function takes server options which can be used to apply custom
// settings to alice, bob and carol.
func newThreeHopNetwork(t testing.TB, aliceChannel, firstBobChannel,
        secondBobChannel, carolChannel *lnwallet.LightningChannel,
        startingHeight uint32, opts ...serverOption) *threeHopNetwork <span class="cov8" title="1">{

        aliceDb := aliceChannel.State().Db.GetParentDB()
        bobDb := firstBobChannel.State().Db.GetParentDB()
        carolDb := carolChannel.State().Db.GetParentDB()

        hopNetwork := newHopNetwork()

        // Create three peers/servers.
        aliceServer, err := newMockServer(
                t, "alice", startingHeight, aliceDb, hopNetwork.defaultDelta,
        )
        require.NoError(t, err, "unable to create alice server")
        bobServer, err := newMockServer(
                t, "bob", startingHeight, bobDb, hopNetwork.defaultDelta,
        )
        require.NoError(t, err, "unable to create bob server")
        carolServer, err := newMockServer(
                t, "carol", startingHeight, carolDb, hopNetwork.defaultDelta,
        )
        require.NoError(t, err, "unable to create carol server")

        // Apply all additional functional options to the servers before
        // creating any links.
        for _, option := range opts </span><span class="cov8" title="1">{
                option(aliceServer, bobServer, carolServer)
        }</span>

        // Create mock decoder instead of sphinx one in order to mock the route
        // which htlc should follow.
        <span class="cov8" title="1">aliceDecoder := newMockIteratorDecoder()
        bobDecoder := newMockIteratorDecoder()
        carolDecoder := newMockIteratorDecoder()

        aliceChannelLink, err := hopNetwork.createChannelLink(aliceServer,
                bobServer, aliceChannel, aliceDecoder,
        )
        if err != nil </span><span class="cov0" title="0">{
                t.Fatal(err)
        }</span>

        <span class="cov8" title="1">firstBobChannelLink, err := hopNetwork.createChannelLink(bobServer,
                aliceServer, firstBobChannel, bobDecoder)
        if err != nil </span><span class="cov0" title="0">{
                t.Fatal(err)
        }</span>

        <span class="cov8" title="1">secondBobChannelLink, err := hopNetwork.createChannelLink(bobServer,
                carolServer, secondBobChannel, bobDecoder)
        if err != nil </span><span class="cov0" title="0">{
                t.Fatal(err)
        }</span>

        <span class="cov8" title="1">carolChannelLink, err := hopNetwork.createChannelLink(carolServer,
                bobServer, carolChannel, carolDecoder)
        if err != nil </span><span class="cov0" title="0">{
                t.Fatal(err)
        }</span>

        <span class="cov8" title="1">return &amp;threeHopNetwork{
                aliceServer:       aliceServer,
                aliceChannelLink:  aliceChannelLink.(*channelLink),
                aliceOnionDecoder: aliceDecoder,

                bobServer:            bobServer,
                firstBobChannelLink:  firstBobChannelLink.(*channelLink),
                secondBobChannelLink: secondBobChannelLink.(*channelLink),
                bobOnionDecoder:      bobDecoder,

                carolServer:       carolServer,
                carolChannelLink:  carolChannelLink.(*channelLink),
                carolOnionDecoder: carolDecoder,

                hopNetwork: *hopNetwork,
        }</span>
}

// serverOption is a function which alters the three servers created for
// a three hop network to allow custom settings on each server.
type serverOption func(aliceServer, bobServer, carolServer *mockServer)

// serverOptionWithHtlcNotifier is a functional option for the creation of
// three hop network servers which allows setting of htlc notifiers.
// Note that these notifiers should be started and stopped by the calling
// function.
func serverOptionWithHtlcNotifier(alice, bob,
        carol *HtlcNotifier) serverOption <span class="cov8" title="1">{

        return func(aliceServer, bobServer, carolServer *mockServer) </span><span class="cov8" title="1">{
                aliceServer.htlcSwitch.cfg.HtlcNotifier = alice
                bobServer.htlcSwitch.cfg.HtlcNotifier = bob
                carolServer.htlcSwitch.cfg.HtlcNotifier = carol
        }</span>
}

// serverOptionRejectHtlc is the functional option for setting the reject
// htlc config option in each server's switch.
func serverOptionRejectHtlc(alice, bob, carol bool) serverOption <span class="cov8" title="1">{
        return func(aliceServer, bobServer, carolServer *mockServer) </span><span class="cov8" title="1">{
                aliceServer.htlcSwitch.cfg.RejectHTLC = alice
                bobServer.htlcSwitch.cfg.RejectHTLC = bob
                carolServer.htlcSwitch.cfg.RejectHTLC = carol
        }</span>
}

// createMirroredChannel creates two LightningChannel objects which represent
// the state machines on either side of a single channel between alice and bob.
func createMirroredChannel(t *testing.T, aliceToBob,
        bobToAlice btcutil.Amount) (*testLightningChannel,
        *testLightningChannel, error) <span class="cov8" title="1">{

        _, _, firstChanID, _ := genIDs()

        // Create lightning channels between Alice&lt;-&gt;Bob for Alice and Bob
        alice, bob, err := createTestChannel(t, alicePrivKey, bobPrivKey,
                aliceToBob, bobToAlice, 0, 0, firstChanID,
        )
        if err != nil </span><span class="cov0" title="0">{
                return nil, nil, errors.Errorf("unable to create "+
                        "alice&lt;-&gt;bob channel: %v", err)
        }</span>

        <span class="cov8" title="1">return alice, bob, nil</span>
}

// hopNetwork is the base struct for two and three hop networks
type hopNetwork struct {
        feeEstimator *mockFeeEstimator
        globalPolicy models.ForwardingPolicy
        obfuscator   hop.ErrorEncrypter

        defaultDelta uint32
}

func newHopNetwork() *hopNetwork <span class="cov8" title="1">{
        defaultDelta := uint32(6)

        globalPolicy := models.ForwardingPolicy{
                MinHTLCOut:    lnwire.NewMSatFromSatoshis(5),
                BaseFee:       lnwire.NewMSatFromSatoshis(1),
                TimeLockDelta: defaultDelta,
        }
        obfuscator := NewMockObfuscator()

        return &amp;hopNetwork{
                feeEstimator: newMockFeeEstimator(),
                globalPolicy: globalPolicy,
                obfuscator:   obfuscator,
                defaultDelta: defaultDelta,
        }
}</span>

func (h *hopNetwork) createChannelLink(server, peer *mockServer,
        channel *lnwallet.LightningChannel,
        decoder *mockIteratorDecoder) (ChannelLink, error) <span class="cov8" title="1">{

        const (
                fwdPkgTimeout       = 15 * time.Second
                minFeeUpdateTimeout = 30 * time.Minute
                maxFeeUpdateTimeout = 40 * time.Minute
        )

        notifyUpdateChan := make(chan *contractcourt.ContractUpdate)
        doneChan := make(chan struct{})
        notifyContractUpdate := func(u *contractcourt.ContractUpdate) error </span><span class="cov8" title="1">{
                select </span>{
                case notifyUpdateChan &lt;- u:<span class="cov8" title="1"></span>
                case &lt;-doneChan:<span class="cov8" title="1"></span>
                }

                <span class="cov8" title="1">return nil</span>
        }

        <span class="cov8" title="1">getAliases := func(
                base lnwire.ShortChannelID) []lnwire.ShortChannelID </span><span class="cov8" title="1">{

                return nil
        }</span>

        <span class="cov8" title="1">forwardPackets := func(linkQuit &lt;-chan struct{}, _ bool,
                packets ...*htlcPacket) error </span><span class="cov8" title="1">{

                return server.htlcSwitch.ForwardPackets(linkQuit, packets...)
        }</span>

        //nolint:ll
        <span class="cov8" title="1">link := NewChannelLink(
                ChannelLinkConfig{
                        BestHeight:         server.htlcSwitch.BestHeight,
                        FwrdingPolicy:      h.globalPolicy,
                        Peer:               peer,
                        Circuits:           server.htlcSwitch.CircuitModifier(),
                        ForwardPackets:     forwardPackets,
                        DecodeHopIterators: decoder.DecodeHopIterators,
                        ExtractErrorEncrypter: func(*btcec.PublicKey) (
                                hop.ErrorEncrypter, lnwire.FailCode) </span><span class="cov8" title="1">{
                                return h.obfuscator, lnwire.CodeNone
                        }</span>,
                        FetchLastChannelUpdate: mockGetChanUpdateMessage,
                        Registry:               server.registry,
                        FeeEstimator:           h.feeEstimator,
                        PreimageCache:          server.pCache,
                        UpdateContractSignals: func(*contractcourt.ContractSignals) error <span class="cov8" title="1">{
                                return nil
                        }</span>,
                        NotifyContractUpdate:    notifyContractUpdate,
                        ChainEvents:             &amp;contractcourt.ChainEventSubscription{},
                        SyncStates:              true,
                        BatchSize:               10,
                        BatchTicker:             ticker.NewForce(testBatchTimeout),
                        FwdPkgGCTicker:          ticker.NewForce(fwdPkgTimeout),
                        PendingCommitTicker:     ticker.New(2 * time.Minute),
                        MinUpdateTimeout:        minFeeUpdateTimeout,
                        MaxUpdateTimeout:        maxFeeUpdateTimeout,
                        OnChannelFailure:        func(lnwire.ChannelID, lnwire.ShortChannelID, LinkFailureError) {<span class="cov8" title="1">}</span>,
                        OutgoingCltvRejectDelta: 3,
                        MaxOutgoingCltvExpiry:   DefaultMaxOutgoingCltvExpiry,
                        MaxFeeAllocation:        DefaultMaxLinkFeeAllocation,
                        MaxAnchorsCommitFeeRate: chainfee.SatPerKVByte(10 * 1000).FeePerKWeight(),
                        NotifyActiveLink:        func(wire.OutPoint) {<span class="cov8" title="1">}</span>,
                        NotifyActiveChannel:     func(wire.OutPoint) {<span class="cov8" title="1">}</span>,
                        NotifyInactiveChannel:   func(wire.OutPoint) {<span class="cov8" title="1">}</span>,
                        NotifyInactiveLinkEvent: func(wire.OutPoint) {<span class="cov8" title="1">}</span>,
                        HtlcNotifier:            server.htlcSwitch.cfg.HtlcNotifier,
                        GetAliases:              getAliases,
                        ShouldFwdExpEndorsement: func() bool <span class="cov8" title="1">{ return true }</span>,
                },
                channel,
        )
        <span class="cov8" title="1">if err := server.htlcSwitch.AddLink(link); err != nil </span><span class="cov0" title="0">{
                return nil, fmt.Errorf("unable to add channel link: %w", err)
        }</span>

        <span class="cov8" title="1">go func() </span><span class="cov8" title="1">{
                if chanLink, ok := link.(*channelLink); ok </span><span class="cov8" title="1">{
                        for </span><span class="cov8" title="1">{
                                select </span>{
                                case &lt;-notifyUpdateChan:<span class="cov8" title="1"></span>
                                case &lt;-chanLink.cg.Done():<span class="cov8" title="1">
                                        close(doneChan)
                                        return</span>
                                }
                        }
                }
        }()

        <span class="cov8" title="1">return link, nil</span>
}

// twoHopNetwork is used for managing the created cluster of 2 hops.
type twoHopNetwork struct {
        hopNetwork

        aliceServer      *mockServer
        aliceChannelLink *channelLink

        bobServer      *mockServer
        bobChannelLink *channelLink
}

// newTwoHopNetwork function creates and starts the following topology and
// returns the control object to manage this cluster:
//
// alice                      bob
// server - &lt;-connection-&gt; - server
//
//        |                      |
//
// alice htlc               bob htlc
// switch                   switch
//
//        |                      |
//        |                      |
//
// alice                      bob
// channel link           channel link.
func newTwoHopNetwork(t testing.TB,
        aliceChannel, bobChannel *lnwallet.LightningChannel,
        startingHeight uint32) *twoHopNetwork <span class="cov8" title="1">{

        aliceDb := aliceChannel.State().Db.GetParentDB()
        bobDb := bobChannel.State().Db.GetParentDB()

        hopNetwork := newHopNetwork()

        // Create two peers/servers.
        aliceServer, err := newMockServer(
                t, "alice", startingHeight, aliceDb, hopNetwork.defaultDelta,
        )
        require.NoError(t, err, "unable to create alice server")
        bobServer, err := newMockServer(
                t, "bob", startingHeight, bobDb, hopNetwork.defaultDelta,
        )
        require.NoError(t, err, "unable to create bob server")

        // Create mock decoder instead of sphinx one in order to mock the route
        // which htlc should follow.
        aliceDecoder := newMockIteratorDecoder()
        bobDecoder := newMockIteratorDecoder()

        aliceChannelLink, err := hopNetwork.createChannelLink(
                aliceServer, bobServer, aliceChannel, aliceDecoder,
        )
        if err != nil </span><span class="cov0" title="0">{
                t.Fatal(err)
        }</span>

        <span class="cov8" title="1">bobChannelLink, err := hopNetwork.createChannelLink(
                bobServer, aliceServer, bobChannel, bobDecoder,
        )
        if err != nil </span><span class="cov0" title="0">{
                t.Fatal(err)
        }</span>

        <span class="cov8" title="1">n := &amp;twoHopNetwork{
                aliceServer:      aliceServer,
                aliceChannelLink: aliceChannelLink.(*channelLink),

                bobServer:      bobServer,
                bobChannelLink: bobChannelLink.(*channelLink),

                hopNetwork: *hopNetwork,
        }

        require.NoError(t, n.start())
        t.Cleanup(n.stop)

        return n</span>
}

// start starts the two hop network alice,bob servers.
func (n *twoHopNetwork) start() error <span class="cov8" title="1">{
        if err := n.aliceServer.Start(); err != nil </span><span class="cov0" title="0">{
                return err
        }</span>
        <span class="cov8" title="1">if err := n.bobServer.Start(); err != nil </span><span class="cov0" title="0">{
                n.aliceServer.Stop()
                return err
        }</span>

        <span class="cov8" title="1">return waitLinksEligible(map[string]*channelLink{
                "alice": n.aliceChannelLink,
                "bob":   n.bobChannelLink,
        })</span>
}

// stop stops nodes and cleanup its databases.
func (n *twoHopNetwork) stop() <span class="cov8" title="1">{
        done := make(chan struct{})
        go func() </span><span class="cov8" title="1">{
                n.aliceServer.Stop()
                done &lt;- struct{}{}
        }</span>()

        <span class="cov8" title="1">go func() </span><span class="cov8" title="1">{
                n.bobServer.Stop()
                done &lt;- struct{}{}
        }</span>()

        <span class="cov8" title="1">for i := 0; i &lt; 2; i++ </span><span class="cov8" title="1">{
                &lt;-done
        }</span>
}

func (n *twoHopNetwork) makeHoldPayment(sendingPeer, receivingPeer lnpeer.Peer,
        firstHop lnwire.ShortChannelID, hops []*hop.Payload,
        invoiceAmt, htlcAmt lnwire.MilliSatoshi,
        timelock uint32, preimage lntypes.Preimage) chan error <span class="cov8" title="1">{

        paymentErr := make(chan error, 1)

        sender := sendingPeer.(*mockServer)
        receiver := receivingPeer.(*mockServer)

        // Generate route convert it to blob, and return next destination for
        // htlc add request.
        blob, err := generateRoute(hops...)
        if err != nil </span><span class="cov0" title="0">{
                paymentErr &lt;- err
                return paymentErr
        }</span>

        <span class="cov8" title="1">rhash := preimage.Hash()

        var payAddr [32]byte
        if _, err := crand.Read(payAddr[:]); err != nil </span><span class="cov0" title="0">{
                panic(err)</span>
        }

        // Generate payment: invoice and htlc.
        <span class="cov8" title="1">invoice, htlc, pid, err := generatePaymentWithPreimage(
                invoiceAmt, htlcAmt, timelock, blob,
                nil, rhash, payAddr,
        )
        if err != nil </span><span class="cov0" title="0">{
                paymentErr &lt;- err
                return paymentErr
        }</span>

        // Check who is last in the route and add invoice to server registry.
        <span class="cov8" title="1">if err := receiver.registry.AddInvoice(
                context.Background(), *invoice, rhash,
        ); err != nil </span><span class="cov0" title="0">{
                paymentErr &lt;- err
                return paymentErr
        }</span>

        // Send payment and expose err channel.
        <span class="cov8" title="1">err = sender.htlcSwitch.SendHTLC(firstHop, pid, htlc)
        if err != nil </span><span class="cov0" title="0">{
                paymentErr &lt;- err
                return paymentErr
        }</span>

        <span class="cov8" title="1">go func() </span><span class="cov8" title="1">{
                resultChan, err := sender.htlcSwitch.GetAttemptResult(
                        pid, rhash, newMockDeobfuscator(),
                )
                if err != nil </span><span class="cov0" title="0">{
                        paymentErr &lt;- err
                        return
                }</span>

                <span class="cov8" title="1">result, ok := &lt;-resultChan
                if !ok </span><span class="cov0" title="0">{
                        paymentErr &lt;- fmt.Errorf("shutting down")
                        return
                }</span>

                <span class="cov8" title="1">if result.Error != nil </span><span class="cov8" title="1">{
                        paymentErr &lt;- result.Error
                        return
                }</span>
                <span class="cov8" title="1">paymentErr &lt;- nil</span>
        }()

        <span class="cov8" title="1">return paymentErr</span>
}

// waitLinksEligible blocks until all links the provided name-to-link map are
// eligible to forward HTLCs.
func waitLinksEligible(links map[string]*channelLink) error <span class="cov8" title="1">{
        return wait.NoError(func() error </span><span class="cov8" title="1">{
                for name, link := range links </span><span class="cov8" title="1">{
                        if link.EligibleToForward() </span><span class="cov8" title="1">{
                                continue</span>
                        }
                        <span class="cov0" title="0">return fmt.Errorf("%s channel link not eligible", name)</span>
                }
                <span class="cov8" title="1">return nil</span>
        }, 3*time.Second)
}

// timeout implements a test level timeout.
func timeout() func() <span class="cov8" title="1">{
        done := make(chan struct{})
        go func() </span><span class="cov8" title="1">{
                select </span>{
                case &lt;-time.After(20 * time.Second):<span class="cov0" title="0">
                        pprof.Lookup("goroutine").WriteTo(os.Stdout, 1)

                        panic("test timeout")</span>
                case &lt;-done:<span class="cov8" title="1"></span>
                }
        }()

        <span class="cov8" title="1">return func() </span><span class="cov8" title="1">{
                close(done)
        }</span>
}
</pre>
		
		</div>
	</body>
	<script>
	(function() {
		var files = document.getElementById('files');
		var visible;
		files.addEventListener('change', onChange, false);
		function select(part) {
			if (visible)
				visible.style.display = 'none';
			visible = document.getElementById(part);
			if (!visible)
				return;
			files.value = part;
			visible.style.display = 'block';
			location.hash = part;
		}
		function onChange() {
			select(files.value);
			window.scrollTo(0, 0);
		}
		if (location.hash != "") {
			select(location.hash.substr(1));
		}
		if (!visible) {
			select("file0");
		}
	})();
	</script>
</html>
